<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Grad Math@USF</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Grad Math@USF</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 19 Oct 2021 09:10:26 -0400</lastBuildDate>
    <image>
      <url>/images/icon_hu52d4978bb57ec1511a90a19194ce34ae_631017_512x512_fill_lanczos_center_2.png</url>
      <title>Grad Math@USF</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Zeros of Harmonic Polynomials and Related Applications</title>
      <link>/talk/gravitationallensing/</link>
      <pubDate>Tue, 19 Oct 2021 09:10:26 -0400</pubDate>
      <guid>/talk/gravitationallensing/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Zeros of Harmonic Polynomials and Related Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Azizah Alrajhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date/Time&lt;/strong&gt;: Friday, October 22nd, 2:00 pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: In this thesis, we study topics related to harmonic functions, where
we are interested in the maximum number of solutions of a harmonic
polynomial equation and how it is related to gravitational lensing. In
Chapter 2, we studied the conditions that we have to have on the real
or complex coe cients of a polynomial p to get the maximum number
of distinct solutions for the equation $p(z) - \bar{z}^2 = 0$, where deg p = 2.
In Chapter 3, we discuss the lens equation when the lens is an ellipse, a
limacon, or a Neumann Oval. Also, we discuss a counterexample for a
conjecture by C. Beneteau and N. Hudson in [1]. We also in particular
discuss estimates related to the maximum number of solutions for the lens
equations for the Neumann Oval.&lt;/p&gt;
&lt;p&gt;[1] C. Beneteau and N. Hudson. &lt;em&gt;A survey on the maximal number of solutions of equations related to gravitational lensing&lt;/em&gt;. Complex Analysis and
Dynamical System, Trends Math. Cham: Birkhauser/Springer, 23-38.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Continuum Limit of Theta Functions</title>
      <link>/talk/continuumthetafunctions/</link>
      <pubDate>Sat, 09 Oct 2021 12:11:31 -0400</pubDate>
      <guid>/talk/continuumthetafunctions/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: The Continuum Limit of Theta Functions&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Fudong Wang (University of Central Florida)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date/Time&lt;/strong&gt;: Friday, October 15th, 2:00pm-3:00pm&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/x75sjMwHzdw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: The purpose of the talk is to introduce Venakides&#39; 1989 CPAM XLII paper on the continuum limit of theta functions. The problem is closely related to the semiclassical limit (or small dispersion, or nonlinear WKB analysis) for the famous nonlinear PDE: Korteweg-de Vries equation. Theta function can be considered as high dimensional Fourier series. The continuum limit of theta function we will discuss is the limit when the size of the period matrix of the theta function goes infinite in a certain way (or one can say the genus of the corresponding Riemann surface goes infinite). And we will associate the leading behavior with a minimizing problem (variational problem).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Ito Calculus</title>
      <link>/talk/itocalculus/</link>
      <pubDate>Tue, 05 Oct 2021 11:25:26 -0400</pubDate>
      <guid>/talk/itocalculus/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Introduction to Ito Calculus&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Nathan Hayford&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date/Time&lt;/strong&gt;: Friday, October 8th, 2:00 pm&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/iVeGX50a45s&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: The Ito calculus is at the foundation of the theory of stochastic PDEs. In this talk, I will introduce the calculus, and prove Ito’s lemma. I will then demonstrate how the theory of martingales + Ito’s lemma can be applied to derive deterministic equations for certain quantities appearing in the stochastic calculus. In particular, I will prove the conformal invariance of Brownian motion and the harmonic measure. If time permits, I will also start a discussion about Schramm-Loewner evolutions (SLEs).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Martingales and Stopping Times</title>
      <link>/talk/martingalesandstoppingtimes/</link>
      <pubDate>Mon, 20 Sep 2021 12:21:07 -0400</pubDate>
      <guid>/talk/martingalesandstoppingtimes/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Martingales and Stopping Times&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Zachary Forrest&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date/Time&lt;/strong&gt;: Friday, September 24th, 2:00 pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part I&lt;/strong&gt;:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/79dmo6O9PqE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part II&lt;/strong&gt;:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Y2OvW7mmLso&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: In Stochastic Calculus and elsewhere, the notion of Martingales and Stopping Times are invaluable for characterizing random processes whose mean behavior is invariant but whose data changes over time. In this talk we will clearly define the notions of Martingales, Stopping Times, and any necessary analytical objects; introduce basic properties of these objects, including Doob&amp;rsquo;s &amp;ldquo;Optimal Sampling&amp;rdquo; Theorem; and briefly touch upon some applications of Martingales and Stopping Times in research.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Drifting through random walks and Brownian motions</title>
      <link>/talk/brownianmotion/</link>
      <pubDate>Mon, 13 Sep 2021 18:58:31 -0400</pubDate>
      <guid>/talk/brownianmotion/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Drifting through random walks and Brownian motions&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Nathan Hayford&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: Friday, September 17th&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 2:00pm-3:00pm&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/r8A-7IbNfBY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;:  In this talk, we will leisurely wander through the area of study of random walks and Brownian motions. We will begin by studying many of the important properties and realizations of random walks. In particular, we will examine the return probabilities of random walks in 1, 2, and 3 dimensions; as more eloquently stated by S. Kakutani, “a drunk man will find his way home, but a drunk bird will be lost forever”. We will also study the scaling limit of a random walk, the so-called Brownian motion. Many of the properties of random walks will be reflected by Brownian motions, and we will see that they are the first example of a very important class of random processes known as martingales.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Random Matrix Theory</title>
      <link>/posts/randommatrices/</link>
      <pubDate>Sat, 28 Aug 2021 18:24:20 -0400</pubDate>
      <guid>/posts/randommatrices/</guid>
      <description>&lt;p&gt;Here, we discuss some selected topics and techniqes from random matrix theory. For sake of clarity, we will first
deal with 1-matrix models, and illustrate how they count the number of planar diagrams in a certain limit. Then, we
discuss the 2-matrix models, which enumerate 2-colored planar diagrams. For this, we will need the Harish-Chandra
(Itzykson-Zuber) formula, which we also derive. Before discussing the connections of these matrix models to enumeration
problems, we first derive some basic facts about the space of Hermitian matrices that we will eventually need in order
to perform calculations.&lt;/p&gt;
&lt;p&gt;We will always let $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; denote the space of $n\times n$ Hermitian matrices, i.e. matrices $X$ such that
$X^\dagger = X$ (&amp;quot;$\dagger$&amp;quot; here denotes hermitian conjugate). The dimension of this space may be
computed by noticing the following: the hermiticity condition requires that the diagonal of the matrix be real
($z = \bar{z}$ implies that $z$ is real), therefore the diagonal entries contribute $n$ variables. Above the diagonal,
there are $2 \cdot \frac{n(n-1)}{2} = n (n-1)$ variables (the factor of two coming from real + imaginary components);
therefore we find the dimension of the space of Hermitian matrices to be
\begin{align*}
\dim \mathcal{H}_n = n + 2 \cdot \frac{n(n-1)}{2} = n^2.
\end{align*}
It is often convenient for us to work in the so-called &lt;em&gt;eigenvalue variables&lt;/em&gt; on the space of Hermitian matrices.
These eigenvalues are of course real, by properties of Hermitian matrices; the remaining variables are variables in
the unitary group $U(n)/U_{diag}(n)$ and are called &lt;em&gt;angular variables&lt;/em&gt;. This is apparent, by writing
$X = U^\dagger \Lambda U$; note that replacing $U \to \text{diag}(e^{i\theta_1},\cdots e^{i\theta_n} )U$ does not change the representation.
Defining $du := dU^\dagger U = -du^{\dagger}$, we find that $dX$ takes the form
\begin{align*}
dX = d(U^\dagger \Lambda U) &amp;amp;= dU^\dagger \Lambda U+ U^\dagger d\Lambda U + U^\dagger \Lambda dU \\\&lt;br&gt;
&amp;amp;= U^\dagger (d u^\dagger \Lambda + d \Lambda  + \Lambda d u) U \\\&lt;br&gt;
&amp;amp;= U^\dagger (d \Lambda + [\Lambda, d u] ) U.
\end{align*}&lt;/p&gt;
&lt;p&gt;The metric can then be computed as
\begin{align*}
g = \text{tr} (dX^\dagger dX) = \text{tr}(d \Lambda d \Lambda) + \text{tr}([\Lambda, d u])^2,
\end{align*}
which is diagonal in the coordinates $(d\lambda_i, du_{ij})$:
\begin{align*}
g = \sum_{i = 1}^n d\lambda_i^2 + 2 \sum_{i &amp;lt; j} (\lambda_i - \lambda_j)^2 (du_{ij})^2
\end{align*}
The volume form is then
\begin{align*}
d\text{vol}(X) 	&amp;amp;= \sqrt{\det g} \prod_{i=1}^n d\lambda_i \prod_{i &amp;lt; j} du_{ij}  \\\&lt;br&gt;
&amp;amp;= 2^{n(n-1)} \prod(\lambda_i - \lambda_j)^2 \prod_{i=1}^n d\lambda_i \prod_{i &amp;lt; j} du_{ij} \\\&lt;br&gt;
&amp;amp;\propto  dU \prod_{i &amp;lt; j}(\lambda_i - \lambda_j)^2 \prod_{i=1}^n d\lambda_i,
\end{align*}
where $dU$ is the Haar measure on $U(n)/U_{diag}(n)$. The quantity
\begin{equation}
V(\lambda) := \prod_{i &amp;lt; j}(\lambda_i - \lambda_j) = \det(\lambda_i^{j-1})
\end{equation}
is called the &lt;em&gt;Vandermonde determinant&lt;/em&gt;. We shall meet it often later. The exponent of two in the Vandermonde determinant
comes from the fact that there are twice as many variables $du_{ij}$, with contributions coming from real and
imaginary parts, respectively. Because it is a quantity we will need later, we also compute the Laplace-Beltrami operator on the submanifold
of the eigenvalue coordinates; this is
\begin{align*}
\Delta_{LB} &amp;amp;= \sum_{i=1}^n \frac{\partial^2}{\partial X_{ii}^2} + \frac{1}{2}\sum_{i &amp;lt; j}\left(\frac{\partial^2}{\partial \text{Re}(X_{ij})^2} +
\frac{\partial^2}{\partial \text{Im}(X_{ij})^2}\right)\\\ &amp;amp;=
\frac{1}{V^2(\lambda)}\sum_{i=1}^n\frac{\partial}{\partial \lambda_i} \left(V^2(\lambda)
\frac{\partial}{\partial \lambda_i} \cdot\right) + (\Delta \textit{ on the angular coordinates}),
\end{align*}
i.e., on only the eigenvalue coordinates, we have
\begin{equation}\label{eigenvalue-laplacian}
\Delta_{eigenvalues} = \frac{1}{V^2(\lambda)}\sum_{i=1}^n\frac{\partial}{\partial \lambda_i} \left(V^2(\lambda) \frac{\partial}{\partial \lambda_i} \cdot\right).
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;1-the-gaussian-unitary-ensemble--planar-diagrams&#34;&gt;$1$. The Gaussian Unitary Ensemble &amp;amp; Planar Diagrams.&lt;/h2&gt;
&lt;p&gt;We now demonstrate how certain integrals over $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; can be related to enumeration of planar diagrams,
a fact which was first noticed by the physicists 
&lt;a href=&#34;https://projecteuclid.org/journals/communications-in-mathematical-physics/volume-59/issue-1/Planar-diagrams/cmp/1103901558.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;E. Brezin, C. Itzykson, G. Parisi, &amp;amp; J. B. Zuber&lt;/a&gt;.
Most of the results presented here are standard; they can be found in places like 
&lt;a href=&#34;https://arxiv.org/abs/math-ph/0406013&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt; by Phillipe Di
Francesco, or in 
&lt;a href=&#34;https://www.labri.fr/perso/zvonkin/Research/matrixint.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this one&lt;/a&gt; by Alexander Zvonkin, for example.
Let $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; denote the space of $n\times n$ Hermitian matrices. We consider the measure
\begin{align*}
d\mathbb{P}_n(X) = \frac{1}{Z_n} e^{-\frac{n}{2}\text{tr} X^2} dX,
\end{align*}
where $dX$ is Haar measure on $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; (which we have just defined in the previous section), and $Z_n$ is a
normalization constant so that $\int d\mathbb{P} = 1$, called the \textit{partition function}. The ensemble of
such matrices, taken together with the probability measure $\mathbb{P}$&lt;sub&gt;n&lt;/sub&gt;, form the &lt;em&gt;Gaussian Unitary Ensemble&lt;/em&gt; (GUE).
We denote the expected value of a random variable $f(X):$ $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; $\to \mathbb{R}$ in this ensemble by $\langle f(X) \rangle_n$; explicitly,
\begin{equation}
\langle f(X) \rangle_n = \frac{1}{Z_n} dX \int_{\mathcal{H}_n} f(X) e^{-\frac{n}{2}\text{tr} X^2}.
\end{equation}
We wish to evaluate expected values of certain random variables in the large $n$ limit, in particular (Notice
that $X\in \mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; is dependent on $n$ as well; we suppress dependence to simplify notations.)
\begin{equation}
\langle \text{tr} X^k \rangle:= \lim_{n\to \infty} \frac{1}{n}\langle \text{tr} X^k \rangle_n.
\end{equation}
We claim that
\begin{equation}
\langle \text{tr} X^k \rangle = C_k,
\end{equation}
where $C_k = \frac{1}{k+1}{2k \choose k}$ is the $k^{th}$ Catalan number. This will follow from some
combinatorial arguments. We begin by proving the following:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 1.1&lt;/strong&gt;&lt;/span&gt;
Let $S\in \mathcal{H}_n$ be a fixed matrix. Then,
\begin{equation}
\langle e^{\text{tr} XS}\rangle_n = e^{\text{tr} S^2/2n}.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;
We have that
\begin{align*}
\langle e^{\text{tr} XS} \rangle_n &amp;amp;= \frac{1}{Z_n}\int_{\mathcal{H}_n}dX e^{-\frac{n}{2}\text{tr} (X^2 -
\frac{2}{n} XS)}\\\ &amp;amp;=
e^{\text{tr} S^2/2n} \frac{1}{Z_n}\int_{\mathcal{H}_n}dX e^{-\frac{n}{2}\text{tr} (X-S/n)^2} \\\&lt;br&gt;
&amp;amp;=e^{\text{tr} S^2/2n} \frac{1}{Z_n}\int_{\mathcal{H}_n}d(X-S/n)  e^{-\frac{n}{2}\text{tr} (X-S/n)^2}\\\
&amp;amp;= e^{\text{tr} S^2/2n} \frac{1}{Z_n}\int_{\mathcal{H}_n}dX e^{-\frac{n}{2}\text{tr} (X)^2}
= e^{\text{tr} S^2/2n},
\end{align*}
where in the last step, we used the fact that $dX$ is translation invariant.&lt;/p&gt;
&lt;div style=&#34;text-align: right&#34;&gt; $\square$ &lt;/div&gt;
&lt;p&gt;Consequentially, we can compute any expected value of interest by differentiating $f(S) := e^{\text{tr} S^2/2n}$
with respect to $S$, and evaluating at $S=0$. For example,
\begin{equation*}
\langle X_{ij} \rangle_n = \frac{\partial f(S)}{\partial S_{ji}} \bigg|_{S=0}, \qquad \qquad
\langle X_{i_1j_1}X_{i_2j_2}\rangle_n = \frac{\partial^2 f(S)}{\partial S_{j_1 i_1} \partial S_{j_2 i_2}}
\bigg|_{S=0}.
\end{equation*}
Explicitly, we have that&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:green&#34;&gt;&lt;strong&gt;Lemma 1.1&lt;/strong&gt;&lt;/span&gt;
For any indices $i_1,j_1,i_2,j_2$,
\begin{equation}
\langle X_{i_1j_1} \rangle_n = 0, \qquad \qquad \langle X_{i_1j_1}X_{i_2j_2}\rangle_n = \frac{1}{n}
\delta_{i_1 j_2}\delta_{i_2 j_1}.
\end{equation}
&lt;em&gt;Proof.&lt;/em&gt;
Note that the exponent of $f(S)$ is $\frac{1}{2n}\text{tr} S^2 = \frac{1}{2n}\sum_{k,l} S_{kl}S_{lk}$, and
that
\begin{equation*}
\frac{\partial}{\partial S_{j_1 i_1}}\frac{1}{2n}\sum_{k,l} S_{kl}S_{lk} =
\frac{1}{2n}\sum_{k,l} \left(\delta_{k j_1}\delta_{l i_1} S_{lk} +
S_{kl}\delta_{l j_1}\delta_{k i_1}\right) = \frac{1}{n}S_{i_1 j_1}.
\end{equation*}
Therefore, by the chain rule,
\begin{equation*}
\langle X_{i_1j_1}\rangle_n = \frac{\partial}{\partial S_{j_1 i_1}}e^{\text{tr} S^2/2n}\bigg|_{S=0}
= \frac{1}{n}S_{i_1 j_1}e^{\text{tr} S^2/2n}\bigg|_{S=0} = 0.
\end{equation*}
Similarly, by the product rule,
\begin{align*}
\langle X_{i_1j_1}X_{i_2j_2} \rangle_n &amp;amp;= \frac{\partial}{\partial S_{j_2 i_2}}\frac{1}{n}S_{i_1 j_1}
e^{\text{tr} S^2/2n}\bigg|_{S=0} \\\&lt;br&gt;
&amp;amp;= \frac{1}{n}\delta_{i_1 j_2}\delta_{i_2 j_1}e^{\text{tr} S^2/2n}\bigg|_{S=0} +
\frac{1}{n}S_{i_1 j_1}S_{i_2 j_2}e^{\text{tr} S^2/2n}\bigg|_{S=0}\\\&lt;br&gt;
&amp;amp;= \frac{1}{n}
\delta_{i_1 j_2}\delta_{i_2 j_1}.
\end{align*}&lt;/p&gt;
 &lt;div style=&#34;text-align: right&#34;&gt; $\square$ &lt;/div&gt;
&lt;p&gt;Furthermore, any &amp;ldquo;higher&amp;rdquo; expected value can be computed via &lt;em&gt;Wick&amp;rsquo;s theorem}:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 1.2.&lt;/strong&gt;&lt;/span&gt;
&lt;em&gt;(Wick&amp;rsquo;s Theorem.)&lt;/em&gt; Let $I = {(i_1,j_1),\cdots (i_{2M},j_{2M})}$ be a collection of indices,
$1 \leq i_k,j_k \leq n$. Then,
\begin{equation}
\langle \prod_{(i,j) \in I}X_{ij} \rangle_n = \sum_{\pi \in \Pi_{2M}}
\prod_{(i,j),(k,l)\in \pi} \langle X_{i j}X_{k l}\rangle_n,
\end{equation}
where $\Pi_{2M}$ is the set of all &lt;em&gt;pairings&lt;/em&gt; of the index set $I$.&lt;/p&gt;
&lt;p&gt;For example, we have that
\begin{align*}
\langle X_{i_1 j_1} X_{i_2 j_2} X_{i_3 j_3} X_{i_4 j_4}\rangle_n &amp;amp;=
\langle X_{i_1 j_1} X_{i_2 j_2}\rangle_n \langle X_{i_3 j_3} X_{i_4 j_4}\rangle_n\\\&lt;br&gt;
&amp;amp;+\langle X_{i_1 j_1} X_{i_3 j_3}\rangle_n \langle X_{i_2 j_2} X_{i_4 j_4}\rangle_n\\\&lt;br&gt;
&amp;amp;+ \langle X_{i_1 j_1} X_{i_4 j_4}\rangle_n\langle X_{i_2 j_2} X_{i_3 j_3}\rangle_n.
\end{align*}&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;
&lt;em&gt;(Sketch.)&lt;/em&gt; The theorem follows from the fact that derivatives must come in pairs for nonzero
contributions to the expected value to appear; see the proof of the lemma. It is clear that the sum
must be taken over all such pairings. The full proof of this fact is given in 
&lt;a href=&#34;https://www.labri.fr/perso/zvonkin/Research/matrixint.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zvonkin&amp;rsquo;s survey&lt;/a&gt;.&lt;/p&gt;
&lt;div style=&#34;text-align: right&#34;&gt; $\square$ &lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color:green&#34;&gt;&lt;strong&gt;Corollary 1.1&lt;/strong&gt;&lt;/span&gt;
The expected value of the product of an odd number of matrix elements is necessarily zero.&lt;/p&gt;
&lt;p&gt;Using the above theorems, we are now able to compute expected values of quantities like $\langle \text{tr} X^k \rangle_n$. By
the corollary, we trivially have that $\langle \text{tr} X^{2k+1}\rangle_n = 0$; it remains to compute the traces of
even powers. This can be accomplished by realizing the pairings in Wick&amp;rsquo;s theorem as sums over families of $1$-
face maps. We briefly summarize these results. Let us begin by way of example. Consider the expected value
$\langle\text{tr} X^4 \rangle_n$; we have that, by linearity of $\langle \cdot \rangle_n$:
\begin{equation*}
\langle \text{tr} X^4 \rangle_n = \sum_{i_1, \cdots, i_4} \langle X_{i_1 i_2} X_{i_2 i_3} X_{i_3 i_4} X_{i_4 i_1} \rangle_n.
\end{equation*}
Using Wick&amp;rsquo;s theorem, we see that there are $3$ contributing terms in the above sum:
\begin{align*}
\langle X_{i_1 i_2} X_{i_2 i_3}\rangle_n &amp;amp;\langle X_{i_3 i_4} X_{i_4 i_1}\rangle_n,\quad
\langle X_{i_1 i_2} X_{i_3 i_4}\rangle_n \langle X_{i_2 i_3} X_{i_4 i_1}\rangle_n,\quad
\text{and }\\\ &amp;amp;\langle X_{i_1 i_2} X_{i_4 i_1}\rangle_n \langle X_{i_2 i_3} X_{i_3 i_4}\rangle_n.
\end{align*}
We will deal with each of these contributions separately. By Lemma 1.1, we see that the first term
is only nonvanishing if $i_1 = i_3$; this leaves three free indices, so that
\begin{equation} \label{termA}
\sum_{i_1, \cdots, i_4}\langle X_{i_1 i_2} X_{i_2 i_3}\rangle_n \langle X_{i_3 i_4} X_{i_4 i_1}\rangle_n
= \sum_{i_1, i_2, i_3} \frac{1}{n}\cdot\frac{1}{n} = n.
\end{equation}
Again by Lemma 1.1, we see that the second term
$\langle X_{i_1 i_2} X_{i_3 i_4}\rangle_n \langle X_{i_2 i_3} X_{i_4 i_1}\rangle_n$ is only nonvanishing
if $i_1 = i_2 = i_3 = i_4$, leaving only one free index, so that this term contributes a total factor of
$\frac{1}{n}$:
\begin{equation}\label{termB}
\sum_{i_1, \cdots, i_4}
\langle X_{i_1 i_2} X_{i_3 i_4}\rangle_n \langle X_{i_2 i_3} X_{i_4 i_1}\rangle_n =
\sum_{i_1} \frac{1}{n}\cdot\frac{1}{n} = \frac{1}{n}.
\end{equation}
Finally, the last term in the sum,
$\langle X_{i_1 i_2} X_{i_4 i_1}\rangle_n \langle X_{i_2 i_3} X_{i_3 i_4}\rangle_n$, similarly contributes
a factor of $n$, since the only constraint on indices here is $i_2 = i_4$, by the lemma:
\begin{equation}\label{termC}
\sum_{i_1, \cdots, i_4} \langle X_{i_1 i_2} X_{i_4 i_1}\rangle_n \langle X_{i_2 i_3} X_{i_3 i_4} \rangle_n
= n.
\end{equation}
Combining the results above, we find that
\begin{equation}
\frac{1}{n}\langle X^4 \rangle_n = \frac{1}{n}\left(n + \frac{1}{n} + n\right) = 2 +
\frac{1}{n^2};
\end{equation}
Thus, we can easily compute the large $n$ limit: $\lim_{n\to \infty}\frac{1}{n} \langle X^4\rangle_n = 2$, which is indeed
the second Catalan number.&lt;/p&gt;
&lt;p&gt;The above computation is essentially generic; one computes $\langle X^{2k} \rangle_n$ by first applying Wick&amp;rsquo;s formula,
then computing the individual contributions of each pairing by investigating the number of free indices, and
finally by summing these contributions. In fact, the problem is essentially reduced to counting the number of
pairings that give only one free index, among the $(2k-1)!!$ possible pairings appearing in the sum. We can
count the number of such pairings using the following geometric interpretation of each of the terms.
Consider the Wick expansion of the trace $\langle\text{tr} X^{m}\rangle_n$; a generic term in this expansion will consist of
a product of &amp;ldquo;pair correlations&amp;rdquo;, i.e., expected values of pairs of matrix elements: $\langle X_{i j}X_{k l}\rangle_n$.
We draw a diagram with $n$ &amp;ldquo;half edges&amp;rdquo;, as seen here:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/RandomMatrices_images/trX2n.png&#34; alt=&#34;Pair Correlations&#34;&gt;&lt;/p&gt;
&lt;p&gt;To each term in the Wick expansion of
$\langle \text{tr} X^{m} \rangle_n$, we associate one of these diagrams: if $(i_N,j_N)$ is paired $(i_M,j_M)$, we connect
the corresponding half-edges to form a full edge. The diagrams
can then be used to compute the contribution of each term in the expansion. For example, two possible pairings
appearing in the Wick expansion of $\langle\text{tr} X^6\rangle_n$ are depicted below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/RandomMatrices_images/trX6_terms.png&#34; alt=&#34;aathis is an image&#34;&gt;&lt;/p&gt;
&lt;p&gt;The leftmost diagram corresponds to the term
$\langle X_{i_1 i_2}X_{i_2 i_3}\rangle_n \langle X_{i_3 i_4}X_{i_4 i_5}\rangle_n
\langle X_{i_5 i_6}X_{i_6 i_1} \rangle_n$; we see that this term is only nonvanishing when $i_1=i_3=i_5$, and for any
$i_2,i_4,i_6$. Thus, the diagram contributes a factor of $n$ to $\langle \textit{tr} X^6 \rangle_n$. On the other hand, the
diagram on the right corresponds to the pairing $\langle X_{i_1 i_2}X_{i_3 i_4}\rangle_n
\langle X_{i_2 i_3}X_{i_6 i_1}\rangle_n \langle X_{i_4 i_5}X_{i_6 i_6}\rangle_n$, which is only nonvanishing when
$i_1 = i_4 = i_6 = i_3 = i_2$, and for any $i_5$. It follows that this diagram contributes a
factor of $1/n$, which will vanish in the large $n$ limit, as the diagram is non-planar.&lt;/p&gt;
&lt;p&gt;Finally, we can conclude that&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 1.3.&lt;/strong&gt;&lt;/span&gt;
\begin{equation}
\langle\text{tr} X^{2k}\rangle_n = \sum_{g=0}^{[k/2]} \varepsilon_g(k) n^{1-2g},
\end{equation}
where $\varepsilon_g(k)$ is the number of labelled one-face maps with $k$ edges of genus $g$.&lt;/p&gt;
&lt;p&gt;In particular,
\begin{equation}
\frac{1}{n}\langle \text{tr} X^{2k}\rangle_n = \varepsilon_0(k) + o(1/n),
\end{equation}
where $\varepsilon_0(k)$ is the number of &lt;em&gt;planar&lt;/em&gt; labelled one-face maps with $k$ edges. For this reason,
the large $n$ limit is sometimes referred to as the &lt;em&gt;planar limit&lt;/em&gt;. It is well-known
that $\varepsilon_0(k) = C_k$, the $k^{th}$ Catalan number.&lt;/p&gt;
&lt;h2 id=&#34;2-matrix-ensembles-and-orthogonal-polynomials&#34;&gt;$2.$ Matrix Ensembles and Orthogonal Polynomials.&lt;/h2&gt;
&lt;p&gt;Most computations within the scheme matrix ensembles are performed with techniques from the theory of orthogonal polynomials. Let us illustrate this
connection in several different ways. Let us consider the Hermitian ensemble with measure
\begin{equation*} \label{V-ensemble}
d\mathbb{P}_n(X) = \frac{1}{Z_n} e^{-\text{tr} Q(X)} dX,
\end{equation*}
where $Q(X) = X^{2n} + \cdots$ is a monic polynomial of even degree. Along with this ensemble, we also consider the family of monic
orthogonal polynomials ${\pi_n(\lambda)}$, which satisfy the relation
\begin{equation}\label{Q-OP}
(\pi_m,\pi_n) := \int_{\mathbb{R}}d\lambda e^{-Q(\lambda)} \pi_n(\lambda)\pi_m(\lambda)  = h_n \delta_{nm},
\end{equation}
where $V(\lambda)$ is the same polynomial, but in the real variable $\lambda$ instead of the matrix variable $X$. Our first connection between the
ensemble defined by the above measure and the polynomials ${\pi_n(\lambda)}$ is given by the following theorem:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 2.1.&lt;/strong&gt;&lt;/span&gt;
(Heine&amp;rsquo;s formula) The polynomials $\pi_n(x)$ satisfy
\begin{equation}
\pi_n(\lambda) = \langle \det(\lambda - X)\rangle_n.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;
The proof is rather straightforward; first, it is clear from its definition that the right hand side of the above equation
is a monic polynomial of degree $n$. Thus, we have only to show that this polynomial is orthogonal to $\lambda^m$, for
$0 \leq m \leq n$. Suppose this is the case; then,
\begin{align*}
Z_n \cdot \big(\lambda^m, \langle \det(\lambda - X)_n\big) &amp;amp;=
\int_{\mathbb{R}} d\lambda  e^{-Q(\lambda)} \lambda^m\bigg[ \int_{\mathbb{R}^n}\prod_{i=1}^n d\lambda_i (\lambda - \lambda_i)e^{-Q(\lambda_i)}  V^2(\lambda_1,\cdots,\lambda_n) \bigg] ;
\end{align*}
Note that $V(\lambda_1,\cdots,\lambda_n)\prod_{i=1}^n(\lambda - \lambda_i) = V(\lambda_1,\cdots,\lambda_n,\lambda)$, the Vandermonde
determinant with one extra variable. Furthermore, the remaining piece of the integrand,  $V(\lambda_1,\cdots,\lambda_n)\lambda^m$, can be
rewritten as (labelling $\lambda = \lambda_{n+1}$):
\begin{equation*}
V(\lambda_1,\cdots,\lambda_n)\lambda^m = \frac{1}{n+1}\sum_{i=1}^{n+1} (-1)^{i+n+1} V(\lambda_1,\cdots, \hat{\lambda_i},\cdots,\lambda_{n+1})\lambda_i^m;
\end{equation*}
This is the expression for the determinant
\begin{equation*}
\begin{pmatrix}
1 &amp;amp; \lambda_1 &amp;amp; \cdots &amp;amp; \lambda_1^{n-1} &amp;amp;\lambda_1^m\\\&lt;br&gt;
1 &amp;amp; \lambda_2 &amp;amp; \cdots &amp;amp; \lambda_2^{n-1} &amp;amp;\lambda_2^m\\\&lt;br&gt;
\vdots &amp;amp; \vdots &amp;amp; \cdots &amp;amp; \ddots &amp;amp;\vdots\\\&lt;br&gt;
1 &amp;amp; \lambda_{n+1} &amp;amp; \cdots &amp;amp; \lambda_{n+1}^{n-1} &amp;amp;\lambda_{n+1}^m\\\&lt;br&gt;
\end{pmatrix};
\end{equation*}
Clearly, this expression vanishes if $m &amp;lt; n$, and so
$\det(\lambda - X)_n$ is orthogonal to the first $m$ monomials, and is thus the (unique) monic orthogonal polynomial, $\pi_n(\lambda)$.&lt;/p&gt;
&lt;div style=&#34;text-align: right&#34;&gt; $\square$ &lt;/div&gt;
&lt;p&gt;In the above proof, if $m=n$, by what we just argued, we have that
\begin{align*}
Z_n \cdot \big(\lambda^n, \langle \det(\lambda - X)_n\big)
&amp;amp;= \frac{1}{n+1}\int_{\mathbb{R}}d\lambda e^{-Q(\lambda)} \bigg[ \int_{\mathbb{R}^n}\prod_{i=1}^n d\lambda_i
e^{-Q(\lambda_i)}  V^2(\lambda_1,\cdots,\lambda_n,\lambda) \bigg] \\\&lt;br&gt;
&amp;amp;= \frac{1}{n+1} Z_{n+1};
\end{align*}
furthermore, since $\big(\lambda^n, \langle \det(\lambda - X)\rangle_n\big) = \big(\lambda^n, \pi_n\big) = (\pi_n, \pi_n) = h_n$, we obtain that
\begin{equation*}
Z_{n+1} = (n+1)h_n;
\end{equation*}
We therefore obtain inductively a second important link between orthogonal polynomials and matrix models:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 2.2.&lt;/strong&gt;&lt;/span&gt;
The partition function of the matrix model defined above is related to the orthogonality constants ${h_k}$ by
\begin{equation}
Z_n = n! \prod_{k=0}^n h_k.
\end{equation}&lt;/p&gt;
&lt;p&gt;We wish to make one final connection between matrix models and orthogonal polynomials. Consider the probability density function on the
eigenvalues for this ensemble:
\begin{equation}
\rho_n(\lambda_1,\cdots, \lambda_n) :=  \frac{1}{Z_n} V^2(\lambda_1 ,\cdots,\lambda_n) e^{-\sum_{k=1}^n Q(\lambda_k)}.
\end{equation}
Recall that the Vandermonde determinant is the determinant of the matrix $\big(\lambda_i^{j-1} \big)$. Note that we can add any scalar
multiple of any row/column to any other row/column without changing the determinant; thus,
\begin{equation*}
\det \begin{pmatrix}
1 &amp;amp; \lambda_1 &amp;amp; \cdots &amp;amp; \lambda_1^{n-1}\\\&lt;br&gt;
1 &amp;amp; \lambda_2 &amp;amp; \cdots &amp;amp; \lambda_2^{n-1}\\\&lt;br&gt;
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp;\vdots\\\&lt;br&gt;
1 &amp;amp; \lambda_{n} &amp;amp; \cdots &amp;amp; \lambda_{n}^{n-1}\\\&lt;br&gt;
\end{pmatrix}
=
\det \begin{pmatrix}
p_0(\lambda_1) &amp;amp; p_1(\lambda_1) &amp;amp; \cdots &amp;amp; p_{n-1}(\lambda_1)\\\&lt;br&gt;
p_0(\lambda_2) &amp;amp; p_1(\lambda_2) &amp;amp; \cdots &amp;amp; p_{n-1}(\lambda_2)\\\&lt;br&gt;
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp;\vdots\\\&lt;br&gt;
p_0(\lambda_n) &amp;amp; p_1(\lambda_n) &amp;amp; \cdots &amp;amp; p_{n-1}(\lambda_n)\\\&lt;br&gt;
\end{pmatrix},
\end{equation*}
for any family of monic polynomials ${p_{k}(\lambda)}$. In particular, we will choose $p_{k}(\lambda) = \pi_k(\lambda)$,
the monic orthogonal polynomials with respect to the weight $e^{Q(\lambda)}$. Combining these two determinants, the density function becomes
\begin{align*}
\rho_n(\lambda_1,\cdots, \lambda_n) &amp;amp;=  \frac{1}{Z_n} \det(\pi_k(\lambda_i))\det(\pi_k(\lambda_j))
e^{-\sum_{k=1}^n Q(\lambda_k)}\\\&lt;br&gt;
&amp;amp;=\frac{1}{n! \prod_{k=0}^n h_k} \det\bigg(\sum_{k=0}^{n-1} \pi_k(\lambda_i)\pi_k(\lambda_j)\bigg)
e^{-\sum_{k=0}^{n-1} Q(\lambda_k)}\\\&lt;br&gt;
&amp;amp;=\frac{1}{n!} \det\bigg(\sum_{k=0}^{n-1} \frac{\pi_k(\lambda_i)\pi_k(\lambda_j)}{h_k}\bigg)e^{-\sum_{k=0}^{n-1}
Q(\lambda_k)}\\\&lt;br&gt;
&amp;amp;=\frac{1}{n!} \det\bigg(\sum_{k=0}^{n-1}
\frac{\pi_k(\lambda_i)e^{-Q(\lambda_i)/2}\pi_k(\lambda_j)e^{-Q(\lambda_j)/2}}{h_k}\bigg)\\\&lt;br&gt;
&amp;amp;:=\frac{1}{n!}\det(K(\lambda_i,\lambda_j)).
\end{align*}
The function
\begin{equation}
K(\lambda,\mu) := \sum_{k=0}^{n-1} \frac{\pi_k(\lambda)\pi_k(\mu)}{h_k}e^{-Q(\lambda)/2}e^{-Q(\mu)/2}
\end{equation}
is called the &lt;em&gt;Christoffel-Darboux&lt;/em&gt; kernel; it acts as a projector onto the span of the first $n$ orthogonal
polynomials, multiplied by $e^{Q/2}$. If $m &amp;lt; n$, integrating out the last $(n-m)$
variables, one finds that the joint density of the first $m$ eigenvalues is
\begin{equation}
\rho_n(\lambda_1,\cdots,\lambda_m) = \frac{(n-m)!}{n!} \det(K(\lambda_i,\lambda_j))_{i,j = 1}^m.
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;3-coulomb-gas-formalism&#34;&gt;$3.$ Coulomb Gas Formalism.&lt;/h2&gt;
&lt;p&gt;In random matrix theory, one is often interested in the large n (&lt;em&gt;planar&lt;/em&gt;) limit of certain quantities; by the
previous section, we are equivalently looking for asymptotic formulae for orthogonal polynomials. One technique
for addressing such questions is the coulomb gas method. For sake of clarity, we will focus our attention on one
particular object, and its asymptotics: the &lt;em&gt;free energy&lt;/em&gt;:
\begin{align*}
F = \frac{1}{n^2}\log Z_n = \frac{1}{n^2} \log \int_{\mathbb{R}^n} V(\lambda_1,\cdots,\lambda_n)
e^{-n\sum_{i=1}^nQ(\lambda_i)} d\lambda_1\cdots d\lambda_n.
\end{align*}
Moving the Vandermonde determinant into the exponent, we can rewrite the free energy as
\begin{align*}
\frac{1}{n^2} \log  \int_{\mathbb{R}^n} e^{-n^2 E(\lambda_1,\cdots,\lambda_n)}d\lambda_1\cdots d\lambda_n,
\end{align*}
where
\begin{equation}
E(\lambda_1,\cdots,\lambda_n) := \frac{1}{n^2}\sum_{i\neq j} \log \frac{1}{\lambda_i - \lambda_j} +
\frac{1}{n}\sum_{i=1}^n Q(\lambda_i).
\end{equation}
The above sum can be interpreted as an energy functional for a system of $n$ charges interacting via the $2D$ Coulomb
potential. The fact that the $\lambda_i&amp;rsquo;s$ are real translates to the condition that the charges are confined to live
on a wire (the real line). The charges all have mass $1/n$, and all have the same sign, and so the affect of the
coulomb interaction is repulsive. On the other hand, the charges sit in a background confining potential $Q(\lambda)$,
which holds them together. Note that both terms in $E(\lambda_1,\cdots,\lambda_n)$ are of order 1, by how we scaled
$Q\to nQ$.&lt;/p&gt;
&lt;p&gt;With this interpetation in mind, let us turn our attention back to the free energy. When $n$ is very large, typical
saddle point analysis tells us that we expect the dominant contributions to the free energy to come from the minima of
$E$, i.e.
\begin{align*}
F &amp;amp;= \frac{1}{n^2} \log \int_{\mathbb{R}^n} e^{-n^2E(\lambda_1,\cdots\lambda_n)} d\lambda_1\cdots d\lambda_n\\\&lt;br&gt;
&amp;amp;\sim \frac{1}{n^2} \log e^{-n^2 \min E(\lambda_1,\cdots\lambda_n)}\\\&lt;br&gt;
&amp;amp;= \min E(\lambda_1,\cdots\lambda_n).
\end{align*}
Writing the density of charges as a measure $\mu = \frac{1}{n}\sum_{k=1}^n \delta_{{\lambda_{k}}}$, we can rewrite this
minimization problem as
\begin{align*}
\min E(\lambda_1,\cdots\lambda_n) &amp;amp;= \min \frac{1}{n^2}\sum_{i\neq j} \log \frac{1}{\lambda_i - \lambda_j} +
\frac{1}{n}\sum_{i=1}^n Q(\lambda_i)\\\&lt;br&gt;
&amp;amp;= \min_{\mu(\mathbb{R}) = 1} \int\int \log\frac{1}{x-y} d\mu(x) d\mu(y) + \int Q(x) d\mu(x).
\end{align*}
As $n \to \infty$, we expect that the charges should coalesce into a continuous distribution of charge $\mu$, i.e.,
$ \frac{1}{n}\sum_{k=1}^n \delta_{{\lambda_{k}}} \to \rho(x) dx$, where $\rho$ has total charge $1$. This sort of
equilibrium problem is well-known in potential theory; we can use our physical intuition to try and find its
solution. In equilibrium, the effective potential (the potential coming from the charges themselves, plus the
external field) should be constant on the support of the charges, otherwise the charges would move around, and we
wouldn&amp;rsquo;t be in equilibrium. This yields the following equilibrium condition:
\begin{equation}
U^\mu(x) + Q(x) = \text{const.},
\end{equation}
for $x$ in the support of the charges. Here, $U^\mu(x)$ is the potential given off by the charges:
\begin{equation}
U^\mu(x) = \int \log\frac{1}{x-y} d\mu(y).
\end{equation}
Differentiating the equilibrium condition, we obtain the equation
\begin{equation}
C^\mu(x) + Q&#39;(x) = 0,
\end{equation}
where $C^\mu(x)$ is the &lt;em&gt;Cauchy transform&lt;/em&gt; of the measure $\mu$:
\begin{equation}
C^\mu(x) = \int \frac{d\mu(y)}{y-x},
\end{equation}
considered here in the principal value sense. We are almost at our goal; all that remains is to recover the measure
$\mu$ from the above formula, and then plug it back in to the energy functional, and then we have our expression
for the dominant contribution to the free energy. But how do we recover a measure if we know its Cauchy transform?
Luckily, this question has already been answered for us; these are the famous Sokhotsky-Plemelj formulae:
\begin{align*}
\frac{d\mu(x)}{dx} &amp;amp;= \frac{1}{2\pi i}\bigg( C^\mu_+(x) - C^\mu_-(x)\bigg),\\\&lt;br&gt;
P.V.\int \frac{d\mu(y)}{y-x} &amp;amp;= C^\mu_+(x) + C^\mu_-(x),
\end{align*}
for $x$ in the support of the charges. These formulae, along with a little knowledge of complex analysis, will allow
us to recover $d\mu(x) = \rho(x)dx$. Consider the function $P(z):=Q&#39;(z)C^{\mu}(z) - (C^\mu(z))^2$. This function is
analytic everywhere in the complex plane, except possibly at the places where $C^\mu(z)$ loses analyticity&amp;ndash;the support
of the charges. Let us compute the jump of $P(z)$ across the support:
\begin{align*}
P_+(z) - P_-(z) &amp;amp;= Q&#39;(z)\bigg(C^{\mu}_+(z)-C^{\mu}_-(z)\bigg) - (C^\mu_+(z))^2 + (C^\mu_-(z))^2\\\&lt;br&gt;
&amp;amp;= \bigg(C^{\mu}_+(z)+C^{\mu}_-(z)\bigg)\bigg(C^{\mu}_+(z)-C^{\mu}_-(z)\bigg)- (C^\mu_+(z))^2 + (C^\mu_-(z))^2\\\&lt;br&gt;
&amp;amp;=0,
\end{align*}
and so $P(z)$ is continuous across the support of the charges, and thus continuous everywhere. By Morera&amp;rsquo;s theorem,
$P(z)$ is an entire function. Furthermore, we know the exact growth rate of $P(z)$ at infinity:
$P(z) = \mathcal{O}(z^{n-2})$, where $\deg Q = n$. Thus, by Liouville&amp;rsquo;s theorem, $P(z)$ &lt;em&gt;is&lt;/em&gt; a polynomial
of degree $n-2$. We thus have obtained an algebraic equation for $C^\mu(z)$:
\begin{equation}
(C^\mu(z))^2 - Q&#39;(z)C^{\mu}(z) - P(z) = 0;
\end{equation}
this equation is quadratic in $C^\mu(z)$, and thus can be solved:
\begin{equation}
C^{\mu}(z) = -\frac{1}{2}Q&#39;(z) \pm \frac{1}{2}\sqrt{Q&#39;(z)^2 - 4P(z)
\end{equation}
Therefore, by the other half of the Sokhotsky-Plemelj formulae, we can recover the density:
\begin{equation}
\rho(x) = \frac{d\mu(x)}{dx} = \frac{1}{2\pi} \sqrt{4P(x) - Q&#39;(x)^2}.
\end{equation}
In practice, computing the coefficients of $P(x)$ is not a trivial task, especially if the support of the measure is
more than one interval. However, if the potential $Q(x)$ is partiularly simple, the computations can be done rather
quickly; if $Q(x) = \frac{1}{2}x^2$, as it is for the GUE, then it is easy to see that $P(x) = 1$, and so the formula
above becomes
\begin{equation}
\rho(x) = \frac{1}{2\pi}\sqrt{4-x^2};
\end{equation}
this is &lt;em&gt;Wigner&amp;rsquo;s semicircle law&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;4-2-matrix-models&#34;&gt;$4.$ 2-Matrix Models.&lt;/h2&gt;
&lt;p&gt;We now discuss $N$-matrix models, which are matrix models $n$ over copies of $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt;; for simplicity, we specialize to the case of $N = 2$.
We denote the matrices in the first copy of $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; by $A$, and in the second copy by $B$. The probability measure we consider here is
\begin{equation*}
d\mathbb{P}_n(A,B) = \frac{1}{Z_n} \exp\bigg( -\frac{n}{2} \text{tr} (A^2 + B^2 + 2cAB)\bigg) dA dB.
\end{equation*}
The quantity in the exponent can be thought of as a quadratic form on pairs of Hermitian matrices $(A,B)$, with defining matrix
\begin{equation*} \label{quad-form}
\begin{pmatrix}
1 &amp;amp; c \\\&lt;br&gt;
c &amp;amp; 1
\end{pmatrix}.
\end{equation*}
Expected values are denoted in the same way as before; this ensemble is also Gaussian in nature, and so the matrix version of Wick&amp;rsquo;s theorem
applies. As before, all one-point functions vanish identically. Thus, the relevant expected values to compute are the two-point functions; they are:
\begin{align*}
\langle A_{i_1j_1}A_{i_2j_2}\rangle_n = \langle B_{i_1j_1}B_{i_2j_2}\rangle_n
&amp;amp;= \frac{1}{n}\frac{1}{1-c^2} \delta_{i_1 j_2}\delta_{i_2 j_1},\\\&lt;br&gt;
\langle A_{i_1j_1}B_{i_2j_2}\rangle_n &amp;amp;= \frac{1}{n}\frac{c}{1-c^2} \delta_{i_1 j_2}\delta_{i_2 j_1}.
\end{align*}
The computations are almost identical to the $1$-matrix case, and so we omit them here (the process involves the extra step of diagonalizing
the quadratic form above. The crucial point here is that the two point functions for &amp;ldquo;like&amp;rdquo; matrices are the same, and differ
from the two point function for the &amp;ldquo;mismatched&amp;rdquo; matrices by an overall constant. Therefore, the same sorts of combinatorial identites that
held before for the correlations now hold for graphs with two possible weights on the vertices.&lt;/p&gt;
&lt;p&gt;This connection of the 2-matrix model to enumeration of 2-colored graphs was exploited by V. Kazakov to answer questions
about the 
&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/0375960186904330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ising model on random planar graphs&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;5-itzykson-zuber-integral-over-the-unitary-group&#34;&gt;$5.$ Itzykson-Zuber Integral over the Unitary Group.&lt;/h2&gt;
&lt;p&gt;With this combinatorial interpretation of the model in mind, we would like to go about explicitly computing certain matrix integrals. To do this,
we will need to switch over to eigenvalue variables; this is not as trivial as a task as it was in the 1-matrix case. This is made apparent as
follows. Suppose we want to compute the expected value of some function $f(A,B) = f(\text{tr} A,\text{tr} B)$. This is written as
\begin{equation}
\langle f(A,B)  \rangle_n = \frac{1}{Z_n}\int_{A\in \mathcal{H}_n} \int_{B \in \mathcal{H}_n}dA dB \exp\bigg( -\frac{n}{2}
\text{tr} (A^2 + B^2 + 2cAB)\bigg)  f(A,B).
\end{equation}
Letting $A = U^\dagger X U$, $B = V^\dagger Y V$, where $X = \text{diag}(x_1,\cdots,x_n)$, $Y = \text{diag}(y_1,\cdots,y_n)$ are diagonal matrices, we see that the above is
\begin{align*}
=&amp;amp; \frac{1}{Z_n}\int_{X,Y\in \mathbb{R}^n} dX dY V(X)^2V(Y)^2f(X,Y) \exp\bigg( -\frac{n}{2} \text{tr} (X^2 + Y^2)\bigg)
\\\ &amp;amp;\times\int_{U,V \in U(n)/U_{diag}(n)} dU dV \exp\big(nc \text{tr} AB\big)
\end{align*}
We must pay careful attention to the integral
\begin{equation*}
\int_{U,V \in U(n)/U_{diag}(n)} dU dV \exp\big(nc \text{tr} AB\big) =  \int_{U,V \in U(n)/U_{diag}(n)} dU dV
\exp\big(nc \text{tr} U^\dagger X UV^\dagger Y V\big).
\end{equation*}
Making the change of variables in $V$: $W = UV^\dagger$, and integrating over $U$ first (and applying translation invariance of the Haar measure),
we see that the above integral becomes
\begin{equation}
\text{vol}(U(n)/U_{diag}(n)) \cdot \int_{W \in U(n)/U_{diag}(n)} dW \exp\big(nc \text{tr}  X W Y W^\dagger\big),
\end{equation}
where $X,Y$ are diagonal matrices, and $W$ is unitary. Integrals of this type are called &lt;em&gt;Harish-Chandra&lt;/em&gt; or
&lt;em&gt;Itzykson-Zuber&lt;/em&gt;
integrals over the unitary group. It turns out that such integrals can be computed explicitly. We will consider here a slight generalization of the
above:
\begin{equation}
I(A,B;s) := \int_{U(n)/U_{diag}(n)} dU \exp\big(s \text{tr}  A U B U^\dagger\big),
\end{equation}
where $A, B$ are Hermitian matrices. Our claim is the following:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 5.1.&lt;/strong&gt;&lt;/span&gt;
Let $I(A,B;s)$ be as defined above, and suppose $A$ has eigenvalues $a_1,\cdots, a_n$, and $B$ has eigenvalues $b_1,\cdots, b_n$. Then
\begin{equation}
I(A,B;s) = s^{-n(n-1)/2} \frac{\det(e^{a_ib_j})}{V(a) V(b)}.
\end{equation}&lt;/p&gt;
&lt;p&gt;The remainder of this section is dedicated to a proof of this fact. We use the heat kernel technique, which is outlined
in 
&lt;a href=&#34;https://arxiv.org/abs/math-ph/0209019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a wonderful paper&lt;/a&gt; by P. Zinn-Justin and J.B. Zuber.
Consider the heat equation in the variable $A$:
\begin{equation}\label{heat-eq-1}
\begin{cases}
\left( \frac{\partial}{\partial t} - \frac{1}{2} \Delta_A \right) u(A,t) = 0,\\\&lt;br&gt;
u(A,0) = f(A).
\end{cases}
\end{equation}
Here, $\Delta_A$ is the usual Laplace operator on $\mathcal{H}_n$:
\begin{equation*}
\Delta_{A} = \sum_{i=1}^n \frac{\partial^2}{\partial A_{ii}^2} + \frac{1}{2}\sum_{i &amp;lt; j}\left(\frac{\partial^2}{\partial \text{Re}(A_{ij})^2} +
\frac{\partial^2}{\partial \text{Im}(A_{ij})^2}\right).
\end{equation*}
The heat kernel for this operator is easily derived (since $\mathcal{H}_n \cong \mathbb{R}^{n^2}$ in scaled coordinates):
\begin{equation*}
K(A,B;t) = \frac{1}{(2\pi t)^{n^2/2}}\exp\left(\frac{1}{2t} \text{tr}(A-B)^2\right);
\end{equation*}
The solution to the heat equation above is thus given by
\begin{equation*}
u(A,t) = \int_{\mathcal{H}_n} dB f(B) \frac{1}{(2\pi t)^{n^2/2}}\exp\left(-\frac{1}{2t} \text{tr}(A-B)^2\right).
\end{equation*}
Let us suppose the initial data $f(A)$ depends only on the eigenvalue
variables, i.e. $f(A) = f(\text{tr}(A))$. Then, converting the above to eigenvalue variables $A = U^\dagger X U$, $B = V^\dagger Y V$, we find that
\begin{align*}\label{heat2}
u(A,t) = &amp;amp;\frac{1}{(2\pi t)^{n^2/2}}\int_{\mathbb{R}^n} dY, V^2(Y)f(Y) \exp\left(-\frac{1}{2t} \text{tr}(X^2 + Y^2) \right)
\\\ &amp;amp;\times\int_{U(n)/U_{diag}(n)} dW \exp\big(\frac{1}{t} \text{tr} X W Y W^\dagger\big),
\end{align*}
where $W = UV^\dagger$. Note that the inner integral is the object we want to compute. Furthermore, the above formula implies that if
$f(U^\dagger X U) = f(X)$, then $u(U^\dagger X U,t) = u(X,t)$ for all $t &amp;gt; 0$ as well. Let us see examine the above equation in eigenvalue
coordinates. With the help of the formula we derived for the laplacian on the eigenvalue coordinates, we find that
\begin{align*}
0 &amp;amp;= \left( \frac{\partial}{\partial t} - \frac{1}{2} \Delta_A \right) u(A,t)
= \left( \frac{\partial}{\partial t} - \frac{1}{2} \Delta_{X} \right) u(X,t)\\\&lt;br&gt;
&amp;amp;= \frac{\partial u }{\partial t} - \frac{1}{2}\frac{1}{V^2(X)}\sum_{i=1}^n\frac{\partial}{\partial x_i}
\left(V^2(X) \frac{\partial u}{\partial x_i} \right)\\\&lt;br&gt;
&amp;amp;= \frac{\partial u }{\partial t} - \frac{1}{2}\frac{1}{V^2(X)}\sum_{i=1}^n \left(2V(X)\frac{\partial V}{\partial x_i}  \frac{\partial u}{\partial x_i}  + V^2(X) \frac{\partial^2 u}{\partial x_i^2}\right);
\end{align*}
Multiplying the above equation by the Vandermonde determinant $V(x)$, and using the fact that $\sum_{i=1}^n \frac{\partial^2 V}{\partial x_i^2}= 0$, the above equation becomes
\begin{align*}
0 &amp;amp;= \frac{\partial V u }{\partial t} - \frac{1}{2}\sum_{i=1}^n \left(2\frac{\partial V}{\partial x_i}
\frac{\partial u}{\partial x_i}  + V(X)\frac{\partial^2 u}{\partial x_i^2}\right)\\\&lt;br&gt;
&amp;amp;= \frac{\partial V u }{\partial t} - \frac{1}{2}\sum_{i=1}^n \left(\underbrace{u\frac{\partial^2
V}{\partial x_i^2}}_{=0} + 2\frac{\partial V}{\partial x_i}  \frac{\partial u}{\partial x_i}  +
V(X)\frac{\partial^2 u}{\partial x_i^2}\right)\\\&lt;br&gt;
&amp;amp;= \left(\frac{\partial}{\partial t} - \frac{1}{2} \sum_{i=1}^n \frac{\partial^2}{\partial x_i^2}\right)V(X)u(X,t),
\end{align*}
and so $V(x)u(X,t)$ satisfies a heat equation of a different form. The generic solution for initial data $g(X)$ to the above heat equation is
\begin{equation*}
h(X,t) = \int_{Y\in \mathbb{R}^n} dY \frac{1}{(2\pi t)^{n/2}}\exp\left(-\frac{1}{2t}\text{tr}(X-Y)^2\right) g(Y);
\end{equation*}
In particular, for initial data $g(X) = V(X)f(X)$, we obtain the
formula
\begin{equation}\label{heat3}
V(X)u(X,t) = \int_{Y\in \mathbb{R}^n} dY \frac{1}{(2\pi t)^{n/2}}\exp\left(-\frac{1}{2t}\text{tr}(X-Y)^2\right) V(Y)f(Y)
\end{equation}
multiplying the previous equation by $V(X)$, we see that we have two expressions for the same function, $V(X)u(X,t)$. Equating these
expressions yields
\begin{align*}
&amp;amp;V(X)\frac{1}{(2\pi t)^{n^2/2}}\int_{\mathbb{R}^n} dY V^2(Y)f(Y) \exp\left(-\frac{1}{2t} \text{tr}(X^2 + Y^2) \right)
\int_{U(n)/U_{diag}(n)} dW \exp\big(\frac{1}{t} \text{tr} X W Y W^\dagger\big)\\\&lt;br&gt;
&amp;amp;=  u(X,t)=\int_{Y\in \mathbb{R}^n} dY \frac{1}{(2\pi t)^{n/2}}\exp\left(-\frac{1}{2t}\text{tr}(X-Y)^2\right) V(Y)f(Y).
\end{align*}
Setting $f(Y) = \delta(Y-Y_0)$, we obtain that
\begin{align*}
&amp;amp;V(X)V^2(Y_0)\frac{1}{(2\pi t)^{n^2/2}}\exp\left(-\frac{1}{2t} \text{tr}(X^2 + Y_0^2) \right)
\int_{U(n)/U_{diag}(n)} dW \exp\big(\frac{1}{t} \text{tr} X W Y_0 W^\dagger\big)\\\&lt;br&gt;
&amp;amp;=   \frac{1}{(2\pi t)^{n/2}}\exp\left(-\frac{1}{2t}\text{tr}(X-Y_0)^2\right) V(Y_0).
\end{align*}
Rearranging, and relabelling $Y_0 = Y$, we get
\begin{equation*}
\int_{U(n)/U_{diag}(n)} dW \exp\big(\frac{1}{t} \text{tr} X W Y W^\dagger\big) =
(2\pi t)^{-n(n-1)/2}\frac{\exp(\frac{1}{t}\text{tr} XY)}{V(Y) V(X)};
\end{equation*}
finally,
\begin{equation}
\int_{U(n)/U_{diag}(n)} dW \exp\big(\frac{1}{t} \text{tr} X W Y W^\dagger\big) =
(2\pi t)^{-n(n-1)/2}\frac{\det e^{\frac{1}{t}x_iy_j }}{V(Y) V(X)}.
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;6-2-matrix-models-and-biorthogonal-polynomials&#34;&gt;$6.$ 2-Matrix Models and Biorthogonal Polynomials.&lt;/h2&gt;
&lt;p&gt;Just as $1$-matrix models are connected to orthogonal polynomials, $2$-matrix models are connected to biorthogonal polynomials. Consider again
the $2$-matrix model defined by the measure
\begin{equation*}
d\mathbb{P}_n(A,B) = \frac{1}{Z_n} \exp\bigg( -\frac{n}{2} \text{tr} (A^2 + B^2 + 2cAB)\bigg) dA dB,
\end{equation*}
and recall that expected values of invariant functions $f(A,B) = f(\text{tr}(A),\text{tr}(B))$ are given by the formula
\begin{equation*}
\langle f(A,B)  \rangle_n = \frac{1}{Z_n}\int_{A\in \mathcal{H}_n} \int_{B \in \mathcal{H}_n}dA dB \exp\bigg( -\frac{n}{2}
\text{tr} (A^2 + B^2 + 2cAB)\bigg)  f(A,B).
\end{equation*}
Letting $A = U^\dagger X U$, $B = V^\dagger Y V$, where $X = \text{diag}(x_1,\cdots,x_n)$, $Y = \text{diag}(y_1,\cdots,y_n)$ are diagonal matrices, we
can integrate out eigenvalue variables to obtain
\begin{align*}
&amp;amp;= \frac{1}{Z_n&#39;}\int_{X,Y\in \mathbb{R}^n} dX dY V(X)^2V(Y)^2f(X,Y) \exp\bigg( -\frac{n}{2} \text{tr} (X^2 + Y^2)\bigg)
\\\ &amp;amp;\times\int_{U \in U(n)/U_{diag}(n)} dW \exp\big(nc \text{tr} XWYW^\dagger\big).
\end{align*}
The integral inside is precisely the Itzykson Zuber integral we computed previously; we insert the expression we derived for it into the
above formula, absorbing all constant factors into the partition function:
\begin{align*}
\langle f(A,B)  \rangle_n &amp;amp;= \frac{1}{Z_n&#39;&#39;}\int_{X,Y\in \mathbb{R}^n} dX dY V(X)^2V(Y)^2f(X,Y) \exp\bigg(
-\frac{n}{2} \text{tr} (X^2 + Y^2)\bigg)
\frac{\det e^{nc x_iy_j }}{V(Y) V(X)}\\\&lt;br&gt;
&amp;amp;=\frac{1}{Z_n&#39;&#39;}\int_{X,Y\in \mathbb{R}^n} \prod_{k=1}^n dx_k dy_k f(x,y) V(x)V(y)\exp\bigg( -\frac{n}{2} \sum_{k=1} x_i^2  + y_i^2 + 2ncx_i y_i\bigg).
\end{align*}&lt;/p&gt;
&lt;p&gt;Thus, just as we were able to connect the 1-matrix model and orthogonal polynomials, we can connect the 2-matrix
to &lt;em&gt;bi-orthogonal polynomials&lt;/em&gt;; i.e., the polynomials $p_n(x),q_n(y)$ satisfying
\begin{align*}
\int_{\mathbb{R}} \int_{\mathbb{R}} p_n(x)q_n(y) e^{-\frac{n}{2}x^2-\frac{n}{2}y^2-2nxy} dxdy = h_n \delta_{nm}
\end{align*}
Of course, the same sorts of arguments can be made for more general potentials besides $x^2,y^2$; we postpone discussion
of this to a later entry.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Probability Theory</title>
      <link>/talk/introprobability1-louis/</link>
      <pubDate>Sat, 28 Aug 2021 16:08:51 -0400</pubDate>
      <guid>/talk/introprobability1-louis/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Introduction to Probability Theory&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Louis Arenas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: Friday, September 3rd&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 2:00pm-3:00pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 1:&lt;/strong&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/pUfZzRKuXqM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 2:&lt;/strong&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/BQkheSwlQhw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: In this talk, we reinterpret the concepts from measure theory under the lens of probability theory. Using concrete examples, the aim is to introduce important concepts in probability theory and interpret their statistical meaning and connection to measure theory. This first talk aims to build an intuitive idea of the machinery used in probability theory to prove weak and strong law of large numbers, a theorem often taken for granted in applied statistics. In the second part we hope to motivate central limit theorem, and give a proof using characteristic functions. The target audience are students who have limited exposure to measure theory (statistics students), and students who are currently studying real analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Helton Howe Trace Formula</title>
      <link>/posts/the-helton-howe-trace-formula/</link>
      <pubDate>Sat, 01 May 2021 15:19:15 -0400</pubDate>
      <guid>/posts/the-helton-howe-trace-formula/</guid>
      <description>&lt;p&gt;We present a proof of a simple version of the Helton-Howe trace
formula (for a more general version of this result, one should consult
the 
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/BFb0058919&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original paper&lt;/a&gt;).
Consider the Hardy space $H^2(\mathbb{D})$. There is a natural
inclusion of $H^2(\mathbb{D})$ into $L^2(\mathbb{T})$, given by the restriction
$f\mapsto f\vert_{\mathbb{T}}$. Let $\mathbb{P}_{H^2} : L^2(\mathbb{T}) \to H^2(\mathbb{D})$ denote the
orthogonal projection of $L^2(\mathbb{T})$ into $H^2(\mathbb{D})$. Now, given
$P \in \mathbb{C}[z,z^{-1}]$, we define the Toeplitz operator with principal
symbol $P$ by $$T_P f(z) = \mathbb{P}_{H^2} \left[ P(z)f(z)\right],$$ for
$f \in H^2(\mathbb{D})$. It is clear that the commutator of any two such
operators operators is trace-class (it is even of finite rank). The
Helton-Howe formula tells us that, given $P,Q \in \mathbb{C}[z,z^{-1}]$,
$$\text{tr } [T_P, T_Q] = \frac{1}{2\pi i}\int_{|z| = 1} P(z) Q&#39;(z) dz = \frac{1}{2\pi i}\int_{\mathbb{D}} dP \wedge dQ$$
The last equality is immediate from Stokes&#39; theorem; the interesting
equality is the first. Let $T_z$ denote the Toeplitz operator of
multiplication by $z$ (this is just the shift operator in the Hardy
space), and $T^*_z$ its adjoint (note that $T^*_z = T_{1/z}$). If
$P = \sum_k P_k z^k \in \mathbb{C}[z,z^{-1}]$, then its corresponding Toeplitz
operator may be written as
$$T_P = \sum_{k\geq 0} P_k T_z^k + \sum_{k&amp;lt;0} P_k {T_z^{*}}^k.$$
Clearly, ${T_z^*}^n{T_z}^n = I$; on the other hand,
${T_z}^n{T_z^*}^n = I - \mathbb{P}_n$, where $\mathbb{P}_n$ is the orthogonal
projector onto the subspace $E_n:=\text{span}$ {$1,z, &amp;hellip;,z^{n-1}$}.
This observation allows us to compute that
$\text{tr }[{T_z^*}^n,{T_z}^n] = \text{tr } \mathbb{P}_n = n$. Now, if $n \neq m$, one finds
by direct computation that $\text{tr }[{T_z^*}^m,{T_z}^n] = 0$. Thus,
$\text{tr }[{T_z^*}^m,{T_z}^n] = n\delta_{n,m}$. If $P, Q \in \mathbb{C}[z,z^{-1}]$,
and $P = \sum_k P_k z^k$, $Q = \sum_k Q_k z^k$, then
$$= \sum_{n,m \geq 0} (P_{-n}Q_m - Q_{-n}P_m) [{T_z^*}^n,{T_z}^m];$$
taking traces yields $$\begin{aligned}
\text{tr } [T_P,T_Q] &amp;amp;= \sum_{n,m \geq 0} (P_{-n}Q_m - Q_{-n}P_m) \text{tr } [{T_z^*}^n,{T_z}^m] \\\&lt;br&gt;
&amp;amp;= \sum_{n,m \geq 0} (P_{-n}Q_m - Q_{-n}P_m) n\delta_{n,m} = \sum_{n\geq 0} n(P_{-n}Q_n - Q_{-n}P_n)\\\&lt;br&gt;
&amp;amp;= \sum_{n}n P_{-n}Q_n.
\end{aligned}$$ On the other hand, by residue theorem,
$$\begin{aligned}
\frac{1}{2\pi i}\int_{|z| = 1} P(z) Q&#39;(z) dz
&amp;amp;= \frac{1}{2\pi i}\int_{|z| = 1} \sum_{\ell}\left(\sum_n nP_{\ell - n} Q_n \right) z^{\ell-1} dz\\\&lt;br&gt;
&amp;amp;= \sum_{\ell}\left(\sum_n nP_{\ell - n} Q_n \right) \delta_{\ell,0}\\\&lt;br&gt;
&amp;amp;= \sum_{n}n P_{-n}Q_n.
\end{aligned}$$ Comparing the results of these two computations
concludes the proof.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working Differential Geometry</title>
      <link>/posts/working-differential-geometry/</link>
      <pubDate>Sat, 01 May 2021 12:52:29 -0400</pubDate>
      <guid>/posts/working-differential-geometry/</guid>
      <description>&lt;p&gt;We present a methodical procedure for computing important geometric quantities on a Riemannian manifold. We want to emphasize that the purpose of this document is not meant to provide a systematic introduction to Riemannian geometry, or motivate/give intuition for the definitions (although we will occasionally give quick consistency checks); rather, our aim is to provide a formalism that makes the objects of interest (Christoffel symbols, Riemann tensor, etc.) &amp;ldquo;manageable&amp;rdquo;. This method is not the most streamlined (see, for example, E. Cartan&amp;rsquo;s theory of moving frames, which is arguably easier to work with; furthermore, if the manifold in question is a Lie group equipped with an invariant metric, the required computations reduce drastically), it is certainly friendlier than what is found in most classical differential geometry texts. Furthermore, most of these techniques carry over to vector and principal bundles, equipped with a connection, and so what follows may also be readily applied there. Before beginning the exposition, we summarize the objectives of this document:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Compute several examples of Riemannian metrics.
2. Compute the Levi-Cevita connection in terms of the metric.
3. Compute the Riemann tensor in terms of the connection coefficients.
4. Compute the Ricci and scalar curvatures in terms of the Riemann tensor, 
as well as the Einstein tensor (in some special cases).
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;1-riemannian-metrics&#34;&gt;$1$. Riemannian metrics.&lt;/h2&gt;
&lt;p&gt;We will always start by fixing a local coordinate patch $(U,x)$, and work within this coordinate patch. For simplicity in our examples, we will choose manifolds/metrics which admit global coordinates. These examples will be carried throughout the text and built upon, in order to show the utility of the methods we present.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Sphere $S^2$.&lt;/em&gt;
Most commonly (and often most intuitively), a manifold is endowed with the metric arising from the restriction Euclidean metric in its embedding space to the manifold. In plain English, this means that distances are measured as they are in the embedding space, but with the additional restriction that we are required to only measure distances restricted to the surface of the space of interest. As a first example, we consider the sphere $S^2 \subset \mathbb{R}^3$. The sphere admits coordinates
$$
x = \sin \theta \cos \varphi, \qquad
y = \sin \theta \sin \varphi, \qquad
z = \cos \theta, \&lt;br&gt;
$$
where $0 \leq \theta \leq \pi, 0 \leq \varphi \leq 2 \pi$. &lt;em&gt;(Note: We are using the standard physics notation for spherical coordinates; we apologize to anyone used to the other notation, and hope it won&amp;rsquo;t cause too much confusion.&lt;/em&gt; In these coordinates, the differentials $dx$, $dy$, and $dz$ may be computed readily:
$$dx = \cos\theta \cos\varphi d\theta - \sin\theta \sin\varphi d\varphi,$$
$$dy = \cos\theta \sin\varphi d\theta + \sin\theta \cos\varphi d\varphi,$$
$$dz = -\sin\theta d\theta.$$
Taking tensor products of each coordinate is a little more tedious, but is still a feasible task (we will write expressions like $dx^2$ or $dxdy$ instead of $dx\otimes dx$ or $dx\otimes dy$, to simplify notations):
$$dx^2 = \cos^2\theta\cos^2\varphi d\theta^2 + \sin^2\theta\sin^2\varphi d\varphi^2
- \cos\theta \sin\theta \cos \varphi \sin\varphi d\theta d\varphi$$
$$ - \cos\theta \sin\theta \cos \varphi \sin\varphi  d\varphi d\theta,$$
$$dy^2 = \cos^2\theta\sin^2\varphi d\theta^2 + \sin^2\theta\cos^2\varphi d\varphi^2$$
$$+ \cos\theta \sin\theta \cos \varphi \sin\varphi d\theta d\varphi + \cos\theta \sin\theta \cos \varphi \sin\varphi  d\varphi d\theta,$$
$$dz^2 = \sin^2\theta d\theta^2.$$
Summing these three terms, we obtain the restriction of the Euclidean metric $g = dx^2 + dy^2 + dz^2$ in $\mathbb{R}^3$ to the sphere. We see that the &amp;ldquo;cross terms&amp;rdquo; (the expressions containing $d\theta d\varphi$ and $d\varphi d\theta$) cancel, and repeated use of trigonometric identities on the remaining terms yields the expression
$$ g\vert_{S^2} = d\theta^2 + \sin^2\theta d\varphi^2. $$
Instead of numbering the coordinates (which will make the notation cumbersome), we will simply use the coordinates themselves as indices, and subsequent sums over indices will be taken over these symbols. This is less complicated than it sounds: For example, the metric above has components: $g_{\theta\theta} = 1$, $g_{\theta\varphi} = g_{\varphi\theta} = 0$, and $g_{\varphi\varphi} = \sin^2\theta$. We will use a similar notation for the inverse metric tensor $g$, but with the indices in the upper position. Since $g$ is diagonal, it is easy to compute the inverse metric tensor: $g^{-1}$ has components $g^{\theta\theta} = 1$, $g^{\theta\varphi} = g^{\varphi\theta} = 0$, and $g^{\varphi\varphi} = 1 / \sin^2\theta$.
**Exercise.** Compute the metric arising from the embedding of the 2-sphere in $\mathbb{R}^3$ by stereographic projection:
\begin{equation}
x = \frac{2X}{1+X^2+Y^2}, \qquad
y = \frac{2Y}{1+X^2+Y^2}, \qquad
z = \frac{-1+X^2+Y^2}{1+X^2+Y^2},
\end{equation}
with $(X,Y) \in \mathbb{R}^2$. (Note: Technically, two stereographic projection charts are required to cover the sphere (say, one from the north pole and one from the south), but it can be checked that the metric arising from these projections has the same form in both charts. Therefore, it is enough to consider only one.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Solution:&lt;/em&gt;
\begin{equation} \label{stereographicmetric}
g|_{S^2} = \frac{4}{(1+X^2+Y^2)^2} (dX^2 + dY^2)
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Torus $\mathbb{T}^2$.&lt;/em&gt;
We apply the same procedure as before to the torus $\mathbb{T}^2$ embedded in $\mathbb{R}^3$. The torus (with $R&amp;gt;r$ being the larger and smaller radii, respectively) has coordinates
\begin{equation}
x = (R + r\cos\theta)\cos\varphi, \qquad
y = (R + r\cos\theta)\sin\varphi, \qquad
z = r\sin\theta,
\end{equation}
where $0 \leq \theta, \varphi \leq 2 \pi$. The differentials of these coordinates are then
\begin{align}
dx &amp;amp;= -r\sin\theta\cos\varphi d\theta - (R + r\cos\theta)\sin\varphi d\varphi,\\\&lt;br&gt;
dy &amp;amp;= -r\sin\theta\sin\varphi d\theta + (R + r\cos\theta)\cos\varphi d\varphi,\\\&lt;br&gt;
dz &amp;amp;= r\cos\theta d\theta.
\end{align}
Taking tensor products of each coordinate, we find that
\begin{aligned}
dx^2 &amp;amp;= r^2\sin^2\theta\cos^2\varphi d\theta^2 + (R + r\cos\theta)^2\sin^2\varphi d\varphi^2
+ r(R + r\cos\theta)\sin\theta\cos\varphi\sin\varphi d\theta d\varphi + \left(\cdot\right)d\varphi d\theta,\\\&lt;br&gt;
dy^2 &amp;amp;= r^2\sin^2\theta\sin^2\varphi d\theta^2 + (R + r\cos\theta)^2\cos^2\varphi d\varphi^2 - r(R + r\cos\theta)\sin\theta\sin\varphi\cos\varphi d\theta d\varphi - (\cdot) d\varphi d\theta,\\\&lt;br&gt;
dz^2 &amp;amp;= r^2\cos^2\theta d\theta^2.
\end{aligned}
(We have suppressed the coefficient of the $d\varphi d\theta$ terms, as they are the same as the coefficient of the $d\theta d\varphi$ terms in both cases). Summing, we see that the &amp;ldquo;cross-terms&amp;rdquo; cancel, and what remains can be simplified via trigonometric identities. The metric then takes the form:
\begin{equation}
g\vert_{\mathbb{T}^2} = r^2 d\theta^2 + (R + r\cos\theta)^2 d\varphi^2.
\end{equation}
Thus, the metric is again diagonal, with nonzero components $g_{\theta\theta} = r^2$, $g_{\varphi \varphi} = (R + r\cos\theta)^2$. Since $g$ is diagonal, we can easily compute the inverse metric tensor: $g^{-1}$ has components $g^{\theta\theta} = 1/r^2$, $g^{\varphi\varphi} = 1 / (R + r\cos\theta)^2$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Upper Half Plane $\mathbb{H}$.&lt;/em&gt;
Of course, the metric on a Riemannian manifold need not come from the embedding space: a good example of this is the &lt;em&gt;hyperbolic metric&lt;/em&gt; on the upper half plane $\mathbb{H} :=$ {$(x,y)\in\mathbb{R}^2 : y &amp;gt; 0$}. This metric is defined as
\begin{equation}\label{hyperbolicmetric}
g\vert_{\mathbb{H}} = \frac{dx^2 + dy^2}{y^2} = \frac{|dz|^2}{\vert\text{Im} z\vert^2},
\end{equation}
where here $z = x+iy$. The nonzero components of the metric are $g_{xx} = g_{yy} = \frac{1}{y^2}$, and the nonzero components of the inverse metric tensor are $g^{xx} = g^{yy} = y^2$. The relevance of this metric is that it is the unique metric on the upper half plane that is preserved under conformal automorphisms of the upper half plane (i.e., setting $w(z) = \frac{az+ b}{cz+d}$ with $a,b,c,d$ real, then $\frac{|dz|^2}{|\text{Im} z|^2} = \frac{|dw|^2}{|\text{Im} w|^2}$). We shall not discuss further properties of this metric here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Use the conformal map $w:\mathbb{D} \to \mathbb{H}$, defined by $w(z) = \frac{z+i}{iz+1}$, to pull back the metric above to the unit disc $\mathbb{D} := ${$z | |z| &amp;lt; 1$}. &lt;em&gt;Solution:&lt;/em&gt;
\begin{equation} \label{hyperbolicmetricD}
g\vert_{\mathbb{D}} = \frac{4|dz|^2}{(1-|z|^2)^2}  = \frac{4}{(1-x^2-y^2)^2} (dx^2 + dy^2).
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The 3-sphere $S^3$.&lt;/em&gt;
Consider the 3-sphere $S^3:=${$(x_0,x_1,x_2,x_3) \mid x_0^2 + x_1^2 + x_2^2 + x_3^2 = 1$}. We can parameterize the
$3$-sphere with the coordinates
\begin{equation}
\begin{cases}
x_0 &amp;amp;= \cos \psi,\\\&lt;br&gt;
x_1 &amp;amp;= \sin \psi \cos \theta,\\\&lt;br&gt;
x_2 &amp;amp;= \sin \psi \sin \theta \cos \varphi,\\\&lt;br&gt;
x_3 &amp;amp;= \sin \psi \sin \theta \sin \varphi,\\\&lt;br&gt;
\end{cases}
\end{equation}
Where $0 \leq \psi, \theta \leq \pi$, and $0 \leq \varphi \leq 2 \pi$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Use these coordinates to compute the embedding metric on $S^3$. &lt;em&gt;Solution:&lt;/em&gt;
\begin{equation} \label{3-sphere-metric}
g\vert_{S^3} = d\psi^2 + \sin^2 \psi d\theta^2 + \sin^2 \psi \sin^2 \theta d\varphi^2.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The left-invariant metric on $S^3 \cong SU(2)$ &amp;amp; Hopf coordinates.&lt;/em&gt;
Let us consider new coordinates on the sphere, called the &lt;em&gt;Hopf coordinates&lt;/em&gt;:
\begin{equation}
\begin{cases}
x_0 &amp;amp;= \cos \xi_1 \sin \eta,\\\&lt;br&gt;
x_1 &amp;amp;= \sin \xi_1 \sin \eta,\\\&lt;br&gt;
x_2 &amp;amp;= \cos \xi_2 \cos \eta,\\\&lt;br&gt;
x_3 &amp;amp;= \sin \xi_2 \cos \eta,\\\&lt;br&gt;
\end{cases}
\end{equation}
Where $0 \leq \eta \leq \frac{\pi}{2}$, $0 \leq \xi_1, \xi_2 \leq 2\pi$. It is easy to check that the embedding
metric in these coordinates is
\begin{equation} \label{SU2-metric}
g\vert_{S^3} = d\eta^2 + \sin^2 \eta d\xi_1^2 + \cos^2\eta d\xi_2^2.
\end{equation}
Putting $z_1 = x_0 + ix_1$, $z_2 = x_2 + ix_3$, we can write
\begin{equation}
U(\xi_1,\xi_2,\eta) =
\begin{pmatrix}
e^{i\xi_1}\sin\eta &amp;amp; -e^{-i\xi_2}\cos\eta\\\&lt;br&gt;
e^{i\xi_2}\cos\eta &amp;amp; e^{-i\xi_1}\sin\eta
\end{pmatrix}
=
\begin{pmatrix}
z_1 &amp;amp; -\bar{z}_2\\\&lt;br&gt;
z_2 &amp;amp; \bar{z}_1
\end{pmatrix},
\end{equation}
Which is the generic form of a matrix in $SU(2):=${$U \in GL_2(\mathbb{C}) \mid U^\dagger U = \mathbb{I}, \det U = 1$}. The fact that $(x_0,x_1,x_2,x_3)$ parameterize $S^3$ guarantees that the matrix $U(\xi_1,\xi_2,\eta)$ is unitary, and has unit determinant. This establishes a diffeomorphism between the sphere $S^3$ and the Lie group $SU(2)$. Because of the group structure of $S^3$, we now search for a metric which is invariant under the group action on itself. In fact, we shall see that, up to an overall constant, the unique left-invariant metric on $SU(2)$ is just the usual embedding metric (See the previous example). We make use of the fact that the metric above may be expressed
as $g = |dz_1|^2 + |dz_2|^2$. Let
\begin{equation}
W:= \begin{pmatrix}
w_1 &amp;amp; -\bar{w}_2\\\&lt;br&gt;
w_2 &amp;amp; \bar{w}_1
\end{pmatrix}
\end{equation}
Be another matrix in $SU(2)$, i.e. $|w_1|^2 + |w_2|^2 = 1$. Then, under a left translation,
\begin{equation}
W\cdot Z =
\begin{pmatrix}
w_1 &amp;amp; -\bar{w}_2\\\&lt;br&gt;
w_2 &amp;amp; \bar{w}_1
\end{pmatrix} &lt;br&gt;
\begin{pmatrix}
z_1 &amp;amp; -\bar{z}_2\\\&lt;br&gt;
z_2 &amp;amp; \bar{z}_1
\end{pmatrix}
=&lt;br&gt;
\begin{pmatrix}
w_1z_1-\bar{w}_2z_2 &amp;amp; -\overline{w_2z_1+\bar{w}_1z_2}\\\&lt;br&gt;
w_2z_1+\bar{w}_1z_2 &amp;amp; \overline{w_1z_1-\bar{w}_2z_2},
\end{pmatrix}
\end{equation}
i.e., the translated coordinates are
\begin{align}
\zeta_1 &amp;amp;= w_1z_1-\bar{w}_2z_2,\\\&lt;br&gt;
\zeta_2 &amp;amp;= w_2z_1+\bar{w}_1z_2.
\end{align}
Invariance of the metric under left translations is equivalent to checking that $|dz_1|^2 + |dz_2|^2 =
|d\zeta_1|^2 + |d\zeta_2|^2$. Indeed, we find that
\begin{align}
d\zeta_1 &amp;amp;= w_1dz_1-\bar{w}_2dz_2,\\\&lt;br&gt;
d\zeta_2 &amp;amp;= w_2dz_1+\bar{w}_1dz_2,
\end{align}
so that
\begin{align}
|d\zeta_1|^2 &amp;amp;= |w_1|^2|dz_1|^2 - \bar{w}_1\bar{w}_2dz_2 d\bar{z}_1 -
w_1 w_2 dz_1 d\bar{z}_2 + |w_2|^2 |dz_2|^2,\\\&lt;br&gt;
|d\zeta_2|^2 &amp;amp;= |w_2|^2|dz_1|^2 + \bar{w}_1\bar{w}_2dz_2 d\bar{z}_1 +
w_1 w_2 dz_1 d\bar{z}_2 + |w_1|^2 |dz_2|^2;
\end{align}
adding these expressions yields the identity $|dz_1|^2 + |dz_2|^2 = |d\zeta_1|^2 + |d\zeta_2|^2$.
**Exercise.**
Check that the vector fields
\begin{align}
\sigma_1 &amp;amp;:= \frac{1}{2} \left( -\tan\eta\cos(\xi_1+\xi_2)\frac{\partial}{\partial \xi_1}
+ \cot\eta\cos(\xi_1+\xi_2)\frac{\partial}{\partial \xi_2}+ \sin(\xi_1 + \xi_2) \frac{\partial}{\partial \eta}\right),\\\&lt;br&gt;
\sigma_2 &amp;amp;:= \frac{1}{2} \left(\tan\eta\sin(\xi_1+\xi_2)\frac{\partial}{\partial \xi_1} -
\cot\eta\sin(\xi_1+\xi_2)\frac{\partial}{\partial \xi_2} + \cos(\xi_1 + \xi_2) \frac{\partial}{\partial \eta}\right),\\\&lt;br&gt;
\sigma_3 &amp;amp;:= \frac{1}{2} \left( \frac{\partial}{\partial \xi_1} + \frac{\partial}{\partial \xi_2} \right),
\end{align}
satisfy the commutation relations $[\sigma_i, \sigma_j] = \epsilon_{ijk} \sigma_k$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Schwarzschild metric.&lt;/em&gt;
Riemannian geometry goes hand-in-hand with Einstein&amp;rsquo;s theory of general relativity. One of the first exact solutions to Einstein&amp;rsquo;s field equations is the &lt;em&gt;Schwarzschild solution&lt;/em&gt;:
\begin{equation}\label{Schwarzschildmetric}
g = -\left(1-\frac{r_s}{r}\right) dt^2 + \left(1-\frac{r_s}{r}\right)^{-1} dr^2 + r^2 d\theta^2 + r^2 \sin^2\theta d\varphi^2,
\end{equation}
which (as we shall see) describes a $\delta$-function mass at the origin of an otherwise empty spacetime. $g$ is a pseudo-Riemannian metric on $\mathbb{R}^4$, and has an apparent singularity at $r = r_s$, the &lt;em&gt;Schwarzschild radius&lt;/em&gt;. However, this &amp;ldquo;singularity&amp;rdquo; can be removed by an appropriate change of coordinates (see, for example, 
&lt;a href=&#34;http://www.fulviofrisone.com/attachments/article/486/Wald%20-%20General%20Relativity.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robert Wald&amp;rsquo;s book&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;2-the-levi-cevita-connection&#34;&gt;$2$. The Levi-Cevita Connection.&lt;/h2&gt;
&lt;p&gt;There are several ways to derive the Levi-Cevita connection (i.e., the unique connection on the tangent bundle which annihilates the metric and has vanishing torsion) from the metric; however, it is not our purpose to derive the Levi-Cevita connection, and so we will present the result here. The components of the connection, called the &lt;em&gt;Christoffel symbols&lt;/em&gt;, have three associated indices: $\Gamma^i_{jk}$; in terms of the metric, the component $\Gamma^i_{jk}$ is given by
\begin{equation} \label{Christoffel}
\Gamma^i_{jk} = \frac{1}{2}\sum_\ell g^{i\ell} \left( \partial_j g_{\ell k} + \partial_k g_{j \ell} -\partial_\ell g_{jk}\right).
\end{equation}
Here, $\partial_j = \frac{\partial}{\partial x_j}$, the derivative with respect to the $j^{th}$ local coordinate. It is worth committing the formula above to memory. Here are a few useful techniques for doing this: Looking closely at the indices, we see that the first two terms are obtained by switching one of the lower indices in $\Gamma^i_{jk}$ with $\ell$, and then differentiating with respect to this variable; note that both terms have a &amp;ldquo;$+$&amp;rdquo; sign in front of them. The last term is obtained by differentiating the component of the metric tensor with indices matching the lower indices of $\Gamma^i_{jk}$ with respect to the summation variable; note that this is the only term with a &amp;ldquo;$-$&amp;rdquo; coefficient. Finally, note that the only place that the upper index appears is in the inverse metric tensor expression.&lt;/p&gt;
&lt;p&gt;Some further remarks about the Christoffel symbols:&lt;/p&gt;
&lt;p&gt;$1$. Note that $\Gamma^i_{jk}$ is symmetric in the indices $j$ and $k$, as a consequence of the fact that $g_{ij} = g_{ji}$.&lt;/p&gt;
&lt;p&gt;$2$. If the metric is diagonal, i.e. $g_{ij} \neq 0 $ only if $i= j$, then $g^{ij}$ is also diagonal, and thus there is no sum over $\ell$ in the above equation: the Christoffel symbols are simply
\begin{equation} \label{simplechrist}
\Gamma^i_{jk} = \frac{1}{2} g^{ii} \left( \partial_j g_{ik} + \partial_k g_{ji} -\partial_i g_{jk}\right);
\end{equation}
This often drastically reduces the number of computations one has to make.
Although, formally speaking, we can now compute the connection, the Christoffel symbols are somewhat difficult to manage; an object with three indices (and which is moreover not tensorial) can be difficult to deal with in general. However, we can improve our situation, at least conceptually. We will think of the connection as a _matrix-valued 1-form_:
\begin{equation}\label{matrix1form}
\Gamma = \sum_k \Gamma_k dx^k,
\end{equation}
where each of the $\Gamma_k$&amp;rsquo;s is a matrix. The ${(ij)}^{th}$ component of the matrix $\Gamma_k$ is given by $(\Gamma_k)^i_j = \Gamma^i_{jk}$. Note that the placement of indices on the left hand side is consistent with that of the right. This definition also allows us to clearly write the Levi-Cevita connection as an operator on vector fields: given a vector field $X$, the connection applied to $X$ can be written as (at least in the local coordinate patch on which we are working)
\begin{equation}
\nabla X = d X + \Gamma X,
\end{equation}
Where $\Gamma$ acts on $X$ as a matrix applied to a vector. As a consistency check, we see that both terms in the above have one upper and one lower index, and are thus of the same type, so that $\nabla X$ is indeed a well-defined and (as we will soon check) tensorial quantity.
Let us begin to demonstrate the utility of this notation. Suppose $(U,x)$, $(V,y)$ are overlapping coordinate charts; consider the change of coordinates $y \to x(y)$. Using our original formulafor the Christoffel symbols, we can (rather tediously) compute $\tilde{\Gamma}^i_{jk}$ in the new coordinates:
\begin{equation}
\tilde{\Gamma}^i_{jk} = \sum_{m,n,p}\left(\frac{\partial x^i}{\partial y^m}\frac{\partial y^n}{\partial x^j}\frac{\partial y^p}{\partial x^k} \Gamma^m_{np} + \frac{\partial^2 y^m}{\partial x^j \partial x^k} \frac{\partial x^i}{\partial y^m} \right).
\end{equation}
Now, adding in the contribution of $dx^k \to \frac{\partial x^k}{\partial y^r} dy^r$, we see that one of the $\frac{\partial y}{\partial x}$ factors
cancels in the expression for $\tilde{\Gamma}^i_{jk} dy^k$.
We can simplify this expression (at least notationally speaking) significantly, Define $\lambda$ to be the matrix-valued function with entries $(\lambda)^i_j = \frac{\partial y^i}{\partial x^j}$. Then, in matrix notation the new expression for the Christoffel symbols reads
\begin{equation}
\tilde{\Gamma} = \lambda \Gamma \lambda^{-1} - d\lambda \cdot\lambda^{-1}.
\end{equation}
The discerning reader will recognize this as what is known as a _gauge transformation_ in physics. This is, of course, no coincidence: a Riemannian manifold can be viewed as a principal- $GL_n(\mathbb{R})$ bundle and (gauge-invariant) connection $\nabla = d + \Gamma$. (If this sentence does not mean anything to you right now, do not worry; it is not relevant to the remainder of this exposition).&lt;/p&gt;
&lt;p&gt;Now, we will check that, indeed, for any vector field $X$, $\nabla X$ is tensorial. Let us use the coordinate transformation of the previous section. Under such a transformation, the vector field $X$ transforms as $\lambda X$. Altogether, we find that, in the new coordinates,
\begin{align}
\tilde{\nabla} \tilde{X} &amp;amp;= \left(d +  \lambda \Gamma \lambda^{-1} - d\lambda \cdot\lambda^{-1}\right) \lambda X\\\
&amp;amp;= d\lambda \cdot X + \lambda dX + \lambda \Gamma X - d\lambda \cdot X \\\&lt;br&gt;
&amp;amp;= \lambda dX + \lambda \Gamma X = \lambda \nabla X,
\end{align}
so that $\nabla X$ is a tensorial quantity, and transforms like a tensor with one additional lower index than $X$.
We conclude this section with some computations carried over from the previous section.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;em&gt;The Sphere $S^2$.&lt;/em&gt;
Let us compute the connection coefficients of the Levi-Cevita connection arising from the metric we derived in the previous section. Since the metric is diagonal, we can use formula we derived for the $\Gamma$&amp;rsquo;s for a diagonal metric to compute the Christoffel symbols. Furthermore, since the metric is diagonal, and the only nonconstant component of the metric is $g_{\varphi \varphi}$, we see that the only non-vanishing Christoffel symbols will be those that involve an expression of the form $\partial_\theta g_{\varphi \varphi}$ (since partial derivatives of constants will yield zero). Comparing with diagonal formula, we conclude that the only non-vanishing Christoffel symbols are $\Gamma^{\varphi}_{\varphi\theta} = \Gamma^{\varphi}_{\theta\varphi}$, and $\Gamma^{\theta}_{\varphi\varphi}$. We compute these entries explicitly:
\begin{align}
\Gamma^{\varphi}_{\varphi\theta} = \Gamma^{\varphi}_{\theta\varphi} &amp;amp;= \frac{1}{2} g^{\varphi \varphi} \left( \partial_\varphi g_{\varphi\theta} + \partial_\theta g_{\varphi\varphi} - \partial_\varphi g_{\varphi\theta} \right)
= \frac{1}{2} g^{\varphi \varphi} \left( \partial_\theta g_{\varphi\varphi} \right) = \frac{1}{2} \frac{1}{\sin^2\theta} (2\sin\theta \cos \theta) = \cot\theta, \\\&lt;br&gt;
\Gamma^{\theta}_{\varphi\varphi} &amp;amp;= \frac{1}{2} g^{\theta \theta} \left( \partial_\varphi g_{\theta\varphi} + \partial_\varphi g_{\varphi\theta} - \partial_\theta g_{\varphi\varphi} \right) = -\frac{1}{2} g^{\theta \theta}\left(  \partial_\theta g_{\varphi\varphi} \right) = -\frac{1}{2} (2\sin\theta \cos \theta) = -\sin\theta \cos \theta.
\end{align}
We now write the connection coefficients as a matrix-valued 1-form. Formally, this 1-form should look like
\begin{equation}
\Gamma = \Gamma_\theta d\theta+ \Gamma_\varphi d\varphi=
\begin{pmatrix} \Gamma^\theta_{\theta \theta} &amp;amp;  \Gamma^\theta_{\varphi\theta}\\\ \Gamma^\varphi_{\theta\theta} &amp;amp; \Gamma^\varphi_{\varphi\theta} \end{pmatrix}
d \theta +
\begin{pmatrix}  \Gamma^\theta_{\theta \varphi} &amp;amp;  \Gamma^\theta_{\varphi\varphi}\\\ \Gamma^\varphi_{\theta\varphi} &amp;amp; \Gamma^\varphi_{\varphi\varphi} \end{pmatrix}
d\varphi.
\end{equation}
Using the expressions we found earlier for the Christoffel symbols, we find that $\Gamma$ explicitly takes the form
\begin{equation} \label{sphereconn}
\Gamma = \Gamma_\theta d\theta + \Gamma_\varphi d\varphi=
\begin{pmatrix} 0 &amp;amp; 0 \\\ 0 &amp;amp; \cot \theta \end{pmatrix}
d \theta +
\begin{pmatrix}  0 &amp;amp;  -\sin\theta \cos\theta \\\ \cot\theta&amp;amp; 0 \end{pmatrix}
d\varphi.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Compute the connection form of the stereographic metric on the sphere.
&lt;em&gt;Solution:&lt;/em&gt;
Set $\rho(X,Y) = 1 + X^2 + Y^2$. Then the connection form is
\begin{equation}\label{stereographicconn}
\Gamma = \Gamma_X dX + \Gamma_Y dY =
\frac{2}{\rho} \begin{pmatrix} -X &amp;amp; -Y \\\ Y &amp;amp; -X \end{pmatrix}
d X +
\frac{2}{\rho} \begin{pmatrix}  -Y &amp;amp;  X \\\ -X &amp;amp; -Y \end{pmatrix}
dY.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Torus $\mathbb{T}^2$.&lt;/em&gt;
Using the explicit expression for the metric on the torus, and the formula for the Christoffel symbols of a diagonal metric, we can compute the coefficients of the Levi-Cevita connection. Let us first determine all nonzero Christoffel symbols. Since the only non-constant entry in the metric is $g_{\varphi\varphi} = (R + r\cos\theta)^2$, and this coefficient depends only on the variable $\theta$, we see that the only nonzero Christoffel symbols will be those which contain the term $\partial_{\theta} g_{\varphi \varphi}$. There are precisely three of these: $\Gamma^\varphi_{\varphi \theta} = \Gamma^\varphi_{\theta\varphi}$, and $\Gamma^{\theta}_{\varphi \varphi}$. We compute these terms explicitly:
\begin{align}
\Gamma^\varphi_{\varphi \theta} = \Gamma^\varphi_{\theta\varphi} &amp;amp;= \frac{1}{2} g^{\varphi \varphi} \left( \partial_\varphi g_{\varphi \theta} + \partial_\theta g_{\varphi \varphi} - \partial_\varphi g_{\varphi \theta}\right) = \frac{1}{2} g^{\varphi \varphi} \left(\partial_\theta g_{\varphi \varphi}\right) = \frac{-r\sin\theta}{R + r\cos\theta},\\\&lt;br&gt;
\Gamma^{\theta}_{\varphi \varphi} &amp;amp;= \frac{1}{2}g^{\theta\theta} \left( \partial_\varphi g_{\theta \varphi} + \partial_\varphi g_{\varphi \theta} - \partial_\theta g_{\varphi\varphi}\right) = \frac{(R + r \cos\theta)}{r} \sin\theta.
\end{align}
Now, writing the connection as a matrix-valued 1-form, we find that
\begin{equation}\label{torusconn}
\Gamma = \Gamma_\theta d\theta + \Gamma_\varphi d\varphi=
\begin{pmatrix} 0 &amp;amp; 0 \\\ 0 &amp;amp; \frac{-r\sin\theta}{R + r\cos\theta} \end{pmatrix}
d \theta +
\begin{pmatrix}  0 &amp;amp; \frac{(R + r \cos\theta)}{r} \sin\theta\\\  \frac{-r\sin\theta}{R + r\cos\theta}&amp;amp; 0 \end{pmatrix}
d\varphi.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Upper Half Plane $\mathbb{H}$.&lt;/em&gt;
We now compute the Christoffel symbols of the hyperbolic metric on the upper half plane. Note that only Christoffel symbols containing the terms $\partial_y g_{xx}$ or $\partial_y g_{yy}$ have a chance of being nonzero. Thus, the nonzero Christoffel symbols are $\Gamma^x_{yx} = \Gamma^x_{xy}$, $\Gamma^y_{xx}$, and $\Gamma^y_{yy}$. Since the metric is diagonal, we can again use the nice formula we derived to compute the Christoffel symbols: we find that
\begin{align}
\Gamma^x_{yx} = \Gamma^x_{xy} &amp;amp;= \frac{1}{2} g^{xx}\left(\partial_y g_{xx} + \partial_x g_{yx} - \partial_x g_{yx}\right) = \frac{1}{2} g^{xx}\left(\partial_y g_{xx}\right) = -1/y, \\\&lt;br&gt;
\Gamma^y_{xx} &amp;amp;= \frac{1}{2} g^{yy}\left(\partial_x g_{yx} + \partial_x g_{xy} - \partial_y g_{xx}\right) = -\frac{1}{2} g^{yy}\left( \partial_y g_{xx}\right) = 1/y,\\\&lt;br&gt;
\Gamma^y_{yy} &amp;amp;= \frac{1}{2} g^{yy}\left(\partial_y g_{yy} + \partial_y g_{yy} - \partial_y g_{yy}\right) =
\frac{1}{2} g^{yy}\left(\partial_y g_{yy}\right) = -1/y.
\end{align}
Written as a matrix-valued 1-form, the connection takes the form
\begin{equation}\label{halfplaneconn}
\Gamma = \Gamma_x dx + \Gamma_y dy = \begin{pmatrix}  0 &amp;amp; -1/y \\\  1/y&amp;amp; 0 \end{pmatrix}  dx + \begin{pmatrix}  -1/y &amp;amp; 0 \\\  0 &amp;amp; -1/y \end{pmatrix} dy.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt; Compute the connection form of the hyperbolic metric on the disc. &lt;em&gt;Solution:&lt;/em&gt; Set $\sigma(x,y) = 1 - x^2 - y^2$; then the connection form is given as
\begin{equation}
\Gamma = \Gamma_x dx + \Gamma_y dy = \frac{2}{\sigma} \begin{pmatrix} x &amp;amp; y \\\ -y &amp;amp; x \end{pmatrix} dx + \frac{2}{\sigma} \begin{pmatrix}  y &amp;amp;  -x \\\ x &amp;amp; y \end{pmatrix} dy.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The 3-sphere $S^3$.&lt;/em&gt;
We compute the connection form from the metric on the 3-sphere we derived previously. Since the only nonzero components of the
metric are $g_{\psi\psi} = 1$, $g_{\theta\theta} = \sin^2\psi$, and $g_{\varphi\varphi} = \sin^2\psi\sin^2\theta$,
the only nonvanishing derivatives of the metric will be $\partial_\psi g_{\theta \theta}$, $\partial_{\psi} g_{\varphi\varphi}$,
and $\partial_{\theta}g_{\varphi \varphi}$. Thus, we must compute the $9$ nontrivial Christoffel symbols:
$\Gamma^{\psi}_{\theta\theta}$, $\Gamma^{\theta}_{\theta \psi} = \Gamma^{\theta}_{\psi \theta}$,
$\Gamma^{\psi}_{\varphi \varphi}$, $\Gamma^{\varphi}_{\psi \varphi} = \Gamma^{\varphi}_{\varphi \psi}$,
$\Gamma^{\theta}_{\varphi \varphi}$, and $\Gamma^{\varphi}_{\theta \varphi} = \Gamma^{\varphi}_{\varphi\theta}$.
By direct computation, we find that
\begin{align}
\Gamma^{\psi}_{\theta\theta} &amp;amp;= -\sin\psi \cos \psi,\\\&lt;br&gt;
\Gamma^{\theta}_{\theta \psi} &amp;amp;= \Gamma^{\theta}_{\psi \theta} = \cot \psi,\\\
\Gamma^{\psi}_{\varphi \varphi} &amp;amp;= -\sin \psi \cos \psi \sin^2\theta,\\\&lt;br&gt;
\Gamma^{\varphi}_{\psi \varphi} &amp;amp;= \Gamma^{\varphi}_{\varphi \psi} = \cot \theta,\\\&lt;br&gt;
\Gamma^{\theta}_{\varphi \varphi} &amp;amp;= -\sin \theta \cos \theta,\\\&lt;br&gt;
\Gamma^{\varphi}_{\theta \varphi} &amp;amp;= \Gamma^{\varphi}_{\varphi\theta} = \cot\theta.
\end{align}
The connection form is thus
\begin{equation}\label{3-sphere-conn}
\Gamma = \begin{pmatrix} 0 &amp;amp; 0 &amp;amp;0\\\ 0 &amp;amp; \cot\psi &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; \cot \psi \end{pmatrix} d\psi
+ \begin{pmatrix} 0 &amp;amp; -\sin\psi\cos\psi &amp;amp;0\\\  \cot\psi &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; \cot \theta \end{pmatrix} d\theta
+ \begin{pmatrix} 0 &amp;amp; 0 &amp;amp; -\sin\psi\cos\psi\sin^2\theta\\\  0 &amp;amp; 0 &amp;amp; -\sin\theta\cos\theta\\\  \cot \psi &amp;amp;  \cot \theta &amp;amp; 0 \end{pmatrix} d\varphi
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The left-invariant metric on $S^3 \cong SU(2)$ &amp;amp; Hopf coordinates.&lt;/em&gt;
Recall the invariant metric on $S^3$ had entries $g_{\eta \eta} = 1$, $g_{\xi_1\xi_1} = \sin^2\eta$,
and $g_{\xi_2\xi_2} = \cos^2\eta$. Thus, the only non-vanishing derivatives of the metric tensor will be
$\partial_{\eta} g_{\xi_1 \xi_1}$ and $\partial_{\eta} g_{\xi_2 \xi_2}$; consequentially, there are only $6$
non-vanishing connection coefficients: $\Gamma^{\eta}_{\xi_1\xi_1}$, $\Gamma^{\xi_1}_{\eta \xi_1} =
\Gamma^{\xi_1}_{\xi_1 \eta}$, $\Gamma^{\eta}_{\xi_2\xi_2}$, and $\Gamma^{\xi_2}_{\eta \xi_2} =
\Gamma^{\xi_2}_{\xi_2 \eta}$. We compute that:
\begin{align*}
\Gamma^{\eta}_{\xi_1 \xi_1} &amp;amp;= -\cos\eta \sin\eta,\\\&lt;br&gt;
\Gamma^{\xi_1}_{\eta \xi_1} &amp;amp;=  \Gamma^{\xi_1}_{\xi_1 \eta} = \cot \eta,\\\&lt;br&gt;
\Gamma^{\eta}_{\xi_2 \xi_2} &amp;amp;= \cos\eta \sin\eta,\\\&lt;br&gt;
\Gamma^{\xi_2}_{\eta \xi_2} &amp;amp;=  \Gamma^{\xi_2}_{\xi_2 \eta} = -\tan \eta.
\end{align*}
The connection form is thus
\begin{equation} \label{SU2-conn}
\Gamma = \begin{pmatrix} 0 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; \cot \eta &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; -\tan \eta \end{pmatrix}d\eta + \begin{pmatrix} 0 &amp;amp; -\cos\eta \sin \eta &amp;amp; 0\\\ \cot\eta &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; 0 \end{pmatrix}d\xi_1 + \begin{pmatrix} 0 &amp;amp; 0 &amp;amp; \cos\eta \sin \eta\\\ 0 &amp;amp; 0 &amp;amp; 0\\\ -\tan\eta &amp;amp; 0 &amp;amp; 0 \end{pmatrix}d\xi_2.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Schwarzschild metric.&lt;/em&gt;
We now compute the connection form from the Schwarzshield metric. By inspection, we see that the only nonvanishing derivatives of the metric are: $\partial_r g_{tt}$, $\partial_r g_{rr}$, $\partial_r g_{\theta\theta}$, $\partial_r g_{\varphi \varphi}$, and $\partial_\theta g_{\varphi \varphi}$. Since the metric is diagonal, the only non-vanishing Christoffel symbols are: $\Gamma^r_{tt}$, $\Gamma^t_{rt} = \Gamma^t_{tr}$, $\Gamma^r_{rr}$, $\Gamma^r_{\theta\theta}$, $\Gamma^\theta_{r \theta} = \Gamma^\theta_{\theta r}$, $\Gamma^r_{\varphi\varphi}$, $\Gamma^\varphi_{r \varphi} = \Gamma^\varphi_{\varphi r}$, $\Gamma^\theta_{\varphi \varphi}$, and finally $\Gamma^\varphi_{\theta \varphi} = \Gamma^\varphi_{\varphi \theta}$. These quantities are computed below:
\begin{align}
\Gamma^r_{tt} &amp;amp;=  -\frac{1}{2}g^{rr} \left(\partial_r g_{tt}\right) = \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right),\\\&lt;br&gt;
\Gamma^t_{rt} = \Gamma^t_{tr} &amp;amp;= \frac{1}{2}g^{tt} \left(\partial_r g_{tt}\right) = \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right)^{-1},\\\&lt;br&gt;
\Gamma^r_{rr} &amp;amp;= \frac{1}{2}g^{rr}\left(\partial_r g_{rr}\right) = -\frac{1}{2}\frac{r_s}{r^2} \left(1-\frac{r_s}{r}\right)^{-1},\\\&lt;br&gt;
\Gamma^r_{\theta\theta} &amp;amp;= -\frac{1}{2}g^{rr} \left(\partial_r g_{\theta\theta}\right) = -r\left(1 - \frac{r_s}{r}\right),\\\&lt;br&gt;
\Gamma^\theta_{r \theta} = \Gamma^\theta_{\theta r} &amp;amp;= \frac{1}{2}g^{\theta\theta} \left(\partial_r g_{\theta\theta}\right) = \frac{1}{r},\\\&lt;br&gt;
\Gamma^r_{\varphi\varphi} &amp;amp;= -\frac{1}{2}g^{rr} \left(\partial_r g_{\varphi\varphi}\right) = -r\sin^2\theta\left(1-\frac{r_s}{r}\right), \\\&lt;br&gt;
\Gamma^\varphi_{r \varphi} = \Gamma^\varphi_{\varphi r} &amp;amp;= \frac{1}{2}g^{\varphi\varphi} \left(\partial_r g_{\varphi\varphi}\right) = \frac{1}{r},\\\&lt;br&gt;
\Gamma^\theta_{\varphi \varphi} &amp;amp;= -\frac{1}{2}g^{\theta\theta} \left(\partial_\theta g_{\varphi\varphi}\right) = -\sin\theta\cos\theta,\\\&lt;br&gt;
\Gamma^\varphi_{\theta \varphi} = \Gamma^\varphi_{\varphi \theta} &amp;amp;= \frac{1}{2}g^{\varphi\varphi} \left(\partial_\theta g_{\varphi\varphi}\right) = \cot\theta.\\\&lt;br&gt;
\end{align}
Therefore, in matrix form, the Schwarzschild connection looks like
\begin{equation}\label{Schwarzschildconn}
\begin{split}
\Gamma&amp;amp;=\begin{pmatrix}  0 &amp;amp; \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right)^{-1} &amp;amp; 0 &amp;amp; 0 \\\ \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right) &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt +
\begin{pmatrix}  \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right)^{-1} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; -\frac{1}{2}\frac{r_s}{r^2} \left(1-\frac{r_s}{r}\right)^{-1} &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; \frac{1}{r} &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \frac{1}{r}\end{pmatrix} dr \\\&lt;br&gt;
&amp;amp;+ \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; -r\left(1 - \frac{r_s}{r}\right) &amp;amp; 0 \\\ 0 &amp;amp; \frac{1}{r} &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cot\theta\end{pmatrix} d\theta +
\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -r\sin^2\theta\left(1-\frac{r_s}{r}\right) \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\sin\theta\cos\theta \\\ 0 &amp;amp; \frac{1}{r} &amp;amp; \cot\theta &amp;amp; 0\end{pmatrix} d\varphi.
\end{split}
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;3-the-riemann-tensor&#34;&gt;$3$. The Riemann Tensor.&lt;/h2&gt;
&lt;p&gt;The next object we would like to be able to compute is the Riemann curvature tensor. Seemingly, we are in a much worse situation than before, as we now have to describe an object with 4 indices. In fact, this will turn out to be manageable: just as we expressed the connection coefficients as a matrix-valued 1-form, we will express the Riemann tensor as a matrix-valued 2-form. Contrary to the notation popular in Riemannian geometry, we will denote the Riemann tensor by $\Omega$, and establish a correspondence to the usual notation $R^i_{jkl}$. As usual, the Riemann tensor is defined as $\Omega := \nabla \circ \nabla$, the connection applied to itself. In our formalism, computing the Riemann tensor amounts to the following computation. Let $X$ be any vector field; then
\begin{align}
\Omega X&amp;amp; = (\nabla \circ \nabla) X = \nabla(dX + \Gamma X)
= d(dX + \Gamma X) + \Gamma (dX + \Gamma X)\\\&lt;br&gt;
&amp;amp;=d(dX) + d\Gamma \cdot X - \Gamma dX + \Gamma dX + (\Gamma \wedge \Gamma) X \\\&lt;br&gt;
&amp;amp;= (d\Gamma + \Gamma \wedge \Gamma)X,
\end{align}
Since $d^2 = 0$. Therefore, locally, we can write the curvature tensor as the matrix-valued 2-form $\Omega = d\Gamma + \Gamma \wedge \Gamma$ _Note: $\Gamma \wedge \Gamma$ is nonzero in general, since $\Gamma$ has matrix coefficients. This will be made apparent in the examples._ This is consistent with the Riemann tensor having 1 upper and 3 lower indices (1 upper and 1 lower contributing to the matrix coefficient, and the remaining 2 lower contributing to the 2-form). Write $\Omega$ as $\Omega = \Omega_{kl} dx^k \wedge dx^l$, where each $\Omega_{kl}$ is a matrix. In the usual notation of the Riemann tensor, the $(ij)^{th}$ component of the matrix $\Omega_{kl}$ is given by $(\Omega_{kl})^i_j = R^i_{jkl}$. As an immediate consequence, we see that the Riemann tensor is antisymmetric in two of its indices.&lt;/p&gt;
&lt;p&gt;Furthermore, this notation allows for an almost trivial proof of the second Bianchi identity: Note that $\Omega \nabla = ${$(d+\Gamma)(d+\Gamma)$}$(d+\Gamma) = (d+\Gamma)${$(d+\Gamma)(d+\Gamma)$} $= \nabla \Omega$. Applying this identity to a vector field $X$, we see that $\Omega \nabla X = \nabla(\Omega X) = \Omega \nabla X + (\nabla \Omega) X$, implying that $\nabla \Omega = 0$.&lt;/p&gt;
&lt;p&gt;The Riemann tensor is now a more manageable quantity, and we now show by direct computation (through the examples of the previous sections) that working with it is not too difficult.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Sphere $S^2$.&lt;/em&gt;
We now compute the Riemann tensor of the sphere using the connection form we found in the last section. We must compute two quantities: $d\Gamma$ and $\Gamma\wedge\Gamma$. Let us begin by computing $d\Gamma$: since $\Gamma = \Gamma_\theta d\theta + \Gamma_\varphi d\varphi$, we find that $d \Gamma = \left( \partial_\theta \Gamma_\varphi - \partial_\varphi \Gamma_\theta\right) d\theta \wedge d\varphi$. Now, since $\Gamma_\theta$ is independent of $\varphi$, we see that this expression simplifies to $d\Gamma = \partial_\theta \Gamma_\varphi d\theta \wedge d\varphi$. Explicitly (i.e., applying this prescription to the connection on the sphere we derived), we find that
\begin{equation}\label{dgammasphere}
d\Gamma = \partial_\theta\begin{pmatrix}  0 &amp;amp; -\sin\theta \cos\theta \\\  \cot\theta&amp;amp; 0 \end{pmatrix}
d\theta \wedge d\varphi = \begin{pmatrix}  0 &amp;amp;  \sin^2\theta - \cos^2\theta\\\ -\csc^2\theta&amp;amp; 0 \end{pmatrix}
d\theta \wedge d\varphi.
\end{equation}
Now, we are left to compute $\Gamma\wedge\Gamma$. We find that $\Gamma\wedge\Gamma = \left(\Gamma_\theta d\theta + \Gamma_\varphi d\varphi\right)\wedge \left(\Gamma_\theta d\theta + \Gamma_\varphi d\varphi\right) = [\Gamma_\theta,\Gamma_\varphi] d\theta\wedge d\varphi$, where $[\cdot,\cdot]$ denotes the matrix commutator. After some simple matrix algebra, one finds that
\begin{equation}\label{gammawedgesphere}
\Gamma\wedge\Gamma = [\Gamma_\theta,\Gamma_\varphi] d\theta\wedge d\varphi = \begin{pmatrix}  0 &amp;amp; \cos^2\theta \\\ \cot^2\theta &amp;amp; 0 \end{pmatrix} d\theta \wedge d\varphi.
\end{equation}
Combining these results, we find that
\begin{equation}\label{spherecurvature}
\Omega = d\Gamma + \Gamma \wedge \Gamma = \begin{pmatrix}  0 &amp;amp; \sin^2\theta \\\ -1 &amp;amp; 0 \end{pmatrix} d\theta \wedge d\varphi.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Compute the curvature form of the sphere with stereographic connection.
&lt;em&gt;Solution:&lt;/em&gt; Again, setting $\rho(X,Y) = 1 + X^2 + Y^2$, the curvature tensor takes the form
\begin{equation} \label{stereographiccurvature}
\Omega = \begin{pmatrix}  0 &amp;amp; 4/\rho \\\ -4/\rho &amp;amp; 0 \end{pmatrix} dX \wedge dY.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;em&gt;The Torus $\mathbb{T}^2$.&lt;/em&gt;
Continuing the example of the torus, we now compute the Riemann tensor on the torus from the connection. We separately compute $d\Gamma$ and $\Gamma\wedge\Gamma$. Now, $d\Gamma = \left( \partial_\theta \Gamma_\varphi - \partial_\varphi \Gamma_\theta \right) d\theta\wedge d\varphi = \partial_\theta \Gamma_\varphi d\theta\wedge d\varphi$, since $\Gamma_\theta$ is independent of $\varphi$. Thus, we find that
\begin{equation}\label{torusdgamma}
d\Gamma =  \partial_\theta \Gamma_\varphi d\theta\wedge d\varphi = \partial_\theta \begin{pmatrix}  0 &amp;amp; \frac{(R + r \cos\theta)}{r}\sin\theta \\\  \frac{-r\sin\theta}{R + r\cos\theta}&amp;amp; 0 \end{pmatrix}
d\theta \wedge d\varphi = \begin{pmatrix}  0 &amp;amp; \frac{\cos\theta(R+r\cos\theta)}{r}-\sin^2\theta \\\  \frac{-r(r+R\cos\theta)}{(R + r\cos\theta)^2}&amp;amp; 0 \end{pmatrix}
d\theta \wedge d\varphi.
\end{equation}
Furthermore, we find that $\Gamma\wedge\Gamma = \left(\Gamma_\theta d\theta + \Gamma_\varphi d\varphi\right)\wedge \left(\Gamma_\theta d\theta + \Gamma_\varphi d\varphi\right) = [\Gamma_\theta,\Gamma_\varphi] d\theta\wedge d\varphi$; after some computations, we see that this simplifies to
\begin{equation}\label{torusgammawedge}
\Gamma\wedge\Gamma = [\Gamma_\theta,\Gamma_\varphi] d\theta\wedge d\varphi = \begin{pmatrix}  0 &amp;amp; \sin^2\theta \\\  \frac{r^2 \sin^2\theta}{(R+ r\cos\theta)^2} &amp;amp; 0 \end{pmatrix}
d\theta \wedge d\varphi.
\end{equation}
Combining these results, we obtain the curvature tensor:
\begin{equation} \label{toruscurvature}
\Omega = d\Gamma + \Gamma\wedge\Gamma =  \begin{pmatrix}  0 &amp;amp; \frac{\cos\theta(R+r\cos\theta)}{r} \\\  -\frac{r \cos\theta}{R+ r\cos\theta} &amp;amp; 0 \end{pmatrix} d\theta \wedge d\varphi.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;strong&gt;The Upper Half Plane $\mathbb{H}$.&lt;/strong&gt;
We now find the Riemann tensor on the upper half plane endowed with the hyperbolic metric; as before, we must compute $d\Gamma$ and $\Gamma \wedge \Gamma$. Now, $d\Gamma = \left(\partial_x \Gamma_y - \partial_y \Gamma_x\right) dx\wedge dy = -\partial_y \Gamma_x dx\wedge dy$, since $\Gamma_y$ is independent of $x$. Therefore:
\begin{equation}\label{dgammaHP}
d\Gamma = -\partial_y \Gamma_x dx\wedge dy = -\partial_y \begin{pmatrix}  0 &amp;amp; -1/y \\\  1/y&amp;amp; 0 \end{pmatrix}  dx\wedge dy = \begin{pmatrix}  0 &amp;amp; -1/y^2 \\\  1/y^2 &amp;amp; 0 \end{pmatrix}  dx\wedge dy.
\end{equation}
Furthermore, we must compute $\Gamma\wedge\Gamma = [\Gamma_x,\Gamma_y] dx\wedge dy$. We find that:
\begin{equation}
\Gamma\wedge\Gamma = [\Gamma_x,\Gamma_y] dx\wedge dy = 0.
\end{equation}
(This is due to the fact that $\Gamma_y$ is a multiple of the identity matrix). Therefore, the curvature tensor is
\begin{equation}
\Omega = d\Gamma + \Gamma\wedge\Gamma = \begin{pmatrix}  0 &amp;amp; -1/y^2 \\\  1/y^2 &amp;amp; 0 \end{pmatrix}  dx\wedge dy.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Compute the curvature form of the disc with the hyperbolic connection. &lt;em&gt;Solution:&lt;/em&gt;
Setting $\sigma(x,y) = 1 - x^2 - y^2$, the curvature form is
\begin{equation}\label{hyperbolicD}
\Omega = \begin{pmatrix}  0 &amp;amp; -4/\sigma^2 \\\  4/\sigma^2 &amp;amp; 0 \end{pmatrix}  dx\wedge dy.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt; &lt;em&gt;The 3-sphere $S^3$.&lt;/em&gt;
Using the first expression for the connection on $S^3$, compute the Riemann tensor.
&lt;em&gt;Solution:&lt;/em&gt;
\begin{equation} \label{3-sphere-curv}
\Omega = \begin{pmatrix} 0 &amp;amp;\sin^2\psi &amp;amp; 0\\\ -1 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix}d\psi\wedge d\theta
+\begin{pmatrix} 0 &amp;amp; 0 &amp;amp; \sin^2\psi \sin^2\theta \\\ 0 &amp;amp; 0 &amp;amp; 0\\\ -1 &amp;amp; 0 &amp;amp; 0\end{pmatrix}d\psi\wedge d\varphi
+\begin{pmatrix} 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; \sin^2\psi \sin^2\theta\\\ 0 &amp;amp; \sin^2\psi &amp;amp; 0\end{pmatrix}d\theta\wedge d\varphi.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt; &lt;em&gt;The left-invariant metric on $S^3 \cong SU(2)$ &amp;amp; Hopf coordinates.&lt;/em&gt;
Using the expression for the invariant connection on $SU(2)$, compute the Riemann tensor.
&lt;em&gt;Solution:&lt;/em&gt;
\begin{equation}\label{SU2-curv}
\Omega = \begin{pmatrix}0 &amp;amp; \sin^2\eta &amp;amp; 0\\\ -1 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix}d\eta\wedge d\xi_1
+ \begin{pmatrix}0 &amp;amp; 0 &amp;amp; \cos^2\eta\\\ 0 &amp;amp; 0 &amp;amp; 0\\\ -1 &amp;amp; 0 &amp;amp; 0\end{pmatrix}d\eta\wedge d\xi_2
+ \begin{pmatrix}0 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; \cos^2\eta\\\ 0 &amp;amp; -\sin^2\eta &amp;amp; 0\end{pmatrix} d\xi_1\wedge d\xi_2
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Schwarzschild metric.&lt;/em&gt;
Using the Schwarzschild connection, we can compute the curvature form. We begin by computing $d\Gamma$; we have that
$d\Gamma = - \partial_r \Gamma_t dt\wedge dr + \partial_r \Gamma_\theta  dr\wedge d\theta + \partial_r \Gamma_\varphi dr\wedge d\varphi + \partial_\theta \Gamma_\theta d\theta \wedge d\varphi$. Using the expression we derived, we find that:
\begin{align}
- \partial_r \Gamma_t dt\wedge dr  &amp;amp;= - \partial_r \begin{pmatrix}  0 &amp;amp; \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right)^{-1} &amp;amp; 0 &amp;amp; 0 \\\ \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right) &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt\wedge dr =\begin{pmatrix}  0 &amp;amp; \frac{r_s(2r-r_s)}{2r^2(r-r_s)^2} &amp;amp; 0 &amp;amp; 0 \\\ \frac{r_s(2r-3r_s)}{2r^4} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt\wedge dr, \\\&lt;br&gt;
\partial_r \Gamma_\theta  dr\wedge d\theta &amp;amp;= \partial_r \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; -r\left(1 - \frac{r_s}{r}\right) &amp;amp; 0 \\\ 0 &amp;amp; \frac{1}{r} &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cot\theta\end{pmatrix}  dr\wedge d\theta = \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 \\\ 0 &amp;amp; -\frac{1}{r^2} &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix}  dr\wedge d\theta,\\\&lt;br&gt;
\partial_r \Gamma_\varphi dr\wedge d\varphi &amp;amp;= \partial_r \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -r\sin^2\theta\left(1-\frac{r_s}{r}\right) \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\sin\theta\cos\theta \\\ 0 &amp;amp; \frac{1}{r} &amp;amp; \cot\theta &amp;amp; 0\end{pmatrix}dr\wedge d\varphi = \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\sin^2\theta \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; -\frac{1}{r^2} &amp;amp; 0 &amp;amp; 0\end{pmatrix} dr\wedge d\varphi,\\\&lt;br&gt;
\partial_\theta \Gamma_\varphi d\theta \wedge d\varphi &amp;amp;= \partial_\theta\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -r\sin^2\theta\left(1-\frac{r_s}{r}\right) \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\sin\theta\cos\theta \\\ 0 &amp;amp; \frac{1}{r} &amp;amp; \cot\theta &amp;amp; 0\end{pmatrix} = \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -2r\sin\theta\cos\theta\left(1-\frac{r_s}{r}\right) \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \sin^2\theta-\cos^2\theta \\\ 0 &amp;amp; 0 &amp;amp; -\csc^2\theta &amp;amp; 0\end{pmatrix} d\theta \wedge d\varphi.
\end{align}
We now must compute $\Gamma \wedge \Gamma$. It is easily shown that $\Gamma\wedge\Gamma = [\Gamma_t,\Gamma_r]dt\wedge dr [\Gamma_t,\Gamma_\theta]dt\wedge d\theta + [\Gamma_t,\Gamma_\varphi]dt\wedge d\varphi + [\Gamma_r,\Gamma_\theta]dr\wedge d\theta + [\Gamma_r,\Gamma_\varphi]dr\wedge d\varphi + [\Gamma_\theta,\Gamma_\varphi]d\theta \wedge d\varphi$. Again, using the connection, and some (rather tedious, but still elementary) matrix algebra, we can eventually compute the Riemann tensor to be:
\begin{equation}
\begin{split}
\Omega &amp;amp;= \begin{pmatrix}  0 &amp;amp; \frac{r_s}{r^2(r-r_s)} &amp;amp; 0 &amp;amp; 0 \\\ \frac{r_s(r-r_s)}{r^4} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt\wedge dr
+ \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; -\frac{r_s}{2r} &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ \frac{r_s(r-r_s)}{2r^4} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt\wedge d\theta\\\&lt;br&gt;
&amp;amp;+\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\frac{r_s \sin^2\theta}{2r} \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ \frac{r_s(r-r_s)}{2r^4} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt\wedge d\varphi
+\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; -\frac{r_s}{2r} &amp;amp; 0 \\\ 0 &amp;amp; \frac{r_s}{r^2(r-r_s)} &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dr\wedge d\theta\\\&lt;br&gt;
&amp;amp;+\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\frac{r_s\sin^2\theta}{2r} \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; \frac{r_s}{2r^2(r-r_s)} &amp;amp; 0 &amp;amp; 0\end{pmatrix} dr\wedge d\varphi
+\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\frac{r_s\sin^2\theta}{r} \\\ 0 &amp;amp; 0 &amp;amp; -\frac{r_s}{r} &amp;amp; 0\end{pmatrix} d\theta\wedge d\varphi
\end{split}
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;4-the-ricci-and-scalar-curvatures&#34;&gt;$4$. The Ricci and scalar curvatures.&lt;/h2&gt;
&lt;p&gt;Commonly, problems in geometry require one to compute derived quantities of the Riemann tensor. Two of the most important are the Ricci and scalar curvatures: the Ricci curvature is defined to be the trace of the Riemann tensor:
\begin{equation} \label{Ricci}
R_{ij} = \sum_k R^k_{ikj}.
\end{equation}
This quantity is symmetric in its remaining indices. We will carry on our examples from the previous section to make the computations here transparent. Finally, the scalar curvature is defined to be the contraction of both indices the Ricci tensor:
\begin{equation}
R = \sum_{i,j} g^{ij}R_{ij}.
\end{equation}
Let us compute these quantities for the examples from the previous sections.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;em&gt;The Sphere $S^2$.&lt;/em&gt;
Using equation the Riemann tensor we computed for $S^2$, we can compute the Ricci and scalar curvatures. We compute each component of the Ricci tensor individually: let us begin with $R_{\theta \theta}$. By equation the expression we have for the Ricci tensor, we have that $R_{\theta \theta} = R^{\theta}_{\theta \theta\theta} + R^{\varphi}_{\theta \varphi\theta} = R^{\varphi}_{\theta \varphi \theta}$, since the Riemann tensor is antisymmetric in the last two indices (it is a matrix-valued 2-form). Write $\Omega = \Omega_{kl} dx^k \wedge dx^l$. Since $(\Omega_{kl})^i_j = R^i_{jkl}$, and using our equation for $\Omega$, we see that $R^{\varphi}_{\theta \varphi\theta} = 1$. Similarly, we compute $R_{\varphi\varphi} = R^{\theta}_{\varphi \theta\varphi} + R^{\varphi}_{\varphi \varphi\varphi} = R^{\theta}_{\varphi \theta\varphi} = \sin^2\theta$. By symmetry of the Riemann tensor, we see that $R_{\theta\varphi} = R_{\varphi\theta} = 0$. Thus, the Ricci tensor (written in matrix form) is
\begin{equation}
\text{Ricci} = \begin{pmatrix}  1 &amp;amp; 0 \\\  0 &amp;amp; \sin^2\theta \end{pmatrix}.
\end{equation}
The scalar curvature is now easy to compute:
\begin{equation}
R = g^{\theta\theta}R_{\theta\theta} + g^{\varphi\varphi} R_{\varphi\varphi} = 1 + \frac{1}{\sin^2\theta}\sin^2\theta = 2,
\end{equation} so that the sphere is of constant positive curvature, as expected. In fact, the scalar curvature computed in this fashion for surfaces is always twice the Gaussian curvature.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exerise.&lt;/strong&gt;
Compute the Ricci and scalar curvatures of $S^2$ using stereographic coordinates. &lt;em&gt;Solution:&lt;/em&gt; The Ricci curvature is
\begin{equation}
\text{Ricci} = \begin{pmatrix}  4/\rho &amp;amp; 0 \\\  0 &amp;amp; 4/\rho \end{pmatrix},
\end{equation}
and the scalar curvature is $R = 2$, which is consistent with the result from the previous example.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Torus $\mathbb{T}^2$.&lt;/em&gt;
The Ricci and scalar curvatures of the torus can be found by directly applying the formalism to the Riemann tensor we derived on the torus. By symmetry of the Riemann tensor, we again see that the components $R_{\theta\varphi} = R_{\varphi \theta} = 0$. It remains to compute the diagonal entries. Using the expression for the Ricci curvature, we have that $R_{\theta \theta} = R^{\theta}_{\theta \theta\theta} + R^{\varphi}_{\theta \varphi\theta} = R^{\varphi}_{\theta \varphi \theta} = \frac{r\cos\theta}{R+r\cos\theta}.$ Similarly, we compute $R_{\varphi\varphi}$ to be $R_{\varphi\varphi} = R^{\theta}_{\varphi \theta\varphi} = \frac{\cos\theta\left(R+r\cos\theta\right)}{r}$. The Ricci tensor is therefore
\begin{equation}
\text{Ricci} = \begin{pmatrix}  \frac{r\cos\theta}{R+r\cos\theta} &amp;amp; 0 \\\  0 &amp;amp; \frac{\cos\theta\left(R+r\cos\theta\right)}{r} \end{pmatrix}.
\end{equation}
We can then compute the scalar curvature as
\begin{equation}
R = g^{\theta\theta}R_{\theta\theta} + g^{\varphi\varphi} R_{\varphi\varphi} = \frac{1}{r^2}\left( \frac{r\cos\theta}{R+r\cos\theta}\right) + \frac{1}{(R+r\cos\theta)^2} \left(\frac{\cos\theta\left(R+r\cos\theta\right)}{r} \right) = \frac{2\cos\theta}{r(R+r\cos\theta)}.
\end{equation}
As a consistency check, let us compute the integrated total curvature:
\begin{align}
\int R dA  = \iint\ R \sqrt{g} d\theta d\varphi = \iint \left(\frac{2\cos\theta}{r(R+r\cos\theta)}\right) r(R+r\cos\theta) d\theta d\varphi = \iint 2\cos \theta d\theta d\varphi = 0,
\end{align}
which is consistent with the Gauss-Bonnet theorem ($\int_{\mathbb{T}^2} R dA  = 4\pi \chi (\mathbb{T}^2) = 0$).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;em&gt;The Upper Half Plane $\mathbb{H}$.&lt;/em&gt;
We now compute the derived curvature quantities for the upper half plane. By symmetry, the off-diagonal components of the Ricci tensor are zero. The $xx$ component of the Ricci tensor is: $R_{xx} = R^x_{xxx} + R^y_{xyx} =  -1/y^2$. Similarly, $R_{yy} = R^x_{yxy} + R^y_{yyy} = -1/y^2$. Thus, the Ricci tensor is
\begin{equation}
\text{Ricci} = \begin{pmatrix}  -1/y^2 &amp;amp; 0 \\\  0 &amp;amp; -1/y^2 \end{pmatrix}.
\end{equation}
We can then compute the scalar curvature to be
\begin{equation}
R = g^{xx} R_{xx} + g^{yy} R_{yy} = -1 -1 = -2,
\end{equation}
So that the upper half plane is of constant negative curvature, as expected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Compute the Ricci and scalar curvatures of the disc with the hyperbolic metric. &lt;em&gt;Solution:&lt;/em&gt; Recall that $\sigma(x,y) = 1 - x^2 - y^2$. The Ricci tensor is:
\begin{equation}
\text{Ricci} = \begin{pmatrix}  -2/\sigma^2 &amp;amp; 0 \\\  0 &amp;amp; -2/\sigma^2 \end{pmatrix}.
\end{equation}
The scalar curvature is $R = -2$, which agrees with the constant negative-curvature solution of the previous example.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The 3-sphere $S^3$.&lt;/em&gt;
Using the expression for the Riemann tensor of the $3$-sphere, and the same procedure as before,
we compute the Ricci tensor to be
\begin{equation} \label{3-sphere-Ricci}
\text{Ricci} = 2\begin{pmatrix} 1 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; \sin^2\psi &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; \sin^2\psi \sin^2 \theta \end{pmatrix}.
\end{equation}
Taking the trace of this against the metric tensor, we compute the scalar curvature to be
\begin{equation}
R = g^{\psi\psi} R_{\psi \psi} + g^{\theta\theta} R_{\theta \theta} + g^{\varphi \varphi} R_{\varphi\varphi}
= 2 + 2 + 2 = 6,
\end{equation}
so that the $3$-sphere is a 3-manifold of constant curvature.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;em&gt;The left-invariant metric on $S^3 \cong SU(2)$ &amp;amp; Hopf coordinates.&lt;/em&gt;
Using the expression we derived for the Riemann tensor in Hopf coordinates, we compute the nontrivial components of the Ricci tensor to be
$R_{\eta \eta} = 2$, $R_{\xi_1 \xi_1} = 2\sin^2 \eta$, and $R_{\xi_2 \xi_2} = 2\cos^2 \eta$, so that
\begin{equation}
\text{Ricci} = 2\begin{pmatrix} 1 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; \sin^2\psi &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; \cos^2\eta \end{pmatrix}.
\end{equation}
Contracting with the metric tensor, we obtain an expression for the scalar curvature:
\begin{equation}
R = g^{\eta \eta} R_{\eta \eta} + g^{\xi_1 \xi_1} R_{\xi_1 \xi_1} + g^{\xi_2 \xi_2} R_{\xi_2 \xi_2} = 2+2+2 = 6,
\end{equation}
which coincides with our previous result for the scalar curvature of $S^3$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The trifecta of Hilbert spaces on the unit disc</title>
      <link>/talk/hilbert-spaces-on-the-disc/</link>
      <pubDate>Thu, 01 Apr 2021 11:16:32 -0400</pubDate>
      <guid>/talk/hilbert-spaces-on-the-disc/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/1TB9cLz5Ja0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Hilbert spaces are common. Connection among them is rare. With this spirit, we will see the connection
among the three classical Hilbert spaces on the unit disc U. These Hilbert spaces are Hardy ($H^2(U)$), Dirichlet
($D^2(U)$) and lastly Bergman ($A^2(U)$). The idea comes from the intuition of evolving the remarkable identity called
as Littlewood Paley Identity for Bergman and Dirichlet spaces. During the presentation, audience will get to know more
about this identity. And also, the importance of this identity in the sense that how it relates to the Nevanlinna
Counting function. Nevanlinna Counting function $N_\phi(w)$ measures the &amp;lsquo;affinity&amp;rsquo; that $\phi$ has for value $w$ where
$\phi$ is a holomorphic map on $U$. Besides this, we will also see more connection among Hardy, Dirichlet and Riemann Zeta functions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Essentials of Viscosity Solutions in $R^n$</title>
      <link>/talk/the-essentials-of-viscosity-solutions/</link>
      <pubDate>Tue, 23 Mar 2021 16:11:32 -0400</pubDate>
      <guid>/talk/the-essentials-of-viscosity-solutions/</guid>
      <description>&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/0FRNeTc6Xyc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/I_fsCAW3xrM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/v3F4kv8f9v8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; In their 1992 paper, authors Michael Crandall, Hitoshi Ishii, and Pierre-Louis Lions describe a class of solutions to certain 2nd order PDEs in R^n which they call &amp;ldquo;viscosity solutions&amp;rdquo;. In brief, a viscosity solution to an appropriate PDE is a continuous function possessing pointwise estimates for derivatives; they also possess a comparison principle which enables one to prove uniqueness of solutions to Dirichlet and Cauchy-Dirichlet problems. In this talk we will introduce the correct family of PDE for viscosity solutions; define viscosity solutions and prove their basic properties; discuss various results comparing viscosity and other notions of solutions; and, time permitting, we will also introduce a semicontinuous Perron&amp;rsquo;s method for existence of viscosity solutions and discuss parabolic viscosity solutions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A short Introduction to the Theta Functions</title>
      <link>/talk/introduction-to-theta-function/</link>
      <pubDate>Tue, 23 Mar 2021 16:10:38 -0400</pubDate>
      <guid>/talk/introduction-to-theta-function/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/cV9YMgCozsw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The theory of theta function plays an important role in finding exact (quasi-) periodic solutions to the nonlinear PDEs. In this talk, we will introduce first the concepts of Riemann’s theta function (with characteristics) and Jacobi’s theta function. Then some of the simplest properties of those theta functions will be presented. If time permits, we will discuss a little bit about algebraic-geometry methods of constructing exact solution to KP equation.😄&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Methods of nonlinear equations in the theory of algebraic geometry</title>
      <link>/talk/riemann-schottky-problem/</link>
      <pubDate>Mon, 15 Feb 2021 13:50:43 -0500</pubDate>
      <guid>/talk/riemann-schottky-problem/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/asVgNLcnAz0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Methods of nonlinear equations in the theory of algebraic geometry&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Nathan Hayford&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date/time&lt;/strong&gt;: Wednesday, February 17th, 3:00pm-4:00pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;:
The Riemann-Schottky problem, in modern language, asks for a characterization of Jacobian varieties of among abelian varieties. From Riemann’s point of view, the question was to address the discrepancy between the 3g-3-dimensional space of moduli for compact Riemann surfaces of genus g from the g(g+1)/2-dimensional space of Siegel matrices, in which the space of Riemann surfaces embeds, via the period matrix. The problem remained open for almost a millennium. The first hint of a solution to this problem was found (somewhat surprisingly) by I. Krichever (1977), when he showed (more or less) that the period matrix for a Riemann surface could be used to construct tau functions for the KP hierarchy. S. Novikov conjectured that the converse also held, thus answering the Riemann-Schottky problem a la methods of integrable systems. This turned out to indeed be the case, with a proof from T. Shiota (1986) shortly thereafter. In this talk, I will attempt to fill in some of the details of this story, and explain some of the consequences of this theorem, both in integrable systems and algebraic geometry.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weyl&#39;s law and  hearing the area of a drum</title>
      <link>/talk/weyl-law-hear-area-drum/</link>
      <pubDate>Wed, 03 Feb 2021 14:24:34 -0500</pubDate>
      <guid>/talk/weyl-law-hear-area-drum/</guid>
      <description>&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/HiCA0n6x3Jo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/-nm5mjcNYDA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Weyl&amp;rsquo;s law and hearing the area of a drum&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Louis Arenas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date/Time&lt;/strong&gt;: Wednesday, February 3rd, 3:00pm-4:00pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;:
H. Lorentz in 1910 gave a series of lectures titled &amp;ldquo;Old and new problems in physics&amp;rdquo;. In the last lecture he mentions that J. Jean is interested in studying the growth rate of overtones of an electromagnetic wave in a bounded domain. He conjectures that the growth rate is independent of the shape of the domain and depends only on the volume and dimension of the domain. Herman Weyl, who was in attendance, precisely describes how fast these overtones grow two years after this lecture series. Since Weyl&amp;rsquo;s law holds for vibrating membranes, we start by describing the overtones of a drumhead via Dirichlet eigenvalues, compute the eigenvalues explicitly for a string with fixed ends, and build towards a proof of Weyl&amp;rsquo;s law in 2D for simply connected domains.&lt;/p&gt;
&lt;p&gt;Link to the Zoom Meeting: &lt;a href=&#34;https://zoom.us/j/92117761401&#34;&gt;https://zoom.us/j/92117761401&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prodsimplicial and p-path complexes on directed graphs</title>
      <link>/talk/p-path-complexes-on-directed-graphs/</link>
      <pubDate>Mon, 18 Jan 2021 14:09:46 -0500</pubDate>
      <guid>/talk/p-path-complexes-on-directed-graphs/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/9rlzTWD8o3o&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;:  Prodsimplicial and p-path complexes on directed graphs&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;:  Lina Fajardo Gomez&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date/Time&lt;/strong&gt;: Wednesday, January 20th, 2020&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: At face value, a graph is a simplicial complex that only has faces of dimension 0 (vertices) and 1 (edges). However, given appropriate definitions, higher dimensional cells can emerge. We present two such definitions, prodsimplicial complexes and p-path complexes, and compare in examples the homology groups obtained for different graphs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zoom Link&lt;/strong&gt;: &lt;a href=&#34;https://zoom.us/j/96345929862&#34;&gt;https://zoom.us/j/96345929862&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Strong Asymptotics of Planar Orthogonal Polynomials: Gaussian Weight Perturbed by Point Charges</title>
      <link>/talk/asymptotic-op/</link>
      <pubDate>Mon, 09 Nov 2020 13:22:22 -0500</pubDate>
      <guid>/talk/asymptotic-op/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Strong Asymptotics of Planar Orthogonal Polynomials: Gaussian Weight Perturbed by Point Charges&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker:&lt;/strong&gt; Meng Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;

&lt;a href=&#34;https://drive.google.com/file/d/14SWXLKDks7bhNrO4tt3XrOQig0-uBWfD/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computing Reflection Coefficient of Schrondinger Operator via WKB method</title>
      <link>/posts/wkb-reflection/</link>
      <pubDate>Sun, 08 Nov 2020 23:25:17 -0500</pubDate>
      <guid>/posts/wkb-reflection/</guid>
      <description>&lt;p&gt;See the PDF:
&lt;script type=&#34;text/javascript&#34; src=&#34;/js/pdf-js/build/pdf.js&#34;&gt;&lt;/script&gt;
&lt;style&gt;
#the-canvas {
  border: 1px solid black;
  direction: ltr;
  width: 100%;
  height: auto;
}
#paginator{
    text-align: center;
    margin-bottom: 10px;
}
&lt;/style&gt;

&lt;div id=&#34;paginator&#34;&gt;
    &lt;button id=&#34;prev&#34;&gt;Previous&lt;/button&gt;
    &lt;button id=&#34;next&#34;&gt;Next&lt;/button&gt;
    &amp;nbsp; &amp;nbsp;
    &lt;span&gt;Page: &lt;span id=&#34;page_num&#34;&gt;&lt;/span&gt; / &lt;span id=&#34;page_count&#34;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;embed-pdf-container&#34;&gt;
    &lt;canvas id=&#34;the-canvas&#34;&gt;&lt;/canvas&gt;
&lt;/div&gt;

&lt;script type=&#34;text/javascript&#34;&gt;
window.onload = function() {



var url = &#34;\/pdf\/wkb.pdf&#34;;


var pdfjsLib = window[&#39;pdfjs-dist/build/pdf&#39;];


pdfjsLib.GlobalWorkerOptions.workerSrc = &#39;/js/pdf-js/build/pdf.worker.js&#39;;


var pdfDoc = null,
    pageNum = 1,
    pageRendering = false,
    pageNumPending = null,
    scale = 3,
    canvas = document.getElementById(&#39;the-canvas&#39;),
    ctx = canvas.getContext(&#39;2d&#39;);



function renderPage(num) {
  pageRendering = true;
  
  pdfDoc.getPage(num).then(function(page) {
    var viewport = page.getViewport({scale: scale});
    canvas.height = viewport.height;
    canvas.width = viewport.width;

    
    var renderContext = {
      canvasContext: ctx,
      viewport: viewport
    };
    var renderTask = page.render(renderContext);

    
    renderTask.promise.then(function() {
      pageRendering = false;
      if (pageNumPending !== null) {
        
        renderPage(pageNumPending);
        pageNumPending = null;
      }
    });
  });

  
  document.getElementById(&#39;page_num&#39;).textContent = num;
}



function queueRenderPage(num) {
  if (pageRendering) {
    pageNumPending = num;
  } else {
    renderPage(num);
  }
}



function onPrevPage() {
  if (pageNum &lt;= 1) {
    return;
  }
  pageNum--;
  queueRenderPage(pageNum);
}
document.getElementById(&#39;prev&#39;).addEventListener(&#39;click&#39;, onPrevPage);



function onNextPage() {
  if (pageNum &gt;= pdfDoc.numPages) {
    return;
  }
  pageNum++;
  queueRenderPage(pageNum);
}
document.getElementById(&#39;next&#39;).addEventListener(&#39;click&#39;, onNextPage);



pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {
  pdfDoc = pdfDoc_;
  document.getElementById(&#39;page_count&#39;).textContent = pdfDoc.numPages;

  
  renderPage(pageNum);
});
}

&lt;/script&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fractional Calculus Approach for Flow Model in Porous Media</title>
      <link>/talk/fractional-calculus-approach/</link>
      <pubDate>Sun, 08 Nov 2020 23:12:31 -0500</pubDate>
      <guid>/talk/fractional-calculus-approach/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Jb1eEDQBUPw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Fractional Calculus Approach for Flow Model in Porous Media&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;
The Fractional Calculus approach is introduced into reservoir simulation. A three-dimensional relaxation model for viscoelastic fluid is built. The model based on the exact solution in Laplace space for some unsteady flow-Maxwell flow in an infinite reservoir  is obtained by Laplace transform and Fourier transform.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On solving the Dirichlet problem</title>
      <link>/talk/dirchlet-problem/</link>
      <pubDate>Mon, 02 Nov 2020 23:12:31 -0500</pubDate>
      <guid>/talk/dirchlet-problem/</guid>
      <description>&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Vu6MB6TQuns&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;strong&gt;Title&lt;/strong&gt;: On solving the Dirichlet problem&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Louis Arenas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: October 23rd&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 11:00 pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: The talk will start by reviewing the mainstream strategies for solving the problem on simply connected domains, with particular attention to the disk. It is hoped that we have enough time to talk about Malmheden&amp;rsquo;s strategy for solving the problem on the disk. The talk should be of interest to anyone taking or wanting a review of basic concepts in complex analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Survey of Equilibrium in the Plane</title>
      <link>/talk/a-survey-of-equilibruim-in-the-plane/</link>
      <pubDate>Wed, 02 Sep 2020 15:01:51 -0400</pubDate>
      <guid>/talk/a-survey-of-equilibruim-in-the-plane/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Hye1gkDkCxU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: A survey of equilibrium in the plane&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Nathan Hayford&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: Friday, September 4th&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 11:00am&lt;/p&gt;
&lt;p&gt;Abstract: Finding equilibria of charge distributions in the plane is a classical problem, which has found numerous applications in many other areas of mathematics and physics, including approximation theory, random matrices, quantum gravity, and integrable systems, among others. In this talk, I will survey some classical results from the theory of equilibrium distributions (using the language of potential theory), and provide some physical intuition for why such formulas hold. We will consider equilibrium with and without external fields. Furthermore, I will discuss some of the more modern applications of the subject; in particular, I will sketch how the equilibrium measures help us describe the asymptotics of certain families of orthogonal polynomials, as well how such problems appear in integrable systems, if time permits.&lt;/p&gt;
&lt;p&gt;Link to zoom meeting: &lt;a href=&#34;https://zoom.us/j/93365223865&#34;&gt;https://zoom.us/j/93365223865&lt;/a&gt;
&lt;strong&gt;Video&lt;/strong&gt;: 
&lt;a href=&#34;https://www.youtube.com/watch?v=Hye1gkDkCxU&amp;amp;t=2780s&amp;amp;ab_channel=USFGradMath&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Asymptotic Methods in Analysis</title>
      <link>/talk/asymptotic-methods-in-analysis/</link>
      <pubDate>Sat, 22 Aug 2020 14:47:05 -0400</pubDate>
      <guid>/talk/asymptotic-methods-in-analysis/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/nDw2wLPQv3M&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Asymptotic Methods in Analysis&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Fudong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: Thursday, 27th August.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 2:00 pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: In this talk, I will introduce the history of several well-known asymptotic methods in analysis. Then we will apply those methods via examples. The first example would be to apply Laplace&amp;rsquo;s method to get one of the oldest asymptotic results, the Stirling&amp;rsquo;s formula, i.e.
$$n!\sim e^{-n}n^n\sqrt{2\pi n},n\rightarrow \infty.$$ Next example is to apply the saddle point method to get the asymptotic of the integral
$$\int_{\mathbb{R}}f(z)e^{it(z^2+z)}dz,\quad t\rightarrow \infty .$$ Then I will introduce one recently developed asymptotic method called $\bar{\partial}-$steepest descent method, and apply it to study the long-time asymptotic for the linear Schrödinger equation:
$$
iu_t+u_{xx}=0,\quad
u(x,0)=q(x)\in H^1(\mathbb{R}).
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;zoom&lt;/strong&gt;:
&lt;a href=&#34;https://zoom.us/j/98760291675&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Primes of the Form $p = x^2 &#43; ny^2$</title>
      <link>/talk/primes-of-the-form/</link>
      <pubDate>Mon, 03 Aug 2020 10:44:14 -0400</pubDate>
      <guid>/talk/primes-of-the-form/</guid>
      <description>&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/VBPrP4nChbo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;strong&gt;Title&lt;/strong&gt;: Primes of the form $p = x^2 + ny^2$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Louis Arenas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: Thursday, August 6th&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 2:00pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: In a letter to Mersenne, dated 1640, Fermat formulated a problem asking for sufficient conditions to the titular primes in the case when $n=1,2,3.$ The first well known proof to the original problem (in the case for when $n=1$) was published by Euler in 1749, with the other two cases being completed in 1772. Naturally, Euler attempted to generalize the problem for arbitrary integers n &amp;gt; 0; while not producing any proofs, the ideas that Euler touched on would be expanded by Legendre, Gauss, and Lagrange. The goal for this talk is to outline a strategy that answers Fermat&amp;rsquo;s original problem along with a few more cases. If time permits, we&amp;rsquo;ll just mention a result that solves the general problem for countably many n integers.&lt;/p&gt;
&lt;p&gt;Link to Zoom meeting: &lt;a href=&#34;https://zoom.us/j/98801740991&#34;&gt;https://zoom.us/j/98801740991&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Video: 
&lt;a href=&#34;https://drive.google.com/file/d/1iVpAtU8iRoVYILWLTB6MgfUe9ugrYsme/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Drive&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Matroids</title>
      <link>/talk/introduction-to-matroids/</link>
      <pubDate>Mon, 27 Jul 2020 22:36:21 -0400</pubDate>
      <guid>/talk/introduction-to-matroids/</guid>
      <description>&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/SljIEkfzloA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;strong&gt;Title&lt;/strong&gt;: Introduction to matroids&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Lina Fajardo Gomez&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: Thursday, July 30th&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 2:00pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: Matroids are mathematical objects that abstract the notion of linearly independent sets. They have applications in many areas of mathematics, ranging from combinatorics to field theory. In this talk I will give an overview of matroids, why they are studied and why I in particular took an interest in them.&lt;/p&gt;
&lt;p&gt;Link to Zoom meeting: &lt;a href=&#34;https://zoom.us/j/93617336971&#34;&gt;https://zoom.us/j/93617336971&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video&lt;/strong&gt;: 
&lt;a href=&#34;https://drive.google.com/file/d/1V3Zg0KFIyk32S_mg2mMJ9I4lpIBchM70/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Drive&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;: 
&lt;a href=&#34;https://drive.google.com/file/d/1tP9Qh3RJCplCVUsds1U9EBmmZd7hEZr0/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Drive&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Doing research as a student: tips and tricks</title>
      <link>/talk/doing-research-as-a-student/</link>
      <pubDate>Mon, 20 Jul 2020 11:41:42 -0400</pubDate>
      <guid>/talk/doing-research-as-a-student/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Doing research as a student: tips and tricks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Giacomo Micheli&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: Thursday, July 23rd&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 2:00pm&lt;/p&gt;
&lt;p&gt;Abstract:
In this talk I will first explain how the publication process works. Then, I will go through some advice to do research as a student. In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;some of the methods I found most effective when I was doing independent research as a student.&lt;/li&gt;
&lt;li&gt;some general advice related to the process of submitting a paper to a journal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Link to Zoom meeting: &lt;a href=&#34;https://zoom.us/j/95032330155&#34;&gt;https://zoom.us/j/95032330155&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Gentle Introduction to Infinite-Dimensional Lie Algebras</title>
      <link>/talk/usf/infinite-lie-algebra/</link>
      <pubDate>Sun, 05 Jul 2020 17:55:36 -0400</pubDate>
      <guid>/talk/usf/infinite-lie-algebra/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/6789LcHqwP4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: A Gentle Introduction to Infinite-Dimensional Lie Algebras&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Nathan Hayford&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: Thursday, July 9th&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 2:00 pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: Infinite-dimensional Lie algebras have found applications in conformal field theory, integrable systems, and number theory, among other areas of mathematics. However, one wonders exactly how these algebras arise in such contexts, and moreover what is even meant by “infinite-dimensional Lie algebra” in the first place. In this talk, I will try to give a friendly introduction to the field of infinite-dimensional Lie algebras, and try to give the listener some operational familiarity with these structures by providing a few explicit examples. Furthermore, I will mention a few specific problems in mathematics where such algebras have been useful.&lt;/p&gt;
&lt;p&gt;Link to Zoom meeting: &lt;a href=&#34;https://zoom.us/j/5082038044?pwd=YUVBdkZvZU5RYXBYZmU5ZGRFZnE1Zz09&#34;&gt;https://zoom.us/j/5082038044?pwd=YUVBdkZvZU5RYXBYZmU5ZGRFZnE1Zz09&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video:&lt;/strong&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1by6zBTatZNO3CxFXer75CTtHR3hujoP5/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Drive&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides:&lt;/strong&gt; 
&lt;a href=&#34;https://drive.google.com/file/d/1zPw8vC7dRtkJMMvsjc5SuMQeIlKBsUxV/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Drive&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Nevanlinna-Pick Interpolation Problem</title>
      <link>/talk/nevanlinna-pick-problem/</link>
      <pubDate>Mon, 29 Jun 2020 13:37:03 -0400</pubDate>
      <guid>/talk/nevanlinna-pick-problem/</guid>
      <description>&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/C8-0k_UJnj8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;strong&gt;Title&lt;/strong&gt;: The Nevanlinna-Pick Interpolation Problem&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: John Kyei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: Thursday, July 2nd&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 2:00pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;:
In this presentation, I will give an introductory discussion on the general Nevanlina-Pick Interpolation problem . Formulation of the Pick Problem and the nature of solutions to the Extremal Pick Problem. Subsequently, Nevalinna Formulation.
We will  also look at  the Finite Blaschke Products, Basic  analytic  and algebraic properties such as the characterizations of holomorphic functions on the disk, approximation and Differentiation of the Finite Blaschke Product.&lt;/p&gt;
&lt;p&gt;Link to Zoom Meeting: &lt;a href=&#34;https://zoom.us/j/99109270972&#34;&gt;https://zoom.us/j/99109270972&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video&lt;/strong&gt;: 
&lt;a href=&#34;https://drive.google.com/file/d/1_eVKFcjc5syRvp8s0CMusthbjAcX9EmX/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;:
&lt;a href=&#34;https://drive.google.com/file/d/1wXFhZwWLGglGkBkSF67V9GKNmdav8GKb/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic Games for PDE</title>
      <link>/talk/usf/stochastic-games/</link>
      <pubDate>Sat, 20 Jun 2020 22:58:12 -0400</pubDate>
      <guid>/talk/usf/stochastic-games/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/TWf_XLzzUPI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: Stochastic Games for PDE&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: 2:00 pm, Thursday, June 25th, 2020&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Forrest, Zachary&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: This talk will serve as an introduction to techniques of stochastic games and their application to proving the existence of viscosity solutions to second order, degenerate-elliptic equations. We will define such equations and the class of viscosity solutions; give definitions and properties of martingales and stopping times, including Doob&amp;rsquo;s Optimal Sampling theorem; and give examples of stochastic games for Pucci&amp;rsquo;s maximal operators and the p-Laplace equation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video&lt;/strong&gt;:
&lt;a href=&#34;https://drive.google.com/file/d/1QlZqbhVYxEg6V5DHfCKCYfzpxzelyGXH/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Drive&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;:
&lt;a href=&#34;https://drive.google.com/file/d/1zSpf3r2OzizRkmCB7auiWQ5-fH9F1-81/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Drive&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Brief Introduction to Differential Geometry and Minimal Surfaces</title>
      <link>/talk/minimal-surfaces/</link>
      <pubDate>Sat, 13 Jun 2020 14:54:17 -0400</pubDate>
      <guid>/talk/minimal-surfaces/</guid>
      <description>&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/-FYXPfWy-bg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;strong&gt;Title&lt;/strong&gt;: A Brief Introduction to Differential Geometry and Minimal Surfaces&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: Thursday, June 18&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 2:00 pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Hayden Hunter (University of Florida)&lt;/p&gt;
&lt;p&gt;Abstract: For this lecture we will be providing a small introduction to Differential Geometry including the definition of a regular surface, first and second Fundamental Forms, and Gauss and Mean Curvature. From here we shall move to describing what a minimal surface is as well as proving their existence and certain properties of minimal surfaces.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video&lt;/strong&gt;:
&lt;a href=&#34;https://drive.google.com/file/d/1h140EIRR51Ny9tkXAHYJQFapxTHXuPRb/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Drive&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Elementary Introduction to Fredholm Determinants</title>
      <link>/talk/fredholm-determinants/</link>
      <pubDate>Mon, 08 Jun 2020 13:01:12 -0400</pubDate>
      <guid>/talk/fredholm-determinants/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/m1CAUg0im4M&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Title&lt;/strong&gt;: An Elementary Introduction to Fredholm Determinants&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker&lt;/strong&gt;: Fudong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: Thursday, June 11&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time&lt;/strong&gt;: 2:00 pm&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: At the beginning of the 20th century, Fredholm introduced a way to solve a class of linear integral equations which are now called them Fredholm integral equations. In his 1903&amp;rsquo;s paper, Fredholm introduced a determinant of a functional, which now is called Fredholm determinant. In this lecture, we will begin with the definition of determinant in the finite-dimensional spaces and review some properties of the determinants. After that, we will focus on the determinant of form $det(I+F)$ where $F$ is &amp;ldquo;small&amp;rdquo;. Following 
&lt;a href=&#34;https://drive.google.com/file/d/1ebU0SUTSXsVQclHXe9sNFJg3YZmiGmtF/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fredholm&amp;rsquo;s 1903 paper&lt;/a&gt;, we will discuss Fredholm determinant $D_f$ and explore some properties of the determinant and how it connects to the finite determinants. Lastly, we will discuss how to construct the famous &amp;ldquo;soliton&amp;rdquo; solution to the equation of Koreteweg-de Vries by the so-called Fredholm determinant method introduced by Pöppe(1983).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video&lt;/strong&gt;: 
&lt;a href=&#34;https://drive.google.com/file/d/1ghnW1hZSzLleX_UQ-t9g7BcbN1-RonFt/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;google drive&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantum invariants of framed links from cohomology of self-distributive structures</title>
      <link>/talk/usf/quantum-invariants/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/talk/usf/quantum-invariants/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker:&lt;/strong&gt; Emanuele Zappala&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time:&lt;/strong&gt; 2:00pm, Thursday, June 4&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; In this talk I will give a brief introduction to knot theory and the notion of knot invariant. Next I will describe the process of algebraization of knot theory which naturally gives Yang-Baxter operators, Yetter-Drinfeld modules and racks/quandles etc. After recalling the definition of cohomology of racks, I will proceed to show how to construct an invariant of links and knotted surfaces from cocycles. I will therefore explain some recent joint works with M. Elhamdadi and M. Saito relating self-distributivity of higher arity structures and invariants of framed links, with a (categorical) perspective on how to realize them as quantum invariants.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zoom links&lt;/strong&gt;: 
&lt;a href=&#34;https://zoom.us/j/99982471312?pwd=Z0NTR2UraHZTQXI5TkdzNlM5K0FPUT09&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;click me.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;: 
&lt;a href=&#34;https://drive.google.com/file/d/1Pef6VJyfGRqouX5uf-QIy3oITf0NW85P/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;google drive&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Darboux Transformation and Soliton Equations </title>
      <link>/talk/usf/darboux-transformation-and-soliton-equations/</link>
      <pubDate>Thu, 20 Feb 2020 20:47:23 -0400</pubDate>
      <guid>/talk/usf/darboux-transformation-and-soliton-equations/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Darboux transformation and soliton equations.&lt;br&gt;
&lt;strong&gt;Speaker:&lt;/strong&gt; Yehui Huang (North China Electric Power University).&lt;br&gt;
&lt;strong&gt;Date:&lt;/strong&gt; Thursday, February $20^{th}$&lt;br&gt;
&lt;strong&gt;Time:&lt;/strong&gt; 11:00am&lt;br&gt;
&lt;strong&gt;Place:&lt;/strong&gt; CIS 1045&lt;br&gt;
&lt;strong&gt;Abstract:&lt;/strong&gt; In this talk, I will introduce a transformation originated
by Darboux in his study of the linear Sturm-Liouville problem. By
generalized Darboux transformations, I will present a solution formula
with multiple parameters for some soliton equations, including KdV and
NLS. After taking special choices for the seed solution and the
eigenfunctions, different types of exact solutions are derived, which
include solitons, breathers and rogue wave solutions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Calderón Problem</title>
      <link>/talk/usf/the-calderon-problem/</link>
      <pubDate>Sat, 15 Feb 2020 20:55:08 -0400</pubDate>
      <guid>/talk/usf/the-calderon-problem/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; The Calderón problem.&lt;br&gt;
&lt;strong&gt;Speaker:&lt;/strong&gt; Lina Fajardo Gomez.&lt;br&gt;
&lt;strong&gt;Date:&lt;/strong&gt; Thursday, February $13^{th}$&lt;br&gt;
&lt;strong&gt;Time:&lt;/strong&gt; 11:00am&lt;br&gt;
&lt;strong&gt;Place:&lt;/strong&gt; CIS 1023&lt;br&gt;
&lt;strong&gt;Abstract:&lt;/strong&gt; What do oil drilling and tumor detection have in common?
How could nonlinear equations be easier to work with? The Calderón
problem was first introduced as an inverse boundary value problem: how
information about the boundary of a domain can be used to determine
information about its interior. We will discuss the set up, some
applications, and a proposed solution for a class of Calderón type
inverse problems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variation Problem and How to Solve It</title>
      <link>/talk/usf/variation-problem-and-how-to-solve-it/</link>
      <pubDate>Sat, 01 Feb 2020 21:02:58 -0400</pubDate>
      <guid>/talk/usf/variation-problem-and-how-to-solve-it/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Variational Problems and How to Solve Them.&lt;br&gt;
&lt;strong&gt;Speaker:&lt;/strong&gt; Nathan Hayford.&lt;br&gt;
&lt;strong&gt;Date:&lt;/strong&gt; Thursday, February $6^{th}$&lt;br&gt;
&lt;strong&gt;Time:&lt;/strong&gt; 11:00am&lt;br&gt;
&lt;strong&gt;Place:&lt;/strong&gt; CIS 1023&lt;br&gt;
&lt;strong&gt;Abstract:&lt;/strong&gt; Many problems in math and physics arise as problems of
extremizing some functional. Such problems include the isoperimetric
problem, minimal surface problem, and the formulation of Lagrangian
mechanics. We shall discuss the calculus of variations, and provide
worked out examples of how these techniques may be applied to solve such
problems. We will also discuss how such techniques may be applied to
write Lagrangian formulations of theories like electromagnetism and
general relativity, if time permits.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lorentz Group</title>
      <link>/talk/usf/lorentz-group/</link>
      <pubDate>Thu, 30 Jan 2020 21:05:22 -0400</pubDate>
      <guid>/talk/usf/lorentz-group/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Introduction to the Lorentz Group.&lt;br&gt;
&lt;strong&gt;Speaker:&lt;/strong&gt; Louis Arenas.&lt;br&gt;
&lt;strong&gt;Date:&lt;/strong&gt; Thursday, January $30^{th}$&lt;br&gt;
&lt;strong&gt;Time:&lt;/strong&gt; 11:00am&lt;br&gt;
&lt;strong&gt;Place:&lt;/strong&gt; CIS 1023&lt;br&gt;
&lt;strong&gt;Abstract:&lt;/strong&gt; The goal of this talk is to introduce the audience to the
definition of the Lorentz group and motivation in general relativity. We
will derive commutation relations, and if time permits show the
restricted Lorentz group is isomorphic to the Möbius group.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Riemann Hilbert Problem</title>
      <link>/talk/usf/riemann-hilbert-problem/</link>
      <pubDate>Thu, 23 Jan 2020 21:05:34 -0400</pubDate>
      <guid>/talk/usf/riemann-hilbert-problem/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Introduction to the Riemann-Hilbert Problem in $L^p$-space.&lt;br&gt;
&lt;strong&gt;Speaker:&lt;/strong&gt; Fudong Wang.&lt;br&gt;
&lt;strong&gt;Date:&lt;/strong&gt; Thursday, January $23^{rd}$&lt;br&gt;
&lt;strong&gt;Time:&lt;/strong&gt; 11:00am&lt;br&gt;
&lt;strong&gt;Place:&lt;/strong&gt; CIS 1023&lt;br&gt;
&lt;strong&gt;Abstract:&lt;/strong&gt; We will first introduce some fundamental results about the
Cauchy operator. Base on the Cauchy operator, we will make it precise
what is the RHP and what is the solution to an RHP. Then we will talk
about the two types of inhomogeneous Riemann-Hilbert problems and
establish the equivalence between them. Finally, a uniqueness theorem of
RHP in $L^p$ will be given. If time permits, its application to the
inverse scattering theory will be mentioned too.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
