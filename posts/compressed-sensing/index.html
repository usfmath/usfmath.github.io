<!DOCTYPE html>
<html lang="en-us">
<head>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>[Talk] Compressed Sensing  - Grad Math@USF</title>
<meta name="description" content="Graduate Math Seminars hosted by Nathan and Fudong">

<link rel="icon" type="image/x-icon" href="https://usfmath.github.io/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="https://usfmath.github.io/favicon.png">

<link rel="stylesheet" href="https://usfmath.github.io/css/style.css?rnd=1590975529" />




<meta property="og:title" content="[Talk] Compressed Sensing" />
<meta property="og:description" content="Title: Compressed Sensing.
Speaker: Vasile Pop.
Date: Thursday, February 27th, 2020
Place: CIS 1045
Abstract: Compressed sensing (CS) is an exciting, rapidly growing
field that has attracted considerable attention in statistics, applied
mathematics, electrical engineering and computer science. CS offers a
framework for simultaneous sensing and compression of finite/infinite
dimensional vectors, that relies on linear dimensionality reduction. The
restricted isometry property (RIP) is at the center of important
developments in compressive sensing. In $\Bbb R^n$, RIP establishes the
success of sparse recovery via basis pursuit for measurement matrices
with small restricted isometry constants $\delta_{2s}&lt;1/3$. A weaker
condition $\delta_{2s} &lt; 0.6246$ is actually sufficient to guarantee
stable and robust recovery of all $s$-sparse vectors via
$l_1$-minimization. In Hilbert space a random linear map satisfies a
general RIP with high probability and allow recovering and extending
many known compressive sampling results. In this session, I provide a
comprehensive introduction to CS. This thesis extends the known
restricted isometric projection of sparse datasets in Euclidean spaces
$\mathbb{R}^N$ down into low-dimensional subspaces
$\mathbb{R}^k, k \ll N,$ to the case of low-dimensional varieties
$\mathcal{M} \subset \mathbb{R}^N,$ of codimension $N - k = \omega(N)$.
Applications to structured/hierarchical datasets are considered." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://usfmath.github.io/posts/compressed-sensing/" />
<meta property="article:published_time" content="2020-02-27T17:24:04-04:00" />
<meta property="article:modified_time" content="2020-02-27T17:24:04-04:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[Talk] Compressed Sensing"/>
<meta name="twitter:description" content="Title: Compressed Sensing.
Speaker: Vasile Pop.
Date: Thursday, February 27th, 2020
Place: CIS 1045
Abstract: Compressed sensing (CS) is an exciting, rapidly growing
field that has attracted considerable attention in statistics, applied
mathematics, electrical engineering and computer science. CS offers a
framework for simultaneous sensing and compression of finite/infinite
dimensional vectors, that relies on linear dimensionality reduction. The
restricted isometry property (RIP) is at the center of important
developments in compressive sensing. In $\Bbb R^n$, RIP establishes the
success of sparse recovery via basis pursuit for measurement matrices
with small restricted isometry constants $\delta_{2s}&lt;1/3$. A weaker
condition $\delta_{2s} &lt; 0.6246$ is actually sufficient to guarantee
stable and robust recovery of all $s$-sparse vectors via
$l_1$-minimization. In Hilbert space a random linear map satisfies a
general RIP with high probability and allow recovering and extending
many known compressive sampling results. In this session, I provide a
comprehensive introduction to CS. This thesis extends the known
restricted isometric projection of sparse datasets in Euclidean spaces
$\mathbb{R}^N$ down into low-dimensional subspaces
$\mathbb{R}^k, k \ll N,$ to the case of low-dimensional varieties
$\mathcal{M} \subset \mathbb{R}^N,$ of codimension $N - k = \omega(N)$.
Applications to structured/hierarchical datasets are considered."/>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-104299470-3', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>





    
</head>
<body>
    <a class="skip-main" href="#main">Skip to main content</a>
    <div class="container">
        <header> 
            
                <h1 class="site-header">
    <a href="/">Grad Math@USF</a>
</h1>
<nav>
    
    
    <a class="" href="https://usfmath.github.io/about/" title="">About</a>
    
    <a class="" href="https://usfmath.github.io/tags/" title="">Topics</a>
    
    <a class="" href="https://usfmath.github.io/posts/" title="">Archive</a>
    
</nav>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

            
        </header>
        <main id="main" tabindex="-1"> 
            

    <article class="post">
        <header>
            <h1>[Talk] Compressed Sensing</h1>
        </header>
        <div class="content">
            <p><strong>Title:</strong> Compressed Sensing.</p>
<p><strong>Speaker:</strong> Vasile Pop.</p>
<p><strong>Date:</strong> Thursday, February 27th, 2020</p>
<p><strong>Place:</strong> CIS 1045</p>
<p><strong>Abstract:</strong> Compressed sensing (CS) is an exciting, rapidly growing
field that has attracted considerable attention in statistics, applied
mathematics, electrical engineering and computer science. CS offers a
framework for simultaneous sensing and compression of finite/infinite
dimensional vectors, that relies on linear dimensionality reduction. The
restricted isometry property (RIP) is at the center of important
developments in compressive sensing. In $\Bbb R^n$, RIP establishes the
success of sparse recovery via basis pursuit for measurement matrices
with small restricted isometry constants $\delta_{2s}&lt;1/3$. A weaker
condition $\delta_{2s} &lt; 0.6246$ is actually sufficient to guarantee
stable and robust recovery of all $s$-sparse vectors via
$l_1$-minimization. In Hilbert space a random linear map satisfies a
general RIP with high probability and allow recovering and extending
many known compressive sampling results. In this session, I provide a
comprehensive introduction to CS. This thesis extends the known
restricted isometric projection of sparse datasets in Euclidean spaces
$\mathbb{R}^N$ down into low-dimensional subspaces
$\mathbb{R}^k, k \ll N,$ to the case of low-dimensional varieties
$\mathcal{M} \subset \mathbb{R}^N,$ of codimension $N - k = \omega(N)$.
Applications to structured/hierarchical datasets are considered.</p>
        </div>
        


<div class="article-info">
    
        <div class="article-date">2020-02-27</div>
    
    <div class="article-taxonomies">
        
            
                <ul class="article-tags">
                    
                        <li><a href="https://usfmath.github.io/tags/compressed-sensing">#Compressed sensing</a></li>
                    
                </ul>
        
    </div>
</div>

    </article>
    


        </main>
        <footer>
            
                
                

                <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<p>Â© Fudong, 2020<br>
Powered by <a target="_blank" href="https://gohugo.io/">Hugo</a>, theme <a target="_blank" href="https://github.com/mitrichius/hugo-theme-anubis">Anubis</a>.
</p>


            
        </footer>
    </div>
</body>
</html>
