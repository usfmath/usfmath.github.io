<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Grad Math@USF</title>
    <link>/posts/</link>
      <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 28 Aug 2021 18:24:20 -0400</lastBuildDate>
    <image>
      <url>/images/icon_hu52d4978bb57ec1511a90a19194ce34ae_631017_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/posts/</link>
    </image>
    
    <item>
      <title>Random Matrix Theory</title>
      <link>/posts/randommatrices/</link>
      <pubDate>Sat, 28 Aug 2021 18:24:20 -0400</pubDate>
      <guid>/posts/randommatrices/</guid>
      <description>&lt;p&gt;Here, we discuss some selected topics and techniqes from random matrix theory. For sake of clarity, we will first
deal with 1-matrix models, and illustrate how they count the number of planar diagrams in a certain limit. Then, we
discuss the 2-matrix models, which enumerate 2-colored planar diagrams. For this, we will need the Harish-Chandra
(Itzykson-Zuber) formula, which we also derive. Before discussing the connections of these matrix models to enumeration
problems, we first derive some basic facts about the space of Hermitian matrices that we will eventually need in order
to perform calculations.&lt;/p&gt;
&lt;p&gt;We will always let $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; denote the space of $n\times n$ Hermitian matrices, i.e. matrices $X$ such that
$X^\dagger = X$ (&amp;quot;$\dagger$&amp;quot; here denotes hermitian conjugate). The dimension of this space may be
computed by noticing the following: the hermiticity condition requires that the diagonal of the matrix be real
($z = \bar{z}$ implies that $z$ is real), therefore the diagonal entries contribute $n$ variables. Above the diagonal,
there are $2 \cdot \frac{n(n-1)}{2} = n (n-1)$ variables (the factor of two coming from real + imaginary components);
therefore we find the dimension of the space of Hermitian matrices to be
\begin{align*}
\dim \mathcal{H}_n = n + 2 \cdot \frac{n(n-1)}{2} = n^2.
\end{align*}
It is often convenient for us to work in the so-called &lt;em&gt;eigenvalue variables&lt;/em&gt; on the space of Hermitian matrices.
These eigenvalues are of course real, by properties of Hermitian matrices; the remaining variables are variables in
the unitary group $U(n)/U_{diag}(n)$ and are called &lt;em&gt;angular variables&lt;/em&gt;. This is apparent, by writing
$X = U^\dagger \Lambda U$; note that replacing $U \to \text{diag}(e^{i\theta_1},\cdots e^{i\theta_n} )U$ does not change the representation.
Defining $du := dU^\dagger U = -du^{\dagger}$, we find that $dX$ takes the form
\begin{align*}
dX = d(U^\dagger \Lambda U) &amp;amp;= dU^\dagger \Lambda U+ U^\dagger d\Lambda U + U^\dagger \Lambda dU \\\&lt;br&gt;
&amp;amp;= U^\dagger (d u^\dagger \Lambda + d \Lambda  + \Lambda d u) U \\\&lt;br&gt;
&amp;amp;= U^\dagger (d \Lambda + [\Lambda, d u] ) U.
\end{align*}&lt;/p&gt;
&lt;p&gt;The metric can then be computed as
\begin{align*}
g = \text{tr} (dX^\dagger dX) = \text{tr}(d \Lambda d \Lambda) + \text{tr}([\Lambda, d u])^2,
\end{align*}
which is diagonal in the coordinates $(d\lambda_i, du_{ij})$:
\begin{align*}
g = \sum_{i = 1}^n d\lambda_i^2 + 2 \sum_{i &amp;lt; j} (\lambda_i - \lambda_j)^2 (du_{ij})^2
\end{align*}
The volume form is then
\begin{align*}
d\text{vol}(X) 	&amp;amp;= \sqrt{\det g} \prod_{i=1}^n d\lambda_i \prod_{i &amp;lt; j} du_{ij}  \\\&lt;br&gt;
&amp;amp;= 2^{n(n-1)} \prod(\lambda_i - \lambda_j)^2 \prod_{i=1}^n d\lambda_i \prod_{i &amp;lt; j} du_{ij} \\\&lt;br&gt;
&amp;amp;\propto  dU \prod_{i &amp;lt; j}(\lambda_i - \lambda_j)^2 \prod_{i=1}^n d\lambda_i,
\end{align*}
where $dU$ is the Haar measure on $U(n)/U_{diag}(n)$. The quantity
\begin{equation}
V(\lambda) := \prod_{i &amp;lt; j}(\lambda_i - \lambda_j) = \det(\lambda_i^{j-1})
\end{equation}
is called the &lt;em&gt;Vandermonde determinant&lt;/em&gt;. We shall meet it often later. The exponent of two in the Vandermonde determinant
comes from the fact that there are twice as many variables $du_{ij}$, with contributions coming from real and
imaginary parts, respectively. Because it is a quantity we will need later, we also compute the Laplace-Beltrami operator on the submanifold
of the eigenvalue coordinates; this is
\begin{align*}
\Delta_{LB} &amp;amp;= \sum_{i=1}^n \frac{\partial^2}{\partial X_{ii}^2} + \frac{1}{2}\sum_{i &amp;lt; j}\left(\frac{\partial^2}{\partial \text{Re}(X_{ij})^2} +
\frac{\partial^2}{\partial \text{Im}(X_{ij})^2}\right)\\\ &amp;amp;=
\frac{1}{V^2(\lambda)}\sum_{i=1}^n\frac{\partial}{\partial \lambda_i} \left(V^2(\lambda)
\frac{\partial}{\partial \lambda_i} \cdot\right) + (\Delta \textit{ on the angular coordinates}),
\end{align*}
i.e., on only the eigenvalue coordinates, we have
\begin{equation}\label{eigenvalue-laplacian}
\Delta_{eigenvalues} = \frac{1}{V^2(\lambda)}\sum_{i=1}^n\frac{\partial}{\partial \lambda_i} \left(V^2(\lambda) \frac{\partial}{\partial \lambda_i} \cdot\right).
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;1-the-gaussian-unitary-ensemble--planar-diagrams&#34;&gt;$1$. The Gaussian Unitary Ensemble &amp;amp; Planar Diagrams.&lt;/h2&gt;
&lt;p&gt;We now demonstrate how certain integrals over $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; can be related to enumeration of planar diagrams,
a fact which was first noticed by the physicists 
&lt;a href=&#34;https://projecteuclid.org/journals/communications-in-mathematical-physics/volume-59/issue-1/Planar-diagrams/cmp/1103901558.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;E. Brezin, C. Itzykson, G. Parisi, &amp;amp; J. B. Zuber&lt;/a&gt;.
Most of the results presented here are standard; they can be found in places like 
&lt;a href=&#34;https://arxiv.org/abs/math-ph/0406013&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt; by Phillipe Di
Francesco, or in 
&lt;a href=&#34;https://www.labri.fr/perso/zvonkin/Research/matrixint.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this one&lt;/a&gt; by Alexander Zvonkin, for example.
Let $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; denote the space of $n\times n$ Hermitian matrices. We consider the measure
\begin{align*}
d\mathbb{P}_n(X) = \frac{1}{Z_n} e^{-\frac{n}{2}\text{tr} X^2} dX,
\end{align*}
where $dX$ is Haar measure on $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; (which we have just defined in the previous section), and $Z_n$ is a
normalization constant so that $\int d\mathbb{P} = 1$, called the \textit{partition function}. The ensemble of
such matrices, taken together with the probability measure $\mathbb{P}$&lt;sub&gt;n&lt;/sub&gt;, form the &lt;em&gt;Gaussian Unitary Ensemble&lt;/em&gt; (GUE).
We denote the expected value of a random variable $f(X):$ $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; $\to \mathbb{R}$ in this ensemble by $\langle f(X) \rangle_n$; explicitly,
\begin{equation}
\langle f(X) \rangle_n = \frac{1}{Z_n} dX \int_{\mathcal{H}_n} f(X) e^{-\frac{n}{2}\text{tr} X^2}.
\end{equation}
We wish to evaluate expected values of certain random variables in the large $n$ limit, in particular\footnote{Notice
that $X\in \mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; is dependent on $n$ as well; we suppress dependence to simplify notations.}
\begin{equation}
\langle \text{tr} X^k \rangle:= \lim_{n\to \infty} \frac{1}{n}\langle \text{tr} X^k \rangle_n.
\end{equation}
We claim that
\begin{equation}
\langle \text{tr} X^k \rangle = C_k,
\end{equation}
where $C_k = \frac{1}{k+1}{2k \choose k}$ is the $k^{th}$ Catalan number. This will follow from some
combinatorial arguments. We begin by proving the following:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 1.1&lt;/strong&gt;&lt;/span&gt;
Let $S\in \mathcal{H}_n$ be a fixed matrix. Then,
\begin{equation}
\langle e^{\text{tr} XS}\rangle_n = e^{\text{tr} S^2/2n}.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;
We have that
\begin{align*}
\langle e^{\text{tr} XS} \rangle_n &amp;amp;= \frac{1}{Z_n}\int_{\mathcal{H}_n}dX e^{-\frac{n}{2}\text{tr} (X^2 -
\frac{2}{n} XS)}\\\ &amp;amp;=
e^{\text{tr} S^2/2n} \frac{1}{Z_n}\int_{\mathcal{H}_n}dX e^{-\frac{n}{2}\text{tr} (X-S/n)^2} \\\&lt;br&gt;
&amp;amp;=e^{\text{tr} S^2/2n} \frac{1}{Z_n}\int_{\mathcal{H}_n}d(X-S/n)  e^{-\frac{n}{2}\text{tr} (X-S/n)^2}\\\
&amp;amp;= e^{\text{tr} S^2/2n} \frac{1}{Z_n}\int_{\mathcal{H}_n}dX e^{-\frac{n}{2}\text{tr} (X)^2}
= e^{\text{tr} S^2/2n},
\end{align*}
where in the last step, we used the fact that $dX$ is translation invariant.&lt;/p&gt;
&lt;div style=&#34;text-align: right&#34;&gt; $\square$ &lt;/div&gt;
&lt;p&gt;Consequentially, we can compute any expected value of interest by differentiating $f(S) := e^{\text{tr} S^2/2n}$
with respect to $S$, and evaluating at $S=0$. For example,
\begin{equation*}
\langle X_{ij} \rangle_n = \frac{\partial f(S)}{\partial S_{ji}} \bigg|_{S=0}, \qquad \qquad
\langle X_{i_1j_1}X_{i_2j_2}\rangle_n = \frac{\partial^2 f(S)}{\partial S_{j_1 i_1} \partial S_{j_2 i_2}}
\bigg|_{S=0}.
\end{equation*}
Explicitly, we have that&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:green&#34;&gt;&lt;strong&gt;Lemma 1.1&lt;/strong&gt;&lt;/span&gt;
For any indices $i_1,j_1,i_2,j_2$,
\begin{equation}
\langle X_{i_1j_1} \rangle_n = 0, \qquad \qquad \langle X_{i_1j_1}X_{i_2j_2}\rangle_n = \frac{1}{n}
\delta_{i_1 j_2}\delta_{i_2 j_1}.
\end{equation}
&lt;em&gt;Proof.&lt;/em&gt;
Note that the exponent of $f(S)$ is $\frac{1}{2n}\text{tr} S^2 = \frac{1}{2n}\sum_{k,l} S_{kl}S_{lk}$, and
that
\begin{equation*}
\frac{\partial}{\partial S_{j_1 i_1}}\frac{1}{2n}\sum_{k,l} S_{kl}S_{lk} =
\frac{1}{2n}\sum_{k,l} \left(\delta_{k j_1}\delta_{l i_1} S_{lk} +
S_{kl}\delta_{l j_1}\delta_{k i_1}\right) = \frac{1}{n}S_{i_1 j_1}.
\end{equation*}
Therefore, by the chain rule,
\begin{equation*}
\langle X_{i_1j_1}\rangle_n = \frac{\partial}{\partial S_{j_1 i_1}}e^{\text{tr} S^2/2n}\bigg|_{S=0}
= \frac{1}{n}S_{i_1 j_1}e^{\text{tr} S^2/2n}\bigg|_{S=0} = 0.
\end{equation*}
Similarly, by the product rule,
\begin{align*}
\langle X_{i_1j_1}X_{i_2j_2} \rangle_n &amp;amp;= \frac{\partial}{\partial S_{j_2 i_2}}\frac{1}{n}S_{i_1 j_1}
e^{\text{tr} S^2/2n}\bigg|_{S=0} \\\&lt;br&gt;
&amp;amp;= \frac{1}{n}\delta_{i_1 j_2}\delta_{i_2 j_1}e^{\text{tr} S^2/2n}\bigg|_{S=0} +
\frac{1}{n}S_{i_1 j_1}S_{i_2 j_2}e^{\text{tr} S^2/2n}\bigg|_{S=0}\\\&lt;br&gt;
&amp;amp;= \frac{1}{n}
\delta_{i_1 j_2}\delta_{i_2 j_1}.
\end{align*}&lt;/p&gt;
 &lt;div style=&#34;text-align: right&#34;&gt; $\square$ &lt;/div&gt;
&lt;p&gt;Furthermore, any &amp;ldquo;higher&amp;rdquo; expected value can be computed via &lt;em&gt;Wick&amp;rsquo;s theorem}:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 1.2.&lt;/strong&gt;&lt;/span&gt;
&lt;em&gt;(Wick&amp;rsquo;s Theorem.)&lt;/em&gt; Let $I = {(i_1,j_1),\cdots (i_{2M},j_{2M})}$ be a collection of indices,
$1 \leq i_k,j_k \leq n$. Then,
\begin{equation}
\langle \prod_{(i,j) \in I}X_{ij} \rangle_n = \sum_{\pi \in \Pi_{2M}}
\prod_{(i,j),(k,l)\in \pi} \langle X_{i j}X_{k l}\rangle_n,
\end{equation}
where $\Pi_{2M}$ is the set of all &lt;em&gt;pairings&lt;/em&gt; of the index set $I$.&lt;/p&gt;
&lt;p&gt;For example, we have that
\begin{align*}
\langle X_{i_1 j_1} X_{i_2 j_2} X_{i_3 j_3} X_{i_4 j_4}\rangle_n &amp;amp;=
\langle X_{i_1 j_1} X_{i_2 j_2}\rangle_n \langle X_{i_3 j_3} X_{i_4 j_4}\rangle_n\\\&lt;br&gt;
&amp;amp;+\langle X_{i_1 j_1} X_{i_3 j_3}\rangle_n \langle X_{i_2 j_2} X_{i_4 j_4}\rangle_n\\\&lt;br&gt;
&amp;amp;+ \langle X_{i_1 j_1} X_{i_4 j_4}\rangle_n\langle X_{i_2 j_2} X_{i_3 j_3}\rangle_n.
\end{align*}&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;
&lt;em&gt;(Sketch.)&lt;/em&gt; The theorem follows from the fact that derivatives must come in pairs for nonzero
contributions to the expected value to appear; see the proof of the lemma. It is clear that the sum
must be taken over all such pairings. The full proof of this fact is given in 
&lt;a href=&#34;https://www.labri.fr/perso/zvonkin/Research/matrixint.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zvonkin&amp;rsquo;s survey&lt;/a&gt;.&lt;/p&gt;
&lt;div style=&#34;text-align: right&#34;&gt; $\square$ &lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color:green&#34;&gt;&lt;strong&gt;Corollary 1.1&lt;/strong&gt;&lt;/span&gt;
The expected value of the product of an odd number of matrix elements is necessarily zero.&lt;/p&gt;
&lt;p&gt;Using the above theorems, we are now able to compute expected values of quantities like $\langle \text{tr} X^k \rangle_n$. By
the corollary, we trivially have that $\langle \text{tr} X^{2k+1}\rangle_n = 0$; it remains to compute the traces of
even powers. This can be accomplished by realizing the pairings in Wick&amp;rsquo;s theorem as sums over families of $1$-
face maps. We briefly summarize these results. Let us begin by way of example. Consider the expected value
$\langle\text{tr} X^4 \rangle_n$; we have that, by linearity of $\langle \cdot \rangle_n$:
\begin{equation*}
\langle \text{tr} X^4 \rangle_n = \sum_{i_1, \cdots, i_4} \langle X_{i_1 i_2} X_{i_2 i_3} X_{i_3 i_4} X_{i_4 i_1} \rangle_n.
\end{equation*}
Using Wick&amp;rsquo;s theorem, we see that there are $3$ contributing terms in the above sum:
\begin{align*}
\langle X_{i_1 i_2} X_{i_2 i_3}\rangle_n &amp;amp;\langle X_{i_3 i_4} X_{i_4 i_1}\rangle_n,\quad
\langle X_{i_1 i_2} X_{i_3 i_4}\rangle_n \langle X_{i_2 i_3} X_{i_4 i_1}\rangle_n,\quad
\text{and }\\\ &amp;amp;\langle X_{i_1 i_2} X_{i_4 i_1}\rangle_n \langle X_{i_2 i_3} X_{i_3 i_4}\rangle_n.
\end{align*}
We will deal with each of these contributions separately. By Lemma 1.1, we see that the first term
is only nonvanishing if $i_1 = i_3$; this leaves three free indices, so that
\begin{equation} \label{termA}
\sum_{i_1, \cdots, i_4}\langle X_{i_1 i_2} X_{i_2 i_3}\rangle_n \langle X_{i_3 i_4} X_{i_4 i_1}\rangle_n
= \sum_{i_1, i_2, i_3} \frac{1}{n}\cdot\frac{1}{n} = n.
\end{equation}
Again by Lemma 1.1, we see that the second term
$\langle X_{i_1 i_2} X_{i_3 i_4}\rangle_n \langle X_{i_2 i_3} X_{i_4 i_1}\rangle_n$ is only nonvanishing
if $i_1 = i_2 = i_3 = i_4$, leaving only one free index, so that this term contributes a total factor of
$\frac{1}{n}$:
\begin{equation}\label{termB}
\sum_{i_1, \cdots, i_4}
\langle X_{i_1 i_2} X_{i_3 i_4}\rangle_n \langle X_{i_2 i_3} X_{i_4 i_1}\rangle_n =
\sum_{i_1} \frac{1}{n}\cdot\frac{1}{n} = \frac{1}{n}.
\end{equation}
Finally, the last term in the sum,
$\langle X_{i_1 i_2} X_{i_4 i_1}\rangle_n \langle X_{i_2 i_3} X_{i_3 i_4}\rangle_n$, similarly contributes
a factor of $n$, since the only constraint on indices here is $i_2 = i_4$, by the lemma:
\begin{equation}\label{termC}
\sum_{i_1, \cdots, i_4} \langle X_{i_1 i_2} X_{i_4 i_1}\rangle_n \langle X_{i_2 i_3} X_{i_3 i_4} \rangle_n
= n.
\end{equation}
Combining the results above, we find that
\begin{equation}
\frac{1}{n}\langle X^4 \rangle_n = \frac{1}{n}\left(n + \frac{1}{n} + n\right) = 2 +
\frac{1}{n^2};
\end{equation}
Thus, we can easily compute the large $n$ limit: $\lim_{n\to \infty}\frac{1}{n} \langle X^4\rangle_n = 2$, which is indeed
the second Catalan number.&lt;/p&gt;
&lt;p&gt;The above computation is essentially generic; one computes $\langle X^{2k} \rangle_n$ by first applying Wick&amp;rsquo;s formula,
then computing the individual contributions of each pairing by investigating the number of free indices, and
finally by summing these contributions. In fact, the problem is essentially reduced to counting the number of
pairings that give only one free index, among the $(2k-1)!!$ possible pairings appearing in the sum. We can
count the number of such pairings using the following geometric interpretation of each of the terms.
Consider the Wick expansion of the trace $\langle\text{tr} X^{m}\rangle_n$; a generic term in this expansion will consist of
a product of &amp;ldquo;pair correlations&amp;rdquo;, i.e., expected values of pairs of matrix elements: $\langle X_{i j}X_{k l}\rangle_n$.
We draw a diagram with $n$ &amp;ldquo;half edges&amp;rdquo;, as seen here:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/RandomMatrices_images/trX2n.png&#34; alt=&#34;Pair Correlations&#34;&gt;&lt;/p&gt;
&lt;p&gt;To each term in the Wick expansion of
$\langle \text{tr} X^{m} \rangle_n$, we associate one of these diagrams: if $(i_N,j_N)$ is paired $(i_M,j_M)$, we connect
the corresponding half-edges to form a full edge. The diagrams
can then be used to compute the contribution of each term in the expansion. For example, two possible pairings
appearing in the Wick expansion of $\langle\text{tr} X^6\rangle_n$ are depicted below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/RandomMatrices_images/trX6_terms.png&#34; alt=&#34;aathis is an image&#34;&gt;&lt;/p&gt;
&lt;p&gt;The leftmost diagram corresponds to the term
$\langle X_{i_1 i_2}X_{i_2 i_3}\rangle_n \langle X_{i_3 i_4}X_{i_4 i_5}\rangle_n
\langle X_{i_5 i_6}X_{i_6 i_1} \rangle_n$; we see that this term is only nonvanishing when $i_1=i_3=i_5$, and for any
$i_2,i_4,i_6$. Thus, the diagram contributes a factor of $n$ to $\langle \textit{tr} X^6 \rangle_n$. On the other hand, the
diagram on the right corresponds to the pairing $\langle X_{i_1 i_2}X_{i_3 i_4}\rangle_n
\langle X_{i_2 i_3}X_{i_6 i_1}\rangle_n \langle X_{i_4 i_5}X_{i_6 i_6}\rangle_n$, which is only nonvanishing when
$i_1 = i_4 = i_6 = i_3 = i_2$, and for any $i_5$. It follows that this diagram contributes a
factor of $1/n$, which will vanish in the large $n$ limit, as the diagram is non-planar.&lt;/p&gt;
&lt;p&gt;Finally, we can conclude that&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 1.3.&lt;/strong&gt;&lt;/span&gt;
\begin{equation}
\langle\text{tr} X^{2k}\rangle_n = \sum_{g=0}^{[k/2]} \varepsilon_g(k) n^{1-2g},
\end{equation}
where $\varepsilon_g(k)$ is the number of labelled one-face maps with $k$ edges of genus $g$.&lt;/p&gt;
&lt;p&gt;In particular,
\begin{equation}
\frac{1}{n}\langle \text{tr} X^{2k}\rangle_n = \varepsilon_0(k) + o(1/n),
\end{equation}
where $\varepsilon_0(k)$ is the number of &lt;em&gt;planar&lt;/em&gt; labelled one-face maps with $k$ edges. For this reason,
the large $n$ limit is sometimes referred to as the &lt;em&gt;planar limit&lt;/em&gt;. It is well-known
that $\varepsilon_0(k) = C_k$, the $k^{th}$ Catalan number.&lt;/p&gt;
&lt;h2 id=&#34;2-matrix-ensembles-and-orthogonal-polynomials&#34;&gt;$2.$ Matrix Ensembles and Orthogonal Polynomials.&lt;/h2&gt;
&lt;p&gt;Most computations within the scheme matrix ensembles are performed with techniques from the theory of orthogonal polynomials. Let us illustrate this
connection in several different ways. Let us consider the Hermitian ensemble with measure
\begin{equation*} \label{V-ensemble}
d\mathbb{P}_n(X) = \frac{1}{Z_n} e^{-\text{tr} Q(X)} dX,
\end{equation*}
where $Q(X) = X^{2n} + \cdots$ is a monic polynomial of even degree. Along with this ensemble, we also consider the family of monic
orthogonal polynomials ${\pi_n(\lambda)}$, which satisfy the relation
\begin{equation}\label{Q-OP}
(\pi_m,\pi_n) := \int_{\mathbb{R}}d\lambda e^{-Q(\lambda)} \pi_n(\lambda)\pi_m(\lambda)  = h_n \delta_{nm},
\end{equation}
where $V(\lambda)$ is the same polynomial, but in the real variable $\lambda$ instead of the matrix variable $X$. Our first connection between the
ensemble defined by the above measure and the polynomials ${\pi_n(\lambda)}$ is given by the following theorem:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 2.1.&lt;/strong&gt;&lt;/span&gt;
(Heine&amp;rsquo;s formula) The polynomials $\pi_n(x)$ satisfy
\begin{equation}
\pi_n(\lambda) = \langle \det(\lambda - X)\rangle_n.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;
The proof is rather straightforward; first, it is clear from its definition that the right hand side of the above equation
is a monic polynomial of degree $n$. Thus, we have only to show that this polynomial is orthogonal to $\lambda^m$, for
$0 \leq m \leq n$. Suppose this is the case; then,
\begin{align*}
Z_n \cdot \big(\lambda^m, \langle \det(\lambda - X)_n\big) &amp;amp;=
\int_{\mathbb{R}} d\lambda  e^{-Q(\lambda)} \lambda^m\bigg[ \int_{\mathbb{R}^n}\prod_{i=1}^n d\lambda_i (\lambda - \lambda_i)e^{-Q(\lambda_i)}  V^2(\lambda_1,\cdots,\lambda_n) \bigg] ;
\end{align*}
Note that $V(\lambda_1,\cdots,\lambda_n)\prod_{i=1}^n(\lambda - \lambda_i) = V(\lambda_1,\cdots,\lambda_n,\lambda)$, the Vandermonde
determinant with one extra variable. Furthermore, the remaining piece of the integrand,  $V(\lambda_1,\cdots,\lambda_n)\lambda^m$, can be
rewritten as (labelling $\lambda = \lambda_{n+1}$):
\begin{equation*}
V(\lambda_1,\cdots,\lambda_n)\lambda^m = \frac{1}{n+1}\sum_{i=1}^{n+1} (-1)^{i+n+1} V(\lambda_1,\cdots, \hat{\lambda_i},\cdots,\lambda_{n+1})\lambda_i^m;
\end{equation*}
This is the expression for the determinant
\begin{equation*}
\begin{pmatrix}
1 &amp;amp; \lambda_1 &amp;amp; \cdots &amp;amp; \lambda_1^{n-1} &amp;amp;\lambda_1^m\\\&lt;br&gt;
1 &amp;amp; \lambda_2 &amp;amp; \cdots &amp;amp; \lambda_2^{n-1} &amp;amp;\lambda_2^m\\\&lt;br&gt;
\vdots &amp;amp; \vdots &amp;amp; \cdots &amp;amp; \ddots &amp;amp;\vdots\\\&lt;br&gt;
1 &amp;amp; \lambda_{n+1} &amp;amp; \cdots &amp;amp; \lambda_{n+1}^{n-1} &amp;amp;\lambda_{n+1}^m\\\&lt;br&gt;
\end{pmatrix};
\end{equation*}
Clearly, this expression vanishes if $m &amp;lt; n$, and so
$\det(\lambda - X)_n$ is orthogonal to the first $m$ monomials, and is thus the (unique) monic orthogonal polynomial, $\pi_n(\lambda)$.&lt;/p&gt;
&lt;div style=&#34;text-align: right&#34;&gt; $\square$ &lt;/div&gt;
&lt;p&gt;In the above proof, if $m=n$, by what we just argued, we have that
\begin{align*}
Z_n \cdot \big(\lambda^n, \langle \det(\lambda - X)_n\big)
&amp;amp;= \frac{1}{n+1}\int_{\mathbb{R}}d\lambda e^{-Q(\lambda)} \bigg[ \int_{\mathbb{R}^n}\prod_{i=1}^n d\lambda_i
e^{-Q(\lambda_i)}  V^2(\lambda_1,\cdots,\lambda_n,\lambda) \bigg] \\\&lt;br&gt;
&amp;amp;= \frac{1}{n+1} Z_{n+1};
\end{align*}
furthermore, since $\big(\lambda^n, \langle \det(\lambda - X)\rangle_n\big) = \big(\lambda^n, \pi_n\big) = (\pi_n, \pi_n) = h_n$, we obtain that
\begin{equation*}
Z_{n+1} = (n+1)h_n;
\end{equation*}
We therefore obtain inductively a second important link between orthogonal polynomials and matrix models:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 2.2.&lt;/strong&gt;&lt;/span&gt;
The partition function of the matrix model defined above is related to the orthogonality constants ${h_k}$ by
\begin{equation}
Z_n = n! \prod_{k=0}^n h_k.
\end{equation}&lt;/p&gt;
&lt;p&gt;We wish to make one final connection between matrix models and orthogonal polynomials. Consider the probability density function on the
eigenvalues for this ensemble:
\begin{equation}
\rho_n(\lambda_1,\cdots, \lambda_n) :=  \frac{1}{Z_n} V^2(\lambda_1 ,\cdots,\lambda_n) e^{-\sum_{k=1}^n Q(\lambda_k)}.
\end{equation}
Recall that the Vandermonde determinant is the determinant of the matrix $\big(\lambda_i^{j-1} \big)$. Note that we can add any scalar
multiple of any row/column to any other row/column without changing the determinant; thus,
\begin{equation*}
\det \begin{pmatrix}
1 &amp;amp; \lambda_1 &amp;amp; \cdots &amp;amp; \lambda_1^{n-1}\\\&lt;br&gt;
1 &amp;amp; \lambda_2 &amp;amp; \cdots &amp;amp; \lambda_2^{n-1}\\\&lt;br&gt;
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp;\vdots\\\&lt;br&gt;
1 &amp;amp; \lambda_{n} &amp;amp; \cdots &amp;amp; \lambda_{n}^{n-1}\\\&lt;br&gt;
\end{pmatrix}
=
\det \begin{pmatrix}
p_0(\lambda_1) &amp;amp; p_1(\lambda_1) &amp;amp; \cdots &amp;amp; p_{n-1}(\lambda_1)\\\&lt;br&gt;
p_0(\lambda_2) &amp;amp; p_1(\lambda_2) &amp;amp; \cdots &amp;amp; p_{n-1}(\lambda_2)\\\&lt;br&gt;
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp;\vdots\\\&lt;br&gt;
p_0(\lambda_n) &amp;amp; p_1(\lambda_n) &amp;amp; \cdots &amp;amp; p_{n-1}(\lambda_n)\\\&lt;br&gt;
\end{pmatrix},
\end{equation*}
for any family of monic polynomials ${p_{k}(\lambda)}$. In particular, we will choose $p_{k}(\lambda) = \pi_k(\lambda)$,
the monic orthogonal polynomials with respect to the weight $e^{Q(\lambda)}$. Combining these two determinants, the density function becomes
\begin{align*}
\rho_n(\lambda_1,\cdots, \lambda_n) &amp;amp;=  \frac{1}{Z_n} \det(\pi_k(\lambda_i))\det(\pi_k(\lambda_j))
e^{-\sum_{k=1}^n Q(\lambda_k)}\\\&lt;br&gt;
&amp;amp;=\frac{1}{n! \prod_{k=0}^n h_k} \det\bigg(\sum_{k=0}^{n-1} \pi_k(\lambda_i)\pi_k(\lambda_j)\bigg)
e^{-\sum_{k=0}^{n-1} Q(\lambda_k)}\\\&lt;br&gt;
&amp;amp;=\frac{1}{n!} \det\bigg(\sum_{k=0}^{n-1} \frac{\pi_k(\lambda_i)\pi_k(\lambda_j)}{h_k}\bigg)e^{-\sum_{k=0}^{n-1}
Q(\lambda_k)}\\\&lt;br&gt;
&amp;amp;=\frac{1}{n!} \det\bigg(\sum_{k=0}^{n-1}
\frac{\pi_k(\lambda_i)e^{-Q(\lambda_i)/2}\pi_k(\lambda_j)e^{-Q(\lambda_j)/2}}{h_k}\bigg)\\\&lt;br&gt;
&amp;amp;:=\frac{1}{n!}\det(K(\lambda_i,\lambda_j)).
\end{align*}
The function
\begin{equation}
K(\lambda,\mu) := \sum_{k=0}^{n-1} \frac{\pi_k(\lambda)\pi_k(\mu)}{h_k}e^{-Q(\lambda)/2}e^{-Q(\mu)/2}
\end{equation}
is called the &lt;em&gt;Christoffel-Darboux&lt;/em&gt; kernel; it acts as a projector onto the span of the first $n$ orthogonal
polynomials, multiplied by $e^{Q/2}$. If $m &amp;lt; n$, integrating out the last $(n-m)$
variables, one finds that the joint density of the first $m$ eigenvalues is
\begin{equation}
\rho_n(\lambda_1,\cdots,\lambda_m) = \frac{(n-m)!}{n!} \det(K(\lambda_i,\lambda_j))_{i,j = 1}^m.
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;3-coulomb-gas-formalism&#34;&gt;$3.$ Coulomb Gas Formalism.&lt;/h2&gt;
&lt;p&gt;In random matrix theory, one is often interested in the large n (&lt;em&gt;planar&lt;/em&gt;) limit of certain quantities; by the
previous section, we are equivalently looking for asymptotic formulae for orthogonal polynomials. One technique
for addressing such questions is the coulomb gas method. For sake of clarity, we will focus our attention on one
particular object, and its asymptotics: the &lt;em&gt;free energy&lt;/em&gt;:
\begin{align*}
F = \frac{1}{n^2}\log Z_n = \frac{1}{n^2} \log \int_{\mathbb{R}^n} V(\lambda_1,\cdots,\lambda_n)
e^{-n\sum_{i=1}^nQ(\lambda_i)} d\lambda_1\cdots d\lambda_n.
\end{align*}
Moving the Vandermonde determinant into the exponent, we can rewrite the free energy as
\begin{align*}
\frac{1}{n^2} \log  \int_{\mathbb{R}^n} e^{-n^2 E(\lambda_1,\cdots,\lambda_n)}d\lambda_1\cdots d\lambda_n,
\end{align*}
where
\begin{equation}
E(\lambda_1,\cdots,\lambda_n) := \frac{1}{n^2}\sum_{i\neq j} \log \frac{1}{\lambda_i - \lambda_j} +
\frac{1}{n}\sum_{i=1}^n Q(\lambda_i).
\end{equation}
The above sum can be interpreted as an energy functional for a system of $n$ charges interacting via the $2D$ Coulomb
potential. The fact that the $\lambda_i&amp;rsquo;s$ are real translates to the condition that the charges are confined to live
on a wire (the real line). The charges all have mass $1/n$, and all have the same sign, and so the affect of the
coulomb interaction is repulsive. On the other hand, the charges sit in a background confining potential $Q(\lambda)$,
which holds them together. Note that both terms in $E(\lambda_1,\cdots,\lambda_n)$ are of order 1, by how we scaled
$Q\to nQ$.&lt;/p&gt;
&lt;p&gt;With this interpetation in mind, let us turn our attention back to the free energy. When $n$ is very large, typical
saddle point analysis tells us that we expect the dominant contributions to the free energy to come from the minima of
$E$, i.e.
\begin{align*}
F &amp;amp;= \frac{1}{n^2} \log \int_{\mathbb{R}^n} e^{-n^2E(\lambda_1,\cdots\lambda_n)} d\lambda_1\cdots d\lambda_n\\\&lt;br&gt;
&amp;amp;\sim \frac{1}{n^2} \log e^{-n^2 \min E(\lambda_1,\cdots\lambda_n)}\\\&lt;br&gt;
&amp;amp;= \min E(\lambda_1,\cdots\lambda_n).
\end{align*}
Writing the density of charges as a measure $\mu = \frac{1}{n}\sum_{k=1}^n \delta_{{\lambda_{k}}}$, we can rewrite this
minimization problem as
\begin{align*}
\min E(\lambda_1,\cdots\lambda_n) &amp;amp;= \min \frac{1}{n^2}\sum_{i\neq j} \log \frac{1}{\lambda_i - \lambda_j} +
\frac{1}{n}\sum_{i=1}^n Q(\lambda_i)\\\&lt;br&gt;
&amp;amp;= \min_{\mu(\mathbb{R}) = 1} \int\int \log\frac{1}{x-y} d\mu(x) d\mu(y) + \int Q(x) d\mu(x).
\end{align*}
As $n \to \infty$, we expect that the charges should coalesce into a continuous distribution of charge $\mu$, i.e.,
$ \frac{1}{n}\sum_{k=1}^n \delta_{{\lambda_{k}}} \to \rho(x) dx$, where $\rho$ has total charge $1$. This sort of
equilibrium problem is well-known in potential theory; we can use our physical intuition to try and find its
solution. In equilibrium, the effective potential (the potential coming from the charges themselves, plus the
external field) should be constant on the support of the charges, otherwise the charges would move around, and we
wouldn&amp;rsquo;t be in equilibrium. This yields the following equilibrium condition:
\begin{equation}
U^\mu(x) + Q(x) = \text{const.},
\end{equation}
for $x$ in the support of the charges. Here, $U^\mu(x)$ is the potential given off by the charges:
\begin{equation}
U^\mu(x) = \int \log\frac{1}{x-y} d\mu(y).
\end{equation}
Differentiating the equilibrium condition, we obtain the equation
\begin{equation}
C^\mu(x) + Q&#39;(x) = 0,
\end{equation}
where $C^\mu(x)$ is the &lt;em&gt;Cauchy transform&lt;/em&gt; of the measure $\mu$:
\begin{equation}
C^\mu(x) = \int \frac{d\mu(y)}{y-x},
\end{equation}
considered here in the principal value sense. We are almost at our goal; all that remains is to recover the measure
$\mu$ from the above formula, and then plug it back in to the energy functional, and then we have our expression
for the dominant contribution to the free energy. But how do we recover a measure if we know its Cauchy transform?
Luckily, this question has already been answered for us; these are the famous Sokhotsky-Plemelj formulae:
\begin{align*}
\frac{d\mu(x)}{dx} &amp;amp;= \frac{1}{2\pi i}\bigg( C^\mu_+(x) - C^\mu_-(x)\bigg),\\\&lt;br&gt;
P.V.\int \frac{d\mu(y)}{y-x} &amp;amp;= C^\mu_+(x) + C^\mu_-(x),
\end{align*}
for $x$ in the support of the charges. These formulae, along with a little knowledge of complex analysis, will allow
us to recover $d\mu(x) = \rho(x)dx$. Consider the function $P(z):=Q&#39;(z)C^{\mu}(z) - (C^\mu(z))^2$. This function is
analytic everywhere in the complex plane, except possibly at the places where $C^\mu(z)$ loses analyticity&amp;ndash;the support
of the charges. Let us compute the jump of $P(z)$ across the support:
\begin{align*}
P_+(z) - P_-(z) &amp;amp;= Q&#39;(z)\bigg(C^{\mu}_+(z)-C^{\mu}_-(z)\bigg) - (C^\mu_+(z))^2 + (C^\mu_-(z))^2\\\&lt;br&gt;
&amp;amp;= \bigg(C^{\mu}_+(z)+C^{\mu}_-(z)\bigg)\bigg(C^{\mu}_+(z)-C^{\mu}_-(z)\bigg)- (C^\mu_+(z))^2 + (C^\mu_-(z))^2\\\&lt;br&gt;
&amp;amp;=0,
\end{align*}
and so $P(z)$ is continuous across the support of the charges, and thus continuous everywhere. By Morera&amp;rsquo;s theorem,
$P(z)$ is an entire function. Furthermore, we know the exact growth rate of $P(z)$ at infinity:
$P(z) = \mathcal{O}(z^{n-2})$, where $\deg Q = n$. Thus, by Liouville&amp;rsquo;s theorem, $P(z)$ &lt;em&gt;is&lt;/em&gt; a polynomial
of degree $n-2$. We thus have obtained an algebraic equation for $C^\mu(z)$:
\begin{equation}
(C^\mu(z))^2 - Q&#39;(z)C^{\mu}(z) - P(z) = 0;
\end{equation}
this equation is quadratic in $C^\mu(z)$, and thus can be solved:
\begin{equation}
C^{\mu}(z) = -\frac{1}{2}Q&#39;(z) \pm \frac{1}{2}\sqrt{Q&#39;(z)^2 - 4P(z)
\end{equation}
Therefore, by the other half of the Sokhotsky-Plemelj formulae, we can recover the density:
\begin{equation}
\rho(x) = \frac{d\mu(x)}{dx} = \frac{1}{2\pi} \sqrt{4P(x) - Q&#39;(x)^2}.
\end{equation}
In practice, computing the coefficients of $P(x)$ is not a trivial task, especially if the support of the measure is
more than one interval. However, if the potential $Q(x)$ is partiularly simple, the computations can be done rather
quickly; if $Q(x) = \frac{1}{2}x^2$, as it is for the GUE, then it is easy to see that $P(x) = 1$, and so the formula
above becomes
\begin{equation}
\rho(x) = \frac{1}{2\pi}\sqrt{4-x^2};
\end{equation}
this is &lt;em&gt;Wigner&amp;rsquo;s semicircle law&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;4-2-matrix-models&#34;&gt;$4.$ 2-Matrix Models.&lt;/h2&gt;
&lt;p&gt;We now discuss $N$-matrix models, which are matrix models $n$ over copies of $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt;; for simplicity, we specialize to the case of $N = 2$.
We denote the matrices in the first copy of $\mathcal{H}$&lt;sub&gt;n&lt;/sub&gt; by $A$, and in the second copy by $B$. The probability measure we consider here is
\begin{equation*}
d\mathbb{P}_n(A,B) = \frac{1}{Z_n} \exp\bigg( -\frac{n}{2} \text{tr} (A^2 + B^2 + 2cAB)\bigg) dA dB.
\end{equation*}
The quantity in the exponent can be thought of as a quadratic form on pairs of Hermitian matrices $(A,B)$, with defining matrix
\begin{equation*} \label{quad-form}
\begin{pmatrix}
1 &amp;amp; c \\\&lt;br&gt;
c &amp;amp; 1
\end{pmatrix}.
\end{equation*}
Expected values are denoted in the same way as before; this ensemble is also Gaussian in nature, and so the matrix version of Wick&amp;rsquo;s theorem
applies. As before, all one-point functions vanish identically. Thus, the relevant expected values to compute are the two-point functions; they are:
\begin{align*}
\langle A_{i_1j_1}A_{i_2j_2}\rangle_n = \langle B_{i_1j_1}B_{i_2j_2}\rangle_n
&amp;amp;= \frac{1}{n}\frac{1}{1-c^2} \delta_{i_1 j_2}\delta_{i_2 j_1},\\\&lt;br&gt;
\langle A_{i_1j_1}B_{i_2j_2}\rangle_n &amp;amp;= \frac{1}{n}\frac{c}{1-c^2} \delta_{i_1 j_2}\delta_{i_2 j_1}.
\end{align*}
The computations are almost identical to the $1$-matrix case, and so we omit them here (the process involves the extra step of diagonalizing
the quadratic form above. The crucial point here is that the two point functions for &amp;ldquo;like&amp;rdquo; matrices are the same, and differ
from the two point function for the &amp;ldquo;mismatched&amp;rdquo; matrices by an overall constant. Therefore, the same sorts of combinatorial identites that
held before for the correlations now hold for graphs with two possible weights on the vertices.&lt;/p&gt;
&lt;p&gt;This connection of the 2-matrix model to enumeration of 2-colored graphs was exploited by V. Kazakov to answer questions
about the 
&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/0375960186904330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ising model on random planar graphs&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;5-itzykson-zuber-integral-over-the-unitary-group&#34;&gt;$5.$ Itzykson-Zuber Integral over the Unitary Group.&lt;/h2&gt;
&lt;p&gt;With this combinatorial interpretation of the model in mind, we would like to go about explicitly computing certain matrix integrals. To do this,
we will need to switch over to eigenvalue variables; this is not as trivial as a task as it was in the 1-matrix case. This is made apparent as
follows. Suppose we want to compute the expected value of some function $f(A,B) = f(\text{tr} A,\text{tr} B)$. This is written as
\begin{equation}
\langle f(A,B)  \rangle_n = \frac{1}{Z_n}\int_{A\in \mathcal{H}_n} \int_{B \in \mathcal{H}_n}dA dB \exp\bigg( -\frac{n}{2}
\text{tr} (A^2 + B^2 + 2cAB)\bigg)  f(A,B).
\end{equation}
Letting $A = U^\dagger X U$, $B = V^\dagger Y V$, where $X = \text{diag}(x_1,\cdots,x_n)$, $Y = \text{diag}(y_1,\cdots,y_n)$ are diagonal matrices, we see that the above is
\begin{align*}
=&amp;amp; \frac{1}{Z_n}\int_{X,Y\in \mathbb{R}^n} dX dY V(X)^2V(Y)^2f(X,Y) \exp\bigg( -\frac{n}{2} \text{tr} (X^2 + Y^2)\bigg)
\\\ &amp;amp;\times\int_{U,V \in U(n)/U_{diag}(n)} dU dV \exp\big(nc \text{tr} AB\big)
\end{align*}
We must pay careful attention to the integral
\begin{equation*}
\int_{U,V \in U(n)/U_{diag}(n)} dU dV \exp\big(nc \text{tr} AB\big) =  \int_{U,V \in U(n)/U_{diag}(n)} dU dV
\exp\big(nc \text{tr} U^\dagger X UV^\dagger Y V\big).
\end{equation*}
Making the change of variables in $V$: $W = UV^\dagger$, and integrating over $U$ first (and applying translation invariance of the Haar measure),
we see that the above integral becomes
\begin{equation}
\text{vol}(U(n)/U_{diag}(n)) \cdot \int_{W \in U(n)/U_{diag}(n)} dW \exp\big(nc \text{tr}  X W Y W^\dagger\big),
\end{equation}
where $X,Y$ are diagonal matrices, and $W$ is unitary. Integrals of this type are called &lt;em&gt;Harish-Chandra&lt;/em&gt; or
&lt;em&gt;Itzykson-Zuber&lt;/em&gt;
integrals over the unitary group. It turns out that such integrals can be computed explicitly. We will consider here a slight generalization of the
above:
\begin{equation}
I(A,B;s) := \int_{U(n)/U_{diag}(n)} dU \exp\big(s \text{tr}  A U B U^\dagger\big),
\end{equation}
where $A, B$ are Hermitian matrices. Our claim is the following:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;background-color:blue&#34;&gt;&lt;strong&gt;Theorem 5.1.&lt;/strong&gt;&lt;/span&gt;
Let $I(A,B;s)$ be as defined above, and suppose $A$ has eigenvalues $a_1,\cdots, a_n$, and $B$ has eigenvalues $b_1,\cdots, b_n$. Then
\begin{equation}
I(A,B;s) = s^{-n(n-1)/2} \frac{\det(e^{a_ib_j})}{V(a) V(b)}.
\end{equation}&lt;/p&gt;
&lt;p&gt;The remainder of this section is dedicated to a proof of this fact. We use the heat kernel technique, which is outlined
in 
&lt;a href=&#34;https://arxiv.org/abs/math-ph/0209019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a wonderful paper&lt;/a&gt; by P. Zinn-Justin and J.B. Zuber.
Consider the heat equation in the variable $A$:
\begin{equation}\label{heat-eq-1}
\begin{cases}
\left( \frac{\partial}{\partial t} - \frac{1}{2} \Delta_A \right) u(A,t) = 0,\\\&lt;br&gt;
u(A,0) = f(A).
\end{cases}
\end{equation}
Here, $\Delta_A$ is the usual Laplace operator on $\mathcal{H}_n$:
\begin{equation*}
\Delta_{A} = \sum_{i=1}^n \frac{\partial^2}{\partial A_{ii}^2} + \frac{1}{2}\sum_{i &amp;lt; j}\left(\frac{\partial^2}{\partial \text{Re}(A_{ij})^2} +
\frac{\partial^2}{\partial \text{Im}(A_{ij})^2}\right).
\end{equation*}
The heat kernel for this operator is easily derived (since $\mathcal{H}_n \cong \mathbb{R}^{n^2}$ in scaled coordinates):
\begin{equation*}
K(A,B;t) = \frac{1}{(2\pi t)^{n^2/2}}\exp\left(\frac{1}{2t} \text{tr}(A-B)^2\right);
\end{equation*}
The solution to the heat equation above is thus given by
\begin{equation*}
u(A,t) = \int_{\mathcal{H}_n} dB f(B) \frac{1}{(2\pi t)^{n^2/2}}\exp\left(-\frac{1}{2t} \text{tr}(A-B)^2\right).
\end{equation*}
Let us suppose the initial data $f(A)$ depends only on the eigenvalue
variables, i.e. $f(A) = f(\text{tr}(A))$. Then, converting the above to eigenvalue variables $A = U^\dagger X U$, $B = V^\dagger Y V$, we find that
\begin{align*}\label{heat2}
u(A,t) = &amp;amp;\frac{1}{(2\pi t)^{n^2/2}}\int_{\mathbb{R}^n} dY, V^2(Y)f(Y) \exp\left(-\frac{1}{2t} \text{tr}(X^2 + Y^2) \right)
\\\ &amp;amp;\times\int_{U(n)/U_{diag}(n)} dW \exp\big(\frac{1}{t} \text{tr} X W Y W^\dagger\big),
\end{align*}
where $W = UV^\dagger$. Note that the inner integral is the object we want to compute. Furthermore, the above formula implies that if
$f(U^\dagger X U) = f(X)$, then $u(U^\dagger X U,t) = u(X,t)$ for all $t &amp;gt; 0$ as well. Let us see examine the above equation in eigenvalue
coordinates. With the help of the formula we derived for the laplacian on the eigenvalue coordinates, we find that
\begin{align*}
0 &amp;amp;= \left( \frac{\partial}{\partial t} - \frac{1}{2} \Delta_A \right) u(A,t)
= \left( \frac{\partial}{\partial t} - \frac{1}{2} \Delta_{X} \right) u(X,t)\\\&lt;br&gt;
&amp;amp;= \frac{\partial u }{\partial t} - \frac{1}{2}\frac{1}{V^2(X)}\sum_{i=1}^n\frac{\partial}{\partial x_i}
\left(V^2(X) \frac{\partial u}{\partial x_i} \right)\\\&lt;br&gt;
&amp;amp;= \frac{\partial u }{\partial t} - \frac{1}{2}\frac{1}{V^2(X)}\sum_{i=1}^n \left(2V(X)\frac{\partial V}{\partial x_i}  \frac{\partial u}{\partial x_i}  + V^2(X) \frac{\partial^2 u}{\partial x_i^2}\right);
\end{align*}
Multiplying the above equation by the Vandermonde determinant $V(x)$, and using the fact that $\sum_{i=1}^n \frac{\partial^2 V}{\partial x_i^2}= 0$, the above equation becomes
\begin{align*}
0 &amp;amp;= \frac{\partial V u }{\partial t} - \frac{1}{2}\sum_{i=1}^n \left(2\frac{\partial V}{\partial x_i}
\frac{\partial u}{\partial x_i}  + V(X)\frac{\partial^2 u}{\partial x_i^2}\right)\\\&lt;br&gt;
&amp;amp;= \frac{\partial V u }{\partial t} - \frac{1}{2}\sum_{i=1}^n \left(\underbrace{u\frac{\partial^2
V}{\partial x_i^2}}_{=0} + 2\frac{\partial V}{\partial x_i}  \frac{\partial u}{\partial x_i}  +
V(X)\frac{\partial^2 u}{\partial x_i^2}\right)\\\&lt;br&gt;
&amp;amp;= \left(\frac{\partial}{\partial t} - \frac{1}{2} \sum_{i=1}^n \frac{\partial^2}{\partial x_i^2}\right)V(X)u(X,t),
\end{align*}
and so $V(x)u(X,t)$ satisfies a heat equation of a different form. The generic solution for initial data $g(X)$ to the above heat equation is
\begin{equation*}
h(X,t) = \int_{Y\in \mathbb{R}^n} dY \frac{1}{(2\pi t)^{n/2}}\exp\left(-\frac{1}{2t}\text{tr}(X-Y)^2\right) g(Y);
\end{equation*}
In particular, for initial data $g(X) = V(X)f(X)$, we obtain the
formula
\begin{equation}\label{heat3}
V(X)u(X,t) = \int_{Y\in \mathbb{R}^n} dY \frac{1}{(2\pi t)^{n/2}}\exp\left(-\frac{1}{2t}\text{tr}(X-Y)^2\right) V(Y)f(Y)
\end{equation}
multiplying the previous equation by $V(X)$, we see that we have two expressions for the same function, $V(X)u(X,t)$. Equating these
expressions yields
\begin{align*}
&amp;amp;V(X)\frac{1}{(2\pi t)^{n^2/2}}\int_{\mathbb{R}^n} dY V^2(Y)f(Y) \exp\left(-\frac{1}{2t} \text{tr}(X^2 + Y^2) \right)
\int_{U(n)/U_{diag}(n)} dW \exp\big(\frac{1}{t} \text{tr} X W Y W^\dagger\big)\\\&lt;br&gt;
&amp;amp;=  u(X,t)=\int_{Y\in \mathbb{R}^n} dY \frac{1}{(2\pi t)^{n/2}}\exp\left(-\frac{1}{2t}\text{tr}(X-Y)^2\right) V(Y)f(Y).
\end{align*}
Setting $f(Y) = \delta(Y-Y_0)$, we obtain that
\begin{align*}
&amp;amp;V(X)V^2(Y_0)\frac{1}{(2\pi t)^{n^2/2}}\exp\left(-\frac{1}{2t} \text{tr}(X^2 + Y_0^2) \right)
\int_{U(n)/U_{diag}(n)} dW \exp\big(\frac{1}{t} \text{tr} X W Y_0 W^\dagger\big)\\\&lt;br&gt;
&amp;amp;=   \frac{1}{(2\pi t)^{n/2}}\exp\left(-\frac{1}{2t}\text{tr}(X-Y_0)^2\right) V(Y_0).
\end{align*}
Rearranging, and relabelling $Y_0 = Y$, we get
\begin{equation*}
\int_{U(n)/U_{diag}(n)} dW \exp\big(\frac{1}{t} \text{tr} X W Y W^\dagger\big) =
(2\pi t)^{-n(n-1)/2}\frac{\exp(\frac{1}{t}\text{tr} XY)}{V(Y) V(X)};
\end{equation*}
finally,
\begin{equation}
\int_{U(n)/U_{diag}(n)} dW \exp\big(\frac{1}{t} \text{tr} X W Y W^\dagger\big) =
(2\pi t)^{-n(n-1)/2}\frac{\det e^{\frac{1}{t}x_iy_j }}{V(Y) V(X)}.
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;6-2-matrix-models-and-biorthogonal-polynomials&#34;&gt;$6.$ 2-Matrix Models and Biorthogonal Polynomials.&lt;/h2&gt;
&lt;p&gt;Just as $1$-matrix models are connected to orthogonal polynomials, $2$-matrix models are connected to biorthogonal polynomials. Consider again
the $2$-matrix model defined by the measure
\begin{equation*}
d\mathbb{P}_n(A,B) = \frac{1}{Z_n} \exp\bigg( -\frac{n}{2} \text{tr} (A^2 + B^2 + 2cAB)\bigg) dA dB,
\end{equation*}
and recall that expected values of invariant functions $f(A,B) = f(\text{tr}(A),\text{tr}(B))$ are given by the formula
\begin{equation*}
\langle f(A,B)  \rangle_n = \frac{1}{Z_n}\int_{A\in \mathcal{H}_n} \int_{B \in \mathcal{H}_n}dA dB \exp\bigg( -\frac{n}{2}
\text{tr} (A^2 + B^2 + 2cAB)\bigg)  f(A,B).
\end{equation*}
Letting $A = U^\dagger X U$, $B = V^\dagger Y V$, where $X = \text{diag}(x_1,\cdots,x_n)$, $Y = \text{diag}(y_1,\cdots,y_n)$ are diagonal matrices, we
can integrate out eigenvalue variables to obtain
\begin{align*}
&amp;amp;= \frac{1}{Z_n&#39;}\int_{X,Y\in \mathbb{R}^n} dX dY V(X)^2V(Y)^2f(X,Y) \exp\bigg( -\frac{n}{2} \text{tr} (X^2 + Y^2)\bigg)
\\\ &amp;amp;\times\int_{U \in U(n)/U_{diag}(n)} dW \exp\big(nc \text{tr} XWYW^\dagger\big).
\end{align*}
The integral inside is precisely the Itzykson Zuber integral we computed previously; we insert the expression we derived for it into the
above formula, absorbing all constant factors into the partition function:
\begin{align*}
\langle f(A,B)  \rangle_n &amp;amp;= \frac{1}{Z_n&#39;&#39;}\int_{X,Y\in \mathbb{R}^n} dX dY V(X)^2V(Y)^2f(X,Y) \exp\bigg(
-\frac{n}{2} \text{tr} (X^2 + Y^2)\bigg)
\frac{\det e^{nc x_iy_j }}{V(Y) V(X)}\\\&lt;br&gt;
&amp;amp;=\frac{1}{Z_n&#39;&#39;}\int_{X,Y\in \mathbb{R}^n} \prod_{k=1}^n dx_k dy_k f(x,y) V(x)V(y)\exp\bigg( -\frac{n}{2} \sum_{k=1} x_i^2  + y_i^2 + 2ncx_i y_i\bigg).
\end{align*}&lt;/p&gt;
&lt;p&gt;Thus, just as we were able to connect the 1-matrix model and orthogonal polynomials, we can connect the 2-matrix
to &lt;em&gt;bi-orthogonal polynomials&lt;/em&gt;; i.e., the polynomials $p_n(x),q_n(y)$ satisfying
\begin{align*}
\int_{\mathbb{R}} \int_{\mathbb{R}} p_n(x)q_n(y) e^{-\frac{n}{2}x^2-\frac{n}{2}y^2-2nxy} dxdy = h_n \delta_{nm}
\end{align*}
Of course, the same sorts of arguments can be made for more general potentials besides $x^2,y^2$; we postpone discussion
of this to a later entry.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Helton Howe Trace Formula</title>
      <link>/posts/the-helton-howe-trace-formula/</link>
      <pubDate>Sat, 01 May 2021 15:19:15 -0400</pubDate>
      <guid>/posts/the-helton-howe-trace-formula/</guid>
      <description>&lt;p&gt;We present a proof of a simple version of the Helton-Howe trace
formula (for a more general version of this result, one should consult
the 
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/BFb0058919&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original paper&lt;/a&gt;).
Consider the Hardy space $H^2(\mathbb{D})$. There is a natural
inclusion of $H^2(\mathbb{D})$ into $L^2(\mathbb{T})$, given by the restriction
$f\mapsto f\vert_{\mathbb{T}}$. Let $\mathbb{P}_{H^2} : L^2(\mathbb{T}) \to H^2(\mathbb{D})$ denote the
orthogonal projection of $L^2(\mathbb{T})$ into $H^2(\mathbb{D})$. Now, given
$P \in \mathbb{C}[z,z^{-1}]$, we define the Toeplitz operator with principal
symbol $P$ by $$T_P f(z) = \mathbb{P}_{H^2} \left[ P(z)f(z)\right],$$ for
$f \in H^2(\mathbb{D})$. It is clear that the commutator of any two such
operators operators is trace-class (it is even of finite rank). The
Helton-Howe formula tells us that, given $P,Q \in \mathbb{C}[z,z^{-1}]$,
$$\text{tr } [T_P, T_Q] = \frac{1}{2\pi i}\int_{|z| = 1} P(z) Q&#39;(z) dz = \frac{1}{2\pi i}\int_{\mathbb{D}} dP \wedge dQ$$
The last equality is immediate from Stokes&#39; theorem; the interesting
equality is the first. Let $T_z$ denote the Toeplitz operator of
multiplication by $z$ (this is just the shift operator in the Hardy
space), and $T^*_z$ its adjoint (note that $T^*_z = T_{1/z}$). If
$P = \sum_k P_k z^k \in \mathbb{C}[z,z^{-1}]$, then its corresponding Toeplitz
operator may be written as
$$T_P = \sum_{k\geq 0} P_k T_z^k + \sum_{k&amp;lt;0} P_k {T_z^{*}}^k.$$
Clearly, ${T_z^*}^n{T_z}^n = I$; on the other hand,
${T_z}^n{T_z^*}^n = I - \mathbb{P}_n$, where $\mathbb{P}_n$ is the orthogonal
projector onto the subspace $E_n:=\text{span}$ {$1,z, &amp;hellip;,z^{n-1}$}.
This observation allows us to compute that
$\text{tr }[{T_z^*}^n,{T_z}^n] = \text{tr } \mathbb{P}_n = n$. Now, if $n \neq m$, one finds
by direct computation that $\text{tr }[{T_z^*}^m,{T_z}^n] = 0$. Thus,
$\text{tr }[{T_z^*}^m,{T_z}^n] = n\delta_{n,m}$. If $P, Q \in \mathbb{C}[z,z^{-1}]$,
and $P = \sum_k P_k z^k$, $Q = \sum_k Q_k z^k$, then
$$= \sum_{n,m \geq 0} (P_{-n}Q_m - Q_{-n}P_m) [{T_z^*}^n,{T_z}^m];$$
taking traces yields $$\begin{aligned}
\text{tr } [T_P,T_Q] &amp;amp;= \sum_{n,m \geq 0} (P_{-n}Q_m - Q_{-n}P_m) \text{tr } [{T_z^*}^n,{T_z}^m] \\\&lt;br&gt;
&amp;amp;= \sum_{n,m \geq 0} (P_{-n}Q_m - Q_{-n}P_m) n\delta_{n,m} = \sum_{n\geq 0} n(P_{-n}Q_n - Q_{-n}P_n)\\\&lt;br&gt;
&amp;amp;= \sum_{n}n P_{-n}Q_n.
\end{aligned}$$ On the other hand, by residue theorem,
$$\begin{aligned}
\frac{1}{2\pi i}\int_{|z| = 1} P(z) Q&#39;(z) dz
&amp;amp;= \frac{1}{2\pi i}\int_{|z| = 1} \sum_{\ell}\left(\sum_n nP_{\ell - n} Q_n \right) z^{\ell-1} dz\\\&lt;br&gt;
&amp;amp;= \sum_{\ell}\left(\sum_n nP_{\ell - n} Q_n \right) \delta_{\ell,0}\\\&lt;br&gt;
&amp;amp;= \sum_{n}n P_{-n}Q_n.
\end{aligned}$$ Comparing the results of these two computations
concludes the proof.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working Differential Geometry</title>
      <link>/posts/working-differential-geometry/</link>
      <pubDate>Sat, 01 May 2021 12:52:29 -0400</pubDate>
      <guid>/posts/working-differential-geometry/</guid>
      <description>&lt;p&gt;We present a methodical procedure for computing important geometric quantities on a Riemannian manifold. We want to emphasize that the purpose of this document is not meant to provide a systematic introduction to Riemannian geometry, or motivate/give intuition for the definitions (although we will occasionally give quick consistency checks); rather, our aim is to provide a formalism that makes the objects of interest (Christoffel symbols, Riemann tensor, etc.) &amp;ldquo;manageable&amp;rdquo;. This method is not the most streamlined (see, for example, E. Cartan&amp;rsquo;s theory of moving frames, which is arguably easier to work with; furthermore, if the manifold in question is a Lie group equipped with an invariant metric, the required computations reduce drastically), it is certainly friendlier than what is found in most classical differential geometry texts. Furthermore, most of these techniques carry over to vector and principal bundles, equipped with a connection, and so what follows may also be readily applied there. Before beginning the exposition, we summarize the objectives of this document:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Compute several examples of Riemannian metrics.
2. Compute the Levi-Cevita connection in terms of the metric.
3. Compute the Riemann tensor in terms of the connection coefficients.
4. Compute the Ricci and scalar curvatures in terms of the Riemann tensor, 
as well as the Einstein tensor (in some special cases).
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;1-riemannian-metrics&#34;&gt;$1$. Riemannian metrics.&lt;/h2&gt;
&lt;p&gt;We will always start by fixing a local coordinate patch $(U,x)$, and work within this coordinate patch. For simplicity in our examples, we will choose manifolds/metrics which admit global coordinates. These examples will be carried throughout the text and built upon, in order to show the utility of the methods we present.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Sphere $S^2$.&lt;/em&gt;
Most commonly (and often most intuitively), a manifold is endowed with the metric arising from the restriction Euclidean metric in its embedding space to the manifold. In plain English, this means that distances are measured as they are in the embedding space, but with the additional restriction that we are required to only measure distances restricted to the surface of the space of interest. As a first example, we consider the sphere $S^2 \subset \mathbb{R}^3$. The sphere admits coordinates
$$
x = \sin \theta \cos \varphi, \qquad
y = \sin \theta \sin \varphi, \qquad
z = \cos \theta, \&lt;br&gt;
$$
where $0 \leq \theta \leq \pi, 0 \leq \varphi \leq 2 \pi$. &lt;em&gt;(Note: We are using the standard physics notation for spherical coordinates; we apologize to anyone used to the other notation, and hope it won&amp;rsquo;t cause too much confusion.&lt;/em&gt; In these coordinates, the differentials $dx$, $dy$, and $dz$ may be computed readily:
$$dx = \cos\theta \cos\varphi d\theta - \sin\theta \sin\varphi d\varphi,$$
$$dy = \cos\theta \sin\varphi d\theta + \sin\theta \cos\varphi d\varphi,$$
$$dz = -\sin\theta d\theta.$$
Taking tensor products of each coordinate is a little more tedious, but is still a feasible task (we will write expressions like $dx^2$ or $dxdy$ instead of $dx\otimes dx$ or $dx\otimes dy$, to simplify notations):
$$dx^2 = \cos^2\theta\cos^2\varphi d\theta^2 + \sin^2\theta\sin^2\varphi d\varphi^2
- \cos\theta \sin\theta \cos \varphi \sin\varphi d\theta d\varphi$$
$$ - \cos\theta \sin\theta \cos \varphi \sin\varphi  d\varphi d\theta,$$
$$dy^2 = \cos^2\theta\sin^2\varphi d\theta^2 + \sin^2\theta\cos^2\varphi d\varphi^2$$
$$+ \cos\theta \sin\theta \cos \varphi \sin\varphi d\theta d\varphi + \cos\theta \sin\theta \cos \varphi \sin\varphi  d\varphi d\theta,$$
$$dz^2 = \sin^2\theta d\theta^2.$$
Summing these three terms, we obtain the restriction of the Euclidean metric $g = dx^2 + dy^2 + dz^2$ in $\mathbb{R}^3$ to the sphere. We see that the &amp;ldquo;cross terms&amp;rdquo; (the expressions containing $d\theta d\varphi$ and $d\varphi d\theta$) cancel, and repeated use of trigonometric identities on the remaining terms yields the expression
$$ g\vert_{S^2} = d\theta^2 + \sin^2\theta d\varphi^2. $$
Instead of numbering the coordinates (which will make the notation cumbersome), we will simply use the coordinates themselves as indices, and subsequent sums over indices will be taken over these symbols. This is less complicated than it sounds: For example, the metric above has components: $g_{\theta\theta} = 1$, $g_{\theta\varphi} = g_{\varphi\theta} = 0$, and $g_{\varphi\varphi} = \sin^2\theta$. We will use a similar notation for the inverse metric tensor $g$, but with the indices in the upper position. Since $g$ is diagonal, it is easy to compute the inverse metric tensor: $g^{-1}$ has components $g^{\theta\theta} = 1$, $g^{\theta\varphi} = g^{\varphi\theta} = 0$, and $g^{\varphi\varphi} = 1 / \sin^2\theta$.
**Exercise.** Compute the metric arising from the embedding of the 2-sphere in $\mathbb{R}^3$ by stereographic projection:
\begin{equation}
x = \frac{2X}{1+X^2+Y^2}, \qquad
y = \frac{2Y}{1+X^2+Y^2}, \qquad
z = \frac{-1+X^2+Y^2}{1+X^2+Y^2},
\end{equation}
with $(X,Y) \in \mathbb{R}^2$. (Note: Technically, two stereographic projection charts are required to cover the sphere (say, one from the north pole and one from the south), but it can be checked that the metric arising from these projections has the same form in both charts. Therefore, it is enough to consider only one.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Solution:&lt;/em&gt;
\begin{equation} \label{stereographicmetric}
g|_{S^2} = \frac{4}{(1+X^2+Y^2)^2} (dX^2 + dY^2)
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Torus $\mathbb{T}^2$.&lt;/em&gt;
We apply the same procedure as before to the torus $\mathbb{T}^2$ embedded in $\mathbb{R}^3$. The torus (with $R&amp;gt;r$ being the larger and smaller radii, respectively) has coordinates
\begin{equation}
x = (R + r\cos\theta)\cos\varphi, \qquad
y = (R + r\cos\theta)\sin\varphi, \qquad
z = r\sin\theta,
\end{equation}
where $0 \leq \theta, \varphi \leq 2 \pi$. The differentials of these coordinates are then
\begin{align}
dx &amp;amp;= -r\sin\theta\cos\varphi d\theta - (R + r\cos\theta)\sin\varphi d\varphi,\\\&lt;br&gt;
dy &amp;amp;= -r\sin\theta\sin\varphi d\theta + (R + r\cos\theta)\cos\varphi d\varphi,\\\&lt;br&gt;
dz &amp;amp;= r\cos\theta d\theta.
\end{align}
Taking tensor products of each coordinate, we find that
\begin{aligned}
dx^2 &amp;amp;= r^2\sin^2\theta\cos^2\varphi d\theta^2 + (R + r\cos\theta)^2\sin^2\varphi d\varphi^2
+ r(R + r\cos\theta)\sin\theta\cos\varphi\sin\varphi d\theta d\varphi + \left(\cdot\right)d\varphi d\theta,\\\&lt;br&gt;
dy^2 &amp;amp;= r^2\sin^2\theta\sin^2\varphi d\theta^2 + (R + r\cos\theta)^2\cos^2\varphi d\varphi^2 - r(R + r\cos\theta)\sin\theta\sin\varphi\cos\varphi d\theta d\varphi - (\cdot) d\varphi d\theta,\\\&lt;br&gt;
dz^2 &amp;amp;= r^2\cos^2\theta d\theta^2.
\end{aligned}
(We have suppressed the coefficient of the $d\varphi d\theta$ terms, as they are the same as the coefficient of the $d\theta d\varphi$ terms in both cases). Summing, we see that the &amp;ldquo;cross-terms&amp;rdquo; cancel, and what remains can be simplified via trigonometric identities. The metric then takes the form:
\begin{equation}
g\vert_{\mathbb{T}^2} = r^2 d\theta^2 + (R + r\cos\theta)^2 d\varphi^2.
\end{equation}
Thus, the metric is again diagonal, with nonzero components $g_{\theta\theta} = r^2$, $g_{\varphi \varphi} = (R + r\cos\theta)^2$. Since $g$ is diagonal, we can easily compute the inverse metric tensor: $g^{-1}$ has components $g^{\theta\theta} = 1/r^2$, $g^{\varphi\varphi} = 1 / (R + r\cos\theta)^2$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Upper Half Plane $\mathbb{H}$.&lt;/em&gt;
Of course, the metric on a Riemannian manifold need not come from the embedding space: a good example of this is the &lt;em&gt;hyperbolic metric&lt;/em&gt; on the upper half plane $\mathbb{H} :=$ {$(x,y)\in\mathbb{R}^2 : y &amp;gt; 0$}. This metric is defined as
\begin{equation}\label{hyperbolicmetric}
g\vert_{\mathbb{H}} = \frac{dx^2 + dy^2}{y^2} = \frac{|dz|^2}{\vert\text{Im} z\vert^2},
\end{equation}
where here $z = x+iy$. The nonzero components of the metric are $g_{xx} = g_{yy} = \frac{1}{y^2}$, and the nonzero components of the inverse metric tensor are $g^{xx} = g^{yy} = y^2$. The relevance of this metric is that it is the unique metric on the upper half plane that is preserved under conformal automorphisms of the upper half plane (i.e., setting $w(z) = \frac{az+ b}{cz+d}$ with $a,b,c,d$ real, then $\frac{|dz|^2}{|\text{Im} z|^2} = \frac{|dw|^2}{|\text{Im} w|^2}$). We shall not discuss further properties of this metric here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Use the conformal map $w:\mathbb{D} \to \mathbb{H}$, defined by $w(z) = \frac{z+i}{iz+1}$, to pull back the metric above to the unit disc $\mathbb{D} := ${$z | |z| &amp;lt; 1$}. &lt;em&gt;Solution:&lt;/em&gt;
\begin{equation} \label{hyperbolicmetricD}
g\vert_{\mathbb{D}} = \frac{4|dz|^2}{(1-|z|^2)^2}  = \frac{4}{(1-x^2-y^2)^2} (dx^2 + dy^2).
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The 3-sphere $S^3$.&lt;/em&gt;
Consider the 3-sphere $S^3:=${$(x_0,x_1,x_2,x_3) \mid x_0^2 + x_1^2 + x_2^2 + x_3^2 = 1$}. We can parameterize the
$3$-sphere with the coordinates
\begin{equation}
\begin{cases}
x_0 &amp;amp;= \cos \psi,\\\&lt;br&gt;
x_1 &amp;amp;= \sin \psi \cos \theta,\\\&lt;br&gt;
x_2 &amp;amp;= \sin \psi \sin \theta \cos \varphi,\\\&lt;br&gt;
x_3 &amp;amp;= \sin \psi \sin \theta \sin \varphi,\\\&lt;br&gt;
\end{cases}
\end{equation}
Where $0 \leq \psi, \theta \leq \pi$, and $0 \leq \varphi \leq 2 \pi$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Use these coordinates to compute the embedding metric on $S^3$. &lt;em&gt;Solution:&lt;/em&gt;
\begin{equation} \label{3-sphere-metric}
g\vert_{S^3} = d\psi^2 + \sin^2 \psi d\theta^2 + \sin^2 \psi \sin^2 \theta d\varphi^2.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The left-invariant metric on $S^3 \cong SU(2)$ &amp;amp; Hopf coordinates.&lt;/em&gt;
Let us consider new coordinates on the sphere, called the &lt;em&gt;Hopf coordinates&lt;/em&gt;:
\begin{equation}
\begin{cases}
x_0 &amp;amp;= \cos \xi_1 \sin \eta,\\\&lt;br&gt;
x_1 &amp;amp;= \sin \xi_1 \sin \eta,\\\&lt;br&gt;
x_2 &amp;amp;= \cos \xi_2 \cos \eta,\\\&lt;br&gt;
x_3 &amp;amp;= \sin \xi_2 \cos \eta,\\\&lt;br&gt;
\end{cases}
\end{equation}
Where $0 \leq \eta \leq \frac{\pi}{2}$, $0 \leq \xi_1, \xi_2 \leq 2\pi$. It is easy to check that the embedding
metric in these coordinates is
\begin{equation} \label{SU2-metric}
g\vert_{S^3} = d\eta^2 + \sin^2 \eta d\xi_1^2 + \cos^2\eta d\xi_2^2.
\end{equation}
Putting $z_1 = x_0 + ix_1$, $z_2 = x_2 + ix_3$, we can write
\begin{equation}
U(\xi_1,\xi_2,\eta) =
\begin{pmatrix}
e^{i\xi_1}\sin\eta &amp;amp; -e^{-i\xi_2}\cos\eta\\\&lt;br&gt;
e^{i\xi_2}\cos\eta &amp;amp; e^{-i\xi_1}\sin\eta
\end{pmatrix}
=
\begin{pmatrix}
z_1 &amp;amp; -\bar{z}_2\\\&lt;br&gt;
z_2 &amp;amp; \bar{z}_1
\end{pmatrix},
\end{equation}
Which is the generic form of a matrix in $SU(2):=${$U \in GL_2(\mathbb{C}) \mid U^\dagger U = \mathbb{I}, \det U = 1$}. The fact that $(x_0,x_1,x_2,x_3)$ parameterize $S^3$ guarantees that the matrix $U(\xi_1,\xi_2,\eta)$ is unitary, and has unit determinant. This establishes a diffeomorphism between the sphere $S^3$ and the Lie group $SU(2)$. Because of the group structure of $S^3$, we now search for a metric which is invariant under the group action on itself. In fact, we shall see that, up to an overall constant, the unique left-invariant metric on $SU(2)$ is just the usual embedding metric (See the previous example). We make use of the fact that the metric above may be expressed
as $g = |dz_1|^2 + |dz_2|^2$. Let
\begin{equation}
W:= \begin{pmatrix}
w_1 &amp;amp; -\bar{w}_2\\\&lt;br&gt;
w_2 &amp;amp; \bar{w}_1
\end{pmatrix}
\end{equation}
Be another matrix in $SU(2)$, i.e. $|w_1|^2 + |w_2|^2 = 1$. Then, under a left translation,
\begin{equation}
W\cdot Z =
\begin{pmatrix}
w_1 &amp;amp; -\bar{w}_2\\\&lt;br&gt;
w_2 &amp;amp; \bar{w}_1
\end{pmatrix} &lt;br&gt;
\begin{pmatrix}
z_1 &amp;amp; -\bar{z}_2\\\&lt;br&gt;
z_2 &amp;amp; \bar{z}_1
\end{pmatrix}
=&lt;br&gt;
\begin{pmatrix}
w_1z_1-\bar{w}_2z_2 &amp;amp; -\overline{w_2z_1+\bar{w}_1z_2}\\\&lt;br&gt;
w_2z_1+\bar{w}_1z_2 &amp;amp; \overline{w_1z_1-\bar{w}_2z_2},
\end{pmatrix}
\end{equation}
i.e., the translated coordinates are
\begin{align}
\zeta_1 &amp;amp;= w_1z_1-\bar{w}_2z_2,\\\&lt;br&gt;
\zeta_2 &amp;amp;= w_2z_1+\bar{w}_1z_2.
\end{align}
Invariance of the metric under left translations is equivalent to checking that $|dz_1|^2 + |dz_2|^2 =
|d\zeta_1|^2 + |d\zeta_2|^2$. Indeed, we find that
\begin{align}
d\zeta_1 &amp;amp;= w_1dz_1-\bar{w}_2dz_2,\\\&lt;br&gt;
d\zeta_2 &amp;amp;= w_2dz_1+\bar{w}_1dz_2,
\end{align}
so that
\begin{align}
|d\zeta_1|^2 &amp;amp;= |w_1|^2|dz_1|^2 - \bar{w}_1\bar{w}_2dz_2 d\bar{z}_1 -
w_1 w_2 dz_1 d\bar{z}_2 + |w_2|^2 |dz_2|^2,\\\&lt;br&gt;
|d\zeta_2|^2 &amp;amp;= |w_2|^2|dz_1|^2 + \bar{w}_1\bar{w}_2dz_2 d\bar{z}_1 +
w_1 w_2 dz_1 d\bar{z}_2 + |w_1|^2 |dz_2|^2;
\end{align}
adding these expressions yields the identity $|dz_1|^2 + |dz_2|^2 = |d\zeta_1|^2 + |d\zeta_2|^2$.
**Exercise.**
Check that the vector fields
\begin{align}
\sigma_1 &amp;amp;:= \frac{1}{2} \left( -\tan\eta\cos(\xi_1+\xi_2)\frac{\partial}{\partial \xi_1}
+ \cot\eta\cos(\xi_1+\xi_2)\frac{\partial}{\partial \xi_2}+ \sin(\xi_1 + \xi_2) \frac{\partial}{\partial \eta}\right),\\\&lt;br&gt;
\sigma_2 &amp;amp;:= \frac{1}{2} \left(\tan\eta\sin(\xi_1+\xi_2)\frac{\partial}{\partial \xi_1} -
\cot\eta\sin(\xi_1+\xi_2)\frac{\partial}{\partial \xi_2} + \cos(\xi_1 + \xi_2) \frac{\partial}{\partial \eta}\right),\\\&lt;br&gt;
\sigma_3 &amp;amp;:= \frac{1}{2} \left( \frac{\partial}{\partial \xi_1} + \frac{\partial}{\partial \xi_2} \right),
\end{align}
satisfy the commutation relations $[\sigma_i, \sigma_j] = \epsilon_{ijk} \sigma_k$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Schwarzschild metric.&lt;/em&gt;
Riemannian geometry goes hand-in-hand with Einstein&amp;rsquo;s theory of general relativity. One of the first exact solutions to Einstein&amp;rsquo;s field equations is the &lt;em&gt;Schwarzschild solution&lt;/em&gt;:
\begin{equation}\label{Schwarzschildmetric}
g = -\left(1-\frac{r_s}{r}\right) dt^2 + \left(1-\frac{r_s}{r}\right)^{-1} dr^2 + r^2 d\theta^2 + r^2 \sin^2\theta d\varphi^2,
\end{equation}
which (as we shall see) describes a $\delta$-function mass at the origin of an otherwise empty spacetime. $g$ is a pseudo-Riemannian metric on $\mathbb{R}^4$, and has an apparent singularity at $r = r_s$, the &lt;em&gt;Schwarzschild radius&lt;/em&gt;. However, this &amp;ldquo;singularity&amp;rdquo; can be removed by an appropriate change of coordinates (see, for example, 
&lt;a href=&#34;http://www.fulviofrisone.com/attachments/article/486/Wald%20-%20General%20Relativity.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robert Wald&amp;rsquo;s book&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;2-the-levi-cevita-connection&#34;&gt;$2$. The Levi-Cevita Connection.&lt;/h2&gt;
&lt;p&gt;There are several ways to derive the Levi-Cevita connection (i.e., the unique connection on the tangent bundle which annihilates the metric and has vanishing torsion) from the metric; however, it is not our purpose to derive the Levi-Cevita connection, and so we will present the result here. The components of the connection, called the &lt;em&gt;Christoffel symbols&lt;/em&gt;, have three associated indices: $\Gamma^i_{jk}$; in terms of the metric, the component $\Gamma^i_{jk}$ is given by
\begin{equation} \label{Christoffel}
\Gamma^i_{jk} = \frac{1}{2}\sum_\ell g^{i\ell} \left( \partial_j g_{\ell k} + \partial_k g_{j \ell} -\partial_\ell g_{jk}\right).
\end{equation}
Here, $\partial_j = \frac{\partial}{\partial x_j}$, the derivative with respect to the $j^{th}$ local coordinate. It is worth committing the formula above to memory. Here are a few useful techniques for doing this: Looking closely at the indices, we see that the first two terms are obtained by switching one of the lower indices in $\Gamma^i_{jk}$ with $\ell$, and then differentiating with respect to this variable; note that both terms have a &amp;ldquo;$+$&amp;rdquo; sign in front of them. The last term is obtained by differentiating the component of the metric tensor with indices matching the lower indices of $\Gamma^i_{jk}$ with respect to the summation variable; note that this is the only term with a &amp;ldquo;$-$&amp;rdquo; coefficient. Finally, note that the only place that the upper index appears is in the inverse metric tensor expression.&lt;/p&gt;
&lt;p&gt;Some further remarks about the Christoffel symbols:&lt;/p&gt;
&lt;p&gt;$1$. Note that $\Gamma^i_{jk}$ is symmetric in the indices $j$ and $k$, as a consequence of the fact that $g_{ij} = g_{ji}$.&lt;/p&gt;
&lt;p&gt;$2$. If the metric is diagonal, i.e. $g_{ij} \neq 0 $ only if $i= j$, then $g^{ij}$ is also diagonal, and thus there is no sum over $\ell$ in the above equation: the Christoffel symbols are simply
\begin{equation} \label{simplechrist}
\Gamma^i_{jk} = \frac{1}{2} g^{ii} \left( \partial_j g_{ik} + \partial_k g_{ji} -\partial_i g_{jk}\right);
\end{equation}
This often drastically reduces the number of computations one has to make.
Although, formally speaking, we can now compute the connection, the Christoffel symbols are somewhat difficult to manage; an object with three indices (and which is moreover not tensorial) can be difficult to deal with in general. However, we can improve our situation, at least conceptually. We will think of the connection as a _matrix-valued 1-form_:
\begin{equation}\label{matrix1form}
\Gamma = \sum_k \Gamma_k dx^k,
\end{equation}
where each of the $\Gamma_k$&amp;rsquo;s is a matrix. The ${(ij)}^{th}$ component of the matrix $\Gamma_k$ is given by $(\Gamma_k)^i_j = \Gamma^i_{jk}$. Note that the placement of indices on the left hand side is consistent with that of the right. This definition also allows us to clearly write the Levi-Cevita connection as an operator on vector fields: given a vector field $X$, the connection applied to $X$ can be written as (at least in the local coordinate patch on which we are working)
\begin{equation}
\nabla X = d X + \Gamma X,
\end{equation}
Where $\Gamma$ acts on $X$ as a matrix applied to a vector. As a consistency check, we see that both terms in the above have one upper and one lower index, and are thus of the same type, so that $\nabla X$ is indeed a well-defined and (as we will soon check) tensorial quantity.
Let us begin to demonstrate the utility of this notation. Suppose $(U,x)$, $(V,y)$ are overlapping coordinate charts; consider the change of coordinates $y \to x(y)$. Using our original formulafor the Christoffel symbols, we can (rather tediously) compute $\tilde{\Gamma}^i_{jk}$ in the new coordinates:
\begin{equation}
\tilde{\Gamma}^i_{jk} = \sum_{m,n,p}\left(\frac{\partial x^i}{\partial y^m}\frac{\partial y^n}{\partial x^j}\frac{\partial y^p}{\partial x^k} \Gamma^m_{np} + \frac{\partial^2 y^m}{\partial x^j \partial x^k} \frac{\partial x^i}{\partial y^m} \right).
\end{equation}
Now, adding in the contribution of $dx^k \to \frac{\partial x^k}{\partial y^r} dy^r$, we see that one of the $\frac{\partial y}{\partial x}$ factors
cancels in the expression for $\tilde{\Gamma}^i_{jk} dy^k$.
We can simplify this expression (at least notationally speaking) significantly, Define $\lambda$ to be the matrix-valued function with entries $(\lambda)^i_j = \frac{\partial y^i}{\partial x^j}$. Then, in matrix notation the new expression for the Christoffel symbols reads
\begin{equation}
\tilde{\Gamma} = \lambda \Gamma \lambda^{-1} - d\lambda \cdot\lambda^{-1}.
\end{equation}
The discerning reader will recognize this as what is known as a _gauge transformation_ in physics. This is, of course, no coincidence: a Riemannian manifold can be viewed as a principal- $GL_n(\mathbb{R})$ bundle and (gauge-invariant) connection $\nabla = d + \Gamma$. (If this sentence does not mean anything to you right now, do not worry; it is not relevant to the remainder of this exposition).&lt;/p&gt;
&lt;p&gt;Now, we will check that, indeed, for any vector field $X$, $\nabla X$ is tensorial. Let us use the coordinate transformation of the previous section. Under such a transformation, the vector field $X$ transforms as $\lambda X$. Altogether, we find that, in the new coordinates,
\begin{align}
\tilde{\nabla} \tilde{X} &amp;amp;= \left(d +  \lambda \Gamma \lambda^{-1} - d\lambda \cdot\lambda^{-1}\right) \lambda X\\\
&amp;amp;= d\lambda \cdot X + \lambda dX + \lambda \Gamma X - d\lambda \cdot X \\\&lt;br&gt;
&amp;amp;= \lambda dX + \lambda \Gamma X = \lambda \nabla X,
\end{align}
so that $\nabla X$ is a tensorial quantity, and transforms like a tensor with one additional lower index than $X$.
We conclude this section with some computations carried over from the previous section.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;em&gt;The Sphere $S^2$.&lt;/em&gt;
Let us compute the connection coefficients of the Levi-Cevita connection arising from the metric we derived in the previous section. Since the metric is diagonal, we can use formula we derived for the $\Gamma$&amp;rsquo;s for a diagonal metric to compute the Christoffel symbols. Furthermore, since the metric is diagonal, and the only nonconstant component of the metric is $g_{\varphi \varphi}$, we see that the only non-vanishing Christoffel symbols will be those that involve an expression of the form $\partial_\theta g_{\varphi \varphi}$ (since partial derivatives of constants will yield zero). Comparing with diagonal formula, we conclude that the only non-vanishing Christoffel symbols are $\Gamma^{\varphi}_{\varphi\theta} = \Gamma^{\varphi}_{\theta\varphi}$, and $\Gamma^{\theta}_{\varphi\varphi}$. We compute these entries explicitly:
\begin{align}
\Gamma^{\varphi}_{\varphi\theta} = \Gamma^{\varphi}_{\theta\varphi} &amp;amp;= \frac{1}{2} g^{\varphi \varphi} \left( \partial_\varphi g_{\varphi\theta} + \partial_\theta g_{\varphi\varphi} - \partial_\varphi g_{\varphi\theta} \right)
= \frac{1}{2} g^{\varphi \varphi} \left( \partial_\theta g_{\varphi\varphi} \right) = \frac{1}{2} \frac{1}{\sin^2\theta} (2\sin\theta \cos \theta) = \cot\theta, \\\&lt;br&gt;
\Gamma^{\theta}_{\varphi\varphi} &amp;amp;= \frac{1}{2} g^{\theta \theta} \left( \partial_\varphi g_{\theta\varphi} + \partial_\varphi g_{\varphi\theta} - \partial_\theta g_{\varphi\varphi} \right) = -\frac{1}{2} g^{\theta \theta}\left(  \partial_\theta g_{\varphi\varphi} \right) = -\frac{1}{2} (2\sin\theta \cos \theta) = -\sin\theta \cos \theta.
\end{align}
We now write the connection coefficients as a matrix-valued 1-form. Formally, this 1-form should look like
\begin{equation}
\Gamma = \Gamma_\theta d\theta+ \Gamma_\varphi d\varphi=
\begin{pmatrix} \Gamma^\theta_{\theta \theta} &amp;amp;  \Gamma^\theta_{\varphi\theta}\\\ \Gamma^\varphi_{\theta\theta} &amp;amp; \Gamma^\varphi_{\varphi\theta} \end{pmatrix}
d \theta +
\begin{pmatrix}  \Gamma^\theta_{\theta \varphi} &amp;amp;  \Gamma^\theta_{\varphi\varphi}\\\ \Gamma^\varphi_{\theta\varphi} &amp;amp; \Gamma^\varphi_{\varphi\varphi} \end{pmatrix}
d\varphi.
\end{equation}
Using the expressions we found earlier for the Christoffel symbols, we find that $\Gamma$ explicitly takes the form
\begin{equation} \label{sphereconn}
\Gamma = \Gamma_\theta d\theta + \Gamma_\varphi d\varphi=
\begin{pmatrix} 0 &amp;amp; 0 \\\ 0 &amp;amp; \cot \theta \end{pmatrix}
d \theta +
\begin{pmatrix}  0 &amp;amp;  -\sin\theta \cos\theta \\\ \cot\theta&amp;amp; 0 \end{pmatrix}
d\varphi.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Compute the connection form of the stereographic metric on the sphere.
&lt;em&gt;Solution:&lt;/em&gt;
Set $\rho(X,Y) = 1 + X^2 + Y^2$. Then the connection form is
\begin{equation}\label{stereographicconn}
\Gamma = \Gamma_X dX + \Gamma_Y dY =
\frac{2}{\rho} \begin{pmatrix} -X &amp;amp; -Y \\\ Y &amp;amp; -X \end{pmatrix}
d X +
\frac{2}{\rho} \begin{pmatrix}  -Y &amp;amp;  X \\\ -X &amp;amp; -Y \end{pmatrix}
dY.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Torus $\mathbb{T}^2$.&lt;/em&gt;
Using the explicit expression for the metric on the torus, and the formula for the Christoffel symbols of a diagonal metric, we can compute the coefficients of the Levi-Cevita connection. Let us first determine all nonzero Christoffel symbols. Since the only non-constant entry in the metric is $g_{\varphi\varphi} = (R + r\cos\theta)^2$, and this coefficient depends only on the variable $\theta$, we see that the only nonzero Christoffel symbols will be those which contain the term $\partial_{\theta} g_{\varphi \varphi}$. There are precisely three of these: $\Gamma^\varphi_{\varphi \theta} = \Gamma^\varphi_{\theta\varphi}$, and $\Gamma^{\theta}_{\varphi \varphi}$. We compute these terms explicitly:
\begin{align}
\Gamma^\varphi_{\varphi \theta} = \Gamma^\varphi_{\theta\varphi} &amp;amp;= \frac{1}{2} g^{\varphi \varphi} \left( \partial_\varphi g_{\varphi \theta} + \partial_\theta g_{\varphi \varphi} - \partial_\varphi g_{\varphi \theta}\right) = \frac{1}{2} g^{\varphi \varphi} \left(\partial_\theta g_{\varphi \varphi}\right) = \frac{-r\sin\theta}{R + r\cos\theta},\\\&lt;br&gt;
\Gamma^{\theta}_{\varphi \varphi} &amp;amp;= \frac{1}{2}g^{\theta\theta} \left( \partial_\varphi g_{\theta \varphi} + \partial_\varphi g_{\varphi \theta} - \partial_\theta g_{\varphi\varphi}\right) = \frac{(R + r \cos\theta)}{r} \sin\theta.
\end{align}
Now, writing the connection as a matrix-valued 1-form, we find that
\begin{equation}\label{torusconn}
\Gamma = \Gamma_\theta d\theta + \Gamma_\varphi d\varphi=
\begin{pmatrix} 0 &amp;amp; 0 \\\ 0 &amp;amp; \frac{-r\sin\theta}{R + r\cos\theta} \end{pmatrix}
d \theta +
\begin{pmatrix}  0 &amp;amp; \frac{(R + r \cos\theta)}{r} \sin\theta\\\  \frac{-r\sin\theta}{R + r\cos\theta}&amp;amp; 0 \end{pmatrix}
d\varphi.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Upper Half Plane $\mathbb{H}$.&lt;/em&gt;
We now compute the Christoffel symbols of the hyperbolic metric on the upper half plane. Note that only Christoffel symbols containing the terms $\partial_y g_{xx}$ or $\partial_y g_{yy}$ have a chance of being nonzero. Thus, the nonzero Christoffel symbols are $\Gamma^x_{yx} = \Gamma^x_{xy}$, $\Gamma^y_{xx}$, and $\Gamma^y_{yy}$. Since the metric is diagonal, we can again use the nice formula we derived to compute the Christoffel symbols: we find that
\begin{align}
\Gamma^x_{yx} = \Gamma^x_{xy} &amp;amp;= \frac{1}{2} g^{xx}\left(\partial_y g_{xx} + \partial_x g_{yx} - \partial_x g_{yx}\right) = \frac{1}{2} g^{xx}\left(\partial_y g_{xx}\right) = -1/y, \\\&lt;br&gt;
\Gamma^y_{xx} &amp;amp;= \frac{1}{2} g^{yy}\left(\partial_x g_{yx} + \partial_x g_{xy} - \partial_y g_{xx}\right) = -\frac{1}{2} g^{yy}\left( \partial_y g_{xx}\right) = 1/y,\\\&lt;br&gt;
\Gamma^y_{yy} &amp;amp;= \frac{1}{2} g^{yy}\left(\partial_y g_{yy} + \partial_y g_{yy} - \partial_y g_{yy}\right) =
\frac{1}{2} g^{yy}\left(\partial_y g_{yy}\right) = -1/y.
\end{align}
Written as a matrix-valued 1-form, the connection takes the form
\begin{equation}\label{halfplaneconn}
\Gamma = \Gamma_x dx + \Gamma_y dy = \begin{pmatrix}  0 &amp;amp; -1/y \\\  1/y&amp;amp; 0 \end{pmatrix}  dx + \begin{pmatrix}  -1/y &amp;amp; 0 \\\  0 &amp;amp; -1/y \end{pmatrix} dy.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt; Compute the connection form of the hyperbolic metric on the disc. &lt;em&gt;Solution:&lt;/em&gt; Set $\sigma(x,y) = 1 - x^2 - y^2$; then the connection form is given as
\begin{equation}
\Gamma = \Gamma_x dx + \Gamma_y dy = \frac{2}{\sigma} \begin{pmatrix} x &amp;amp; y \\\ -y &amp;amp; x \end{pmatrix} dx + \frac{2}{\sigma} \begin{pmatrix}  y &amp;amp;  -x \\\ x &amp;amp; y \end{pmatrix} dy.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The 3-sphere $S^3$.&lt;/em&gt;
We compute the connection form from the metric on the 3-sphere we derived previously. Since the only nonzero components of the
metric are $g_{\psi\psi} = 1$, $g_{\theta\theta} = \sin^2\psi$, and $g_{\varphi\varphi} = \sin^2\psi\sin^2\theta$,
the only nonvanishing derivatives of the metric will be $\partial_\psi g_{\theta \theta}$, $\partial_{\psi} g_{\varphi\varphi}$,
and $\partial_{\theta}g_{\varphi \varphi}$. Thus, we must compute the $9$ nontrivial Christoffel symbols:
$\Gamma^{\psi}_{\theta\theta}$, $\Gamma^{\theta}_{\theta \psi} = \Gamma^{\theta}_{\psi \theta}$,
$\Gamma^{\psi}_{\varphi \varphi}$, $\Gamma^{\varphi}_{\psi \varphi} = \Gamma^{\varphi}_{\varphi \psi}$,
$\Gamma^{\theta}_{\varphi \varphi}$, and $\Gamma^{\varphi}_{\theta \varphi} = \Gamma^{\varphi}_{\varphi\theta}$.
By direct computation, we find that
\begin{align}
\Gamma^{\psi}_{\theta\theta} &amp;amp;= -\sin\psi \cos \psi,\\\&lt;br&gt;
\Gamma^{\theta}_{\theta \psi} &amp;amp;= \Gamma^{\theta}_{\psi \theta} = \cot \psi,\\\
\Gamma^{\psi}_{\varphi \varphi} &amp;amp;= -\sin \psi \cos \psi \sin^2\theta,\\\&lt;br&gt;
\Gamma^{\varphi}_{\psi \varphi} &amp;amp;= \Gamma^{\varphi}_{\varphi \psi} = \cot \theta,\\\&lt;br&gt;
\Gamma^{\theta}_{\varphi \varphi} &amp;amp;= -\sin \theta \cos \theta,\\\&lt;br&gt;
\Gamma^{\varphi}_{\theta \varphi} &amp;amp;= \Gamma^{\varphi}_{\varphi\theta} = \cot\theta.
\end{align}
The connection form is thus
\begin{equation}\label{3-sphere-conn}
\Gamma = \begin{pmatrix} 0 &amp;amp; 0 &amp;amp;0\\\ 0 &amp;amp; \cot\psi &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; \cot \psi \end{pmatrix} d\psi
+ \begin{pmatrix} 0 &amp;amp; -\sin\psi\cos\psi &amp;amp;0\\\  \cot\psi &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; \cot \theta \end{pmatrix} d\theta
+ \begin{pmatrix} 0 &amp;amp; 0 &amp;amp; -\sin\psi\cos\psi\sin^2\theta\\\  0 &amp;amp; 0 &amp;amp; -\sin\theta\cos\theta\\\  \cot \psi &amp;amp;  \cot \theta &amp;amp; 0 \end{pmatrix} d\varphi
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The left-invariant metric on $S^3 \cong SU(2)$ &amp;amp; Hopf coordinates.&lt;/em&gt;
Recall the invariant metric on $S^3$ had entries $g_{\eta \eta} = 1$, $g_{\xi_1\xi_1} = \sin^2\eta$,
and $g_{\xi_2\xi_2} = \cos^2\eta$. Thus, the only non-vanishing derivatives of the metric tensor will be
$\partial_{\eta} g_{\xi_1 \xi_1}$ and $\partial_{\eta} g_{\xi_2 \xi_2}$; consequentially, there are only $6$
non-vanishing connection coefficients: $\Gamma^{\eta}_{\xi_1\xi_1}$, $\Gamma^{\xi_1}_{\eta \xi_1} =
\Gamma^{\xi_1}_{\xi_1 \eta}$, $\Gamma^{\eta}_{\xi_2\xi_2}$, and $\Gamma^{\xi_2}_{\eta \xi_2} =
\Gamma^{\xi_2}_{\xi_2 \eta}$. We compute that:
\begin{align*}
\Gamma^{\eta}_{\xi_1 \xi_1} &amp;amp;= -\cos\eta \sin\eta,\\\&lt;br&gt;
\Gamma^{\xi_1}_{\eta \xi_1} &amp;amp;=  \Gamma^{\xi_1}_{\xi_1 \eta} = \cot \eta,\\\&lt;br&gt;
\Gamma^{\eta}_{\xi_2 \xi_2} &amp;amp;= \cos\eta \sin\eta,\\\&lt;br&gt;
\Gamma^{\xi_2}_{\eta \xi_2} &amp;amp;=  \Gamma^{\xi_2}_{\xi_2 \eta} = -\tan \eta.
\end{align*}
The connection form is thus
\begin{equation} \label{SU2-conn}
\Gamma = \begin{pmatrix} 0 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; \cot \eta &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; -\tan \eta \end{pmatrix}d\eta + \begin{pmatrix} 0 &amp;amp; -\cos\eta \sin \eta &amp;amp; 0\\\ \cot\eta &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; 0 \end{pmatrix}d\xi_1 + \begin{pmatrix} 0 &amp;amp; 0 &amp;amp; \cos\eta \sin \eta\\\ 0 &amp;amp; 0 &amp;amp; 0\\\ -\tan\eta &amp;amp; 0 &amp;amp; 0 \end{pmatrix}d\xi_2.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Schwarzschild metric.&lt;/em&gt;
We now compute the connection form from the Schwarzshield metric. By inspection, we see that the only nonvanishing derivatives of the metric are: $\partial_r g_{tt}$, $\partial_r g_{rr}$, $\partial_r g_{\theta\theta}$, $\partial_r g_{\varphi \varphi}$, and $\partial_\theta g_{\varphi \varphi}$. Since the metric is diagonal, the only non-vanishing Christoffel symbols are: $\Gamma^r_{tt}$, $\Gamma^t_{rt} = \Gamma^t_{tr}$, $\Gamma^r_{rr}$, $\Gamma^r_{\theta\theta}$, $\Gamma^\theta_{r \theta} = \Gamma^\theta_{\theta r}$, $\Gamma^r_{\varphi\varphi}$, $\Gamma^\varphi_{r \varphi} = \Gamma^\varphi_{\varphi r}$, $\Gamma^\theta_{\varphi \varphi}$, and finally $\Gamma^\varphi_{\theta \varphi} = \Gamma^\varphi_{\varphi \theta}$. These quantities are computed below:
\begin{align}
\Gamma^r_{tt} &amp;amp;=  -\frac{1}{2}g^{rr} \left(\partial_r g_{tt}\right) = \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right),\\\&lt;br&gt;
\Gamma^t_{rt} = \Gamma^t_{tr} &amp;amp;= \frac{1}{2}g^{tt} \left(\partial_r g_{tt}\right) = \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right)^{-1},\\\&lt;br&gt;
\Gamma^r_{rr} &amp;amp;= \frac{1}{2}g^{rr}\left(\partial_r g_{rr}\right) = -\frac{1}{2}\frac{r_s}{r^2} \left(1-\frac{r_s}{r}\right)^{-1},\\\&lt;br&gt;
\Gamma^r_{\theta\theta} &amp;amp;= -\frac{1}{2}g^{rr} \left(\partial_r g_{\theta\theta}\right) = -r\left(1 - \frac{r_s}{r}\right),\\\&lt;br&gt;
\Gamma^\theta_{r \theta} = \Gamma^\theta_{\theta r} &amp;amp;= \frac{1}{2}g^{\theta\theta} \left(\partial_r g_{\theta\theta}\right) = \frac{1}{r},\\\&lt;br&gt;
\Gamma^r_{\varphi\varphi} &amp;amp;= -\frac{1}{2}g^{rr} \left(\partial_r g_{\varphi\varphi}\right) = -r\sin^2\theta\left(1-\frac{r_s}{r}\right), \\\&lt;br&gt;
\Gamma^\varphi_{r \varphi} = \Gamma^\varphi_{\varphi r} &amp;amp;= \frac{1}{2}g^{\varphi\varphi} \left(\partial_r g_{\varphi\varphi}\right) = \frac{1}{r},\\\&lt;br&gt;
\Gamma^\theta_{\varphi \varphi} &amp;amp;= -\frac{1}{2}g^{\theta\theta} \left(\partial_\theta g_{\varphi\varphi}\right) = -\sin\theta\cos\theta,\\\&lt;br&gt;
\Gamma^\varphi_{\theta \varphi} = \Gamma^\varphi_{\varphi \theta} &amp;amp;= \frac{1}{2}g^{\varphi\varphi} \left(\partial_\theta g_{\varphi\varphi}\right) = \cot\theta.\\\&lt;br&gt;
\end{align}
Therefore, in matrix form, the Schwarzschild connection looks like
\begin{equation}\label{Schwarzschildconn}
\begin{split}
\Gamma&amp;amp;=\begin{pmatrix}  0 &amp;amp; \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right)^{-1} &amp;amp; 0 &amp;amp; 0 \\\ \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right) &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt +
\begin{pmatrix}  \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right)^{-1} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; -\frac{1}{2}\frac{r_s}{r^2} \left(1-\frac{r_s}{r}\right)^{-1} &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; \frac{1}{r} &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \frac{1}{r}\end{pmatrix} dr \\\&lt;br&gt;
&amp;amp;+ \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; -r\left(1 - \frac{r_s}{r}\right) &amp;amp; 0 \\\ 0 &amp;amp; \frac{1}{r} &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cot\theta\end{pmatrix} d\theta +
\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -r\sin^2\theta\left(1-\frac{r_s}{r}\right) \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\sin\theta\cos\theta \\\ 0 &amp;amp; \frac{1}{r} &amp;amp; \cot\theta &amp;amp; 0\end{pmatrix} d\varphi.
\end{split}
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;3-the-riemann-tensor&#34;&gt;$3$. The Riemann Tensor.&lt;/h2&gt;
&lt;p&gt;The next object we would like to be able to compute is the Riemann curvature tensor. Seemingly, we are in a much worse situation than before, as we now have to describe an object with 4 indices. In fact, this will turn out to be manageable: just as we expressed the connection coefficients as a matrix-valued 1-form, we will express the Riemann tensor as a matrix-valued 2-form. Contrary to the notation popular in Riemannian geometry, we will denote the Riemann tensor by $\Omega$, and establish a correspondence to the usual notation $R^i_{jkl}$. As usual, the Riemann tensor is defined as $\Omega := \nabla \circ \nabla$, the connection applied to itself. In our formalism, computing the Riemann tensor amounts to the following computation. Let $X$ be any vector field; then
\begin{align}
\Omega X&amp;amp; = (\nabla \circ \nabla) X = \nabla(dX + \Gamma X)
= d(dX + \Gamma X) + \Gamma (dX + \Gamma X)\\\&lt;br&gt;
&amp;amp;=d(dX) + d\Gamma \cdot X - \Gamma dX + \Gamma dX + (\Gamma \wedge \Gamma) X \\\&lt;br&gt;
&amp;amp;= (d\Gamma + \Gamma \wedge \Gamma)X,
\end{align}
Since $d^2 = 0$. Therefore, locally, we can write the curvature tensor as the matrix-valued 2-form $\Omega = d\Gamma + \Gamma \wedge \Gamma$ _Note: $\Gamma \wedge \Gamma$ is nonzero in general, since $\Gamma$ has matrix coefficients. This will be made apparent in the examples._ This is consistent with the Riemann tensor having 1 upper and 3 lower indices (1 upper and 1 lower contributing to the matrix coefficient, and the remaining 2 lower contributing to the 2-form). Write $\Omega$ as $\Omega = \Omega_{kl} dx^k \wedge dx^l$, where each $\Omega_{kl}$ is a matrix. In the usual notation of the Riemann tensor, the $(ij)^{th}$ component of the matrix $\Omega_{kl}$ is given by $(\Omega_{kl})^i_j = R^i_{jkl}$. As an immediate consequence, we see that the Riemann tensor is antisymmetric in two of its indices.&lt;/p&gt;
&lt;p&gt;Furthermore, this notation allows for an almost trivial proof of the second Bianchi identity: Note that $\Omega \nabla = ${$(d+\Gamma)(d+\Gamma)$}$(d+\Gamma) = (d+\Gamma)${$(d+\Gamma)(d+\Gamma)$} $= \nabla \Omega$. Applying this identity to a vector field $X$, we see that $\Omega \nabla X = \nabla(\Omega X) = \Omega \nabla X + (\nabla \Omega) X$, implying that $\nabla \Omega = 0$.&lt;/p&gt;
&lt;p&gt;The Riemann tensor is now a more manageable quantity, and we now show by direct computation (through the examples of the previous sections) that working with it is not too difficult.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Sphere $S^2$.&lt;/em&gt;
We now compute the Riemann tensor of the sphere using the connection form we found in the last section. We must compute two quantities: $d\Gamma$ and $\Gamma\wedge\Gamma$. Let us begin by computing $d\Gamma$: since $\Gamma = \Gamma_\theta d\theta + \Gamma_\varphi d\varphi$, we find that $d \Gamma = \left( \partial_\theta \Gamma_\varphi - \partial_\varphi \Gamma_\theta\right) d\theta \wedge d\varphi$. Now, since $\Gamma_\theta$ is independent of $\varphi$, we see that this expression simplifies to $d\Gamma = \partial_\theta \Gamma_\varphi d\theta \wedge d\varphi$. Explicitly (i.e., applying this prescription to the connection on the sphere we derived), we find that
\begin{equation}\label{dgammasphere}
d\Gamma = \partial_\theta\begin{pmatrix}  0 &amp;amp; -\sin\theta \cos\theta \\\  \cot\theta&amp;amp; 0 \end{pmatrix}
d\theta \wedge d\varphi = \begin{pmatrix}  0 &amp;amp;  \sin^2\theta - \cos^2\theta\\\ -\csc^2\theta&amp;amp; 0 \end{pmatrix}
d\theta \wedge d\varphi.
\end{equation}
Now, we are left to compute $\Gamma\wedge\Gamma$. We find that $\Gamma\wedge\Gamma = \left(\Gamma_\theta d\theta + \Gamma_\varphi d\varphi\right)\wedge \left(\Gamma_\theta d\theta + \Gamma_\varphi d\varphi\right) = [\Gamma_\theta,\Gamma_\varphi] d\theta\wedge d\varphi$, where $[\cdot,\cdot]$ denotes the matrix commutator. After some simple matrix algebra, one finds that
\begin{equation}\label{gammawedgesphere}
\Gamma\wedge\Gamma = [\Gamma_\theta,\Gamma_\varphi] d\theta\wedge d\varphi = \begin{pmatrix}  0 &amp;amp; \cos^2\theta \\\ \cot^2\theta &amp;amp; 0 \end{pmatrix} d\theta \wedge d\varphi.
\end{equation}
Combining these results, we find that
\begin{equation}\label{spherecurvature}
\Omega = d\Gamma + \Gamma \wedge \Gamma = \begin{pmatrix}  0 &amp;amp; \sin^2\theta \\\ -1 &amp;amp; 0 \end{pmatrix} d\theta \wedge d\varphi.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Compute the curvature form of the sphere with stereographic connection.
&lt;em&gt;Solution:&lt;/em&gt; Again, setting $\rho(X,Y) = 1 + X^2 + Y^2$, the curvature tensor takes the form
\begin{equation} \label{stereographiccurvature}
\Omega = \begin{pmatrix}  0 &amp;amp; 4/\rho \\\ -4/\rho &amp;amp; 0 \end{pmatrix} dX \wedge dY.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;em&gt;The Torus $\mathbb{T}^2$.&lt;/em&gt;
Continuing the example of the torus, we now compute the Riemann tensor on the torus from the connection. We separately compute $d\Gamma$ and $\Gamma\wedge\Gamma$. Now, $d\Gamma = \left( \partial_\theta \Gamma_\varphi - \partial_\varphi \Gamma_\theta \right) d\theta\wedge d\varphi = \partial_\theta \Gamma_\varphi d\theta\wedge d\varphi$, since $\Gamma_\theta$ is independent of $\varphi$. Thus, we find that
\begin{equation}\label{torusdgamma}
d\Gamma =  \partial_\theta \Gamma_\varphi d\theta\wedge d\varphi = \partial_\theta \begin{pmatrix}  0 &amp;amp; \frac{(R + r \cos\theta)}{r}\sin\theta \\\  \frac{-r\sin\theta}{R + r\cos\theta}&amp;amp; 0 \end{pmatrix}
d\theta \wedge d\varphi = \begin{pmatrix}  0 &amp;amp; \frac{\cos\theta(R+r\cos\theta)}{r}-\sin^2\theta \\\  \frac{-r(r+R\cos\theta)}{(R + r\cos\theta)^2}&amp;amp; 0 \end{pmatrix}
d\theta \wedge d\varphi.
\end{equation}
Furthermore, we find that $\Gamma\wedge\Gamma = \left(\Gamma_\theta d\theta + \Gamma_\varphi d\varphi\right)\wedge \left(\Gamma_\theta d\theta + \Gamma_\varphi d\varphi\right) = [\Gamma_\theta,\Gamma_\varphi] d\theta\wedge d\varphi$; after some computations, we see that this simplifies to
\begin{equation}\label{torusgammawedge}
\Gamma\wedge\Gamma = [\Gamma_\theta,\Gamma_\varphi] d\theta\wedge d\varphi = \begin{pmatrix}  0 &amp;amp; \sin^2\theta \\\  \frac{r^2 \sin^2\theta}{(R+ r\cos\theta)^2} &amp;amp; 0 \end{pmatrix}
d\theta \wedge d\varphi.
\end{equation}
Combining these results, we obtain the curvature tensor:
\begin{equation} \label{toruscurvature}
\Omega = d\Gamma + \Gamma\wedge\Gamma =  \begin{pmatrix}  0 &amp;amp; \frac{\cos\theta(R+r\cos\theta)}{r} \\\  -\frac{r \cos\theta}{R+ r\cos\theta} &amp;amp; 0 \end{pmatrix} d\theta \wedge d\varphi.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;strong&gt;The Upper Half Plane $\mathbb{H}$.&lt;/strong&gt;
We now find the Riemann tensor on the upper half plane endowed with the hyperbolic metric; as before, we must compute $d\Gamma$ and $\Gamma \wedge \Gamma$. Now, $d\Gamma = \left(\partial_x \Gamma_y - \partial_y \Gamma_x\right) dx\wedge dy = -\partial_y \Gamma_x dx\wedge dy$, since $\Gamma_y$ is independent of $x$. Therefore:
\begin{equation}\label{dgammaHP}
d\Gamma = -\partial_y \Gamma_x dx\wedge dy = -\partial_y \begin{pmatrix}  0 &amp;amp; -1/y \\\  1/y&amp;amp; 0 \end{pmatrix}  dx\wedge dy = \begin{pmatrix}  0 &amp;amp; -1/y^2 \\\  1/y^2 &amp;amp; 0 \end{pmatrix}  dx\wedge dy.
\end{equation}
Furthermore, we must compute $\Gamma\wedge\Gamma = [\Gamma_x,\Gamma_y] dx\wedge dy$. We find that:
\begin{equation}
\Gamma\wedge\Gamma = [\Gamma_x,\Gamma_y] dx\wedge dy = 0.
\end{equation}
(This is due to the fact that $\Gamma_y$ is a multiple of the identity matrix). Therefore, the curvature tensor is
\begin{equation}
\Omega = d\Gamma + \Gamma\wedge\Gamma = \begin{pmatrix}  0 &amp;amp; -1/y^2 \\\  1/y^2 &amp;amp; 0 \end{pmatrix}  dx\wedge dy.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Compute the curvature form of the disc with the hyperbolic connection. &lt;em&gt;Solution:&lt;/em&gt;
Setting $\sigma(x,y) = 1 - x^2 - y^2$, the curvature form is
\begin{equation}\label{hyperbolicD}
\Omega = \begin{pmatrix}  0 &amp;amp; -4/\sigma^2 \\\  4/\sigma^2 &amp;amp; 0 \end{pmatrix}  dx\wedge dy.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt; &lt;em&gt;The 3-sphere $S^3$.&lt;/em&gt;
Using the first expression for the connection on $S^3$, compute the Riemann tensor.
&lt;em&gt;Solution:&lt;/em&gt;
\begin{equation} \label{3-sphere-curv}
\Omega = \begin{pmatrix} 0 &amp;amp;\sin^2\psi &amp;amp; 0\\\ -1 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix}d\psi\wedge d\theta
+\begin{pmatrix} 0 &amp;amp; 0 &amp;amp; \sin^2\psi \sin^2\theta \\\ 0 &amp;amp; 0 &amp;amp; 0\\\ -1 &amp;amp; 0 &amp;amp; 0\end{pmatrix}d\psi\wedge d\varphi
+\begin{pmatrix} 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; \sin^2\psi \sin^2\theta\\\ 0 &amp;amp; \sin^2\psi &amp;amp; 0\end{pmatrix}d\theta\wedge d\varphi.
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt; &lt;em&gt;The left-invariant metric on $S^3 \cong SU(2)$ &amp;amp; Hopf coordinates.&lt;/em&gt;
Using the expression for the invariant connection on $SU(2)$, compute the Riemann tensor.
&lt;em&gt;Solution:&lt;/em&gt;
\begin{equation}\label{SU2-curv}
\Omega = \begin{pmatrix}0 &amp;amp; \sin^2\eta &amp;amp; 0\\\ -1 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix}d\eta\wedge d\xi_1
+ \begin{pmatrix}0 &amp;amp; 0 &amp;amp; \cos^2\eta\\\ 0 &amp;amp; 0 &amp;amp; 0\\\ -1 &amp;amp; 0 &amp;amp; 0\end{pmatrix}d\eta\wedge d\xi_2
+ \begin{pmatrix}0 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; \cos^2\eta\\\ 0 &amp;amp; -\sin^2\eta &amp;amp; 0\end{pmatrix} d\xi_1\wedge d\xi_2
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Schwarzschild metric.&lt;/em&gt;
Using the Schwarzschild connection, we can compute the curvature form. We begin by computing $d\Gamma$; we have that
$d\Gamma = - \partial_r \Gamma_t dt\wedge dr + \partial_r \Gamma_\theta  dr\wedge d\theta + \partial_r \Gamma_\varphi dr\wedge d\varphi + \partial_\theta \Gamma_\theta d\theta \wedge d\varphi$. Using the expression we derived, we find that:
\begin{align}
- \partial_r \Gamma_t dt\wedge dr  &amp;amp;= - \partial_r \begin{pmatrix}  0 &amp;amp; \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right)^{-1} &amp;amp; 0 &amp;amp; 0 \\\ \frac{1}{2}\frac{r_s}{r^2}\left(1-\frac{r_s}{r}\right) &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt\wedge dr =\begin{pmatrix}  0 &amp;amp; \frac{r_s(2r-r_s)}{2r^2(r-r_s)^2} &amp;amp; 0 &amp;amp; 0 \\\ \frac{r_s(2r-3r_s)}{2r^4} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt\wedge dr, \\\&lt;br&gt;
\partial_r \Gamma_\theta  dr\wedge d\theta &amp;amp;= \partial_r \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; -r\left(1 - \frac{r_s}{r}\right) &amp;amp; 0 \\\ 0 &amp;amp; \frac{1}{r} &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cot\theta\end{pmatrix}  dr\wedge d\theta = \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 \\\ 0 &amp;amp; -\frac{1}{r^2} &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix}  dr\wedge d\theta,\\\&lt;br&gt;
\partial_r \Gamma_\varphi dr\wedge d\varphi &amp;amp;= \partial_r \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -r\sin^2\theta\left(1-\frac{r_s}{r}\right) \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\sin\theta\cos\theta \\\ 0 &amp;amp; \frac{1}{r} &amp;amp; \cot\theta &amp;amp; 0\end{pmatrix}dr\wedge d\varphi = \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\sin^2\theta \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; -\frac{1}{r^2} &amp;amp; 0 &amp;amp; 0\end{pmatrix} dr\wedge d\varphi,\\\&lt;br&gt;
\partial_\theta \Gamma_\varphi d\theta \wedge d\varphi &amp;amp;= \partial_\theta\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -r\sin^2\theta\left(1-\frac{r_s}{r}\right) \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\sin\theta\cos\theta \\\ 0 &amp;amp; \frac{1}{r} &amp;amp; \cot\theta &amp;amp; 0\end{pmatrix} = \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -2r\sin\theta\cos\theta\left(1-\frac{r_s}{r}\right) \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \sin^2\theta-\cos^2\theta \\\ 0 &amp;amp; 0 &amp;amp; -\csc^2\theta &amp;amp; 0\end{pmatrix} d\theta \wedge d\varphi.
\end{align}
We now must compute $\Gamma \wedge \Gamma$. It is easily shown that $\Gamma\wedge\Gamma = [\Gamma_t,\Gamma_r]dt\wedge dr [\Gamma_t,\Gamma_\theta]dt\wedge d\theta + [\Gamma_t,\Gamma_\varphi]dt\wedge d\varphi + [\Gamma_r,\Gamma_\theta]dr\wedge d\theta + [\Gamma_r,\Gamma_\varphi]dr\wedge d\varphi + [\Gamma_\theta,\Gamma_\varphi]d\theta \wedge d\varphi$. Again, using the connection, and some (rather tedious, but still elementary) matrix algebra, we can eventually compute the Riemann tensor to be:
\begin{equation}
\begin{split}
\Omega &amp;amp;= \begin{pmatrix}  0 &amp;amp; \frac{r_s}{r^2(r-r_s)} &amp;amp; 0 &amp;amp; 0 \\\ \frac{r_s(r-r_s)}{r^4} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt\wedge dr
+ \begin{pmatrix}  0 &amp;amp; 0 &amp;amp; -\frac{r_s}{2r} &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ \frac{r_s(r-r_s)}{2r^4} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt\wedge d\theta\\\&lt;br&gt;
&amp;amp;+\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\frac{r_s \sin^2\theta}{2r} \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ \frac{r_s(r-r_s)}{2r^4} &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dt\wedge d\varphi
+\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; -\frac{r_s}{2r} &amp;amp; 0 \\\ 0 &amp;amp; \frac{r_s}{r^2(r-r_s)} &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\end{pmatrix} dr\wedge d\theta\\\&lt;br&gt;
&amp;amp;+\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\frac{r_s\sin^2\theta}{2r} \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; \frac{r_s}{2r^2(r-r_s)} &amp;amp; 0 &amp;amp; 0\end{pmatrix} dr\wedge d\varphi
+\begin{pmatrix}  0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\frac{r_s\sin^2\theta}{r} \\\ 0 &amp;amp; 0 &amp;amp; -\frac{r_s}{r} &amp;amp; 0\end{pmatrix} d\theta\wedge d\varphi
\end{split}
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;4-the-ricci-and-scalar-curvatures&#34;&gt;$4$. The Ricci and scalar curvatures.&lt;/h2&gt;
&lt;p&gt;Commonly, problems in geometry require one to compute derived quantities of the Riemann tensor. Two of the most important are the Ricci and scalar curvatures: the Ricci curvature is defined to be the trace of the Riemann tensor:
\begin{equation} \label{Ricci}
R_{ij} = \sum_k R^k_{ikj}.
\end{equation}
This quantity is symmetric in its remaining indices. We will carry on our examples from the previous section to make the computations here transparent. Finally, the scalar curvature is defined to be the contraction of both indices the Ricci tensor:
\begin{equation}
R = \sum_{i,j} g^{ij}R_{ij}.
\end{equation}
Let us compute these quantities for the examples from the previous sections.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;em&gt;The Sphere $S^2$.&lt;/em&gt;
Using equation the Riemann tensor we computed for $S^2$, we can compute the Ricci and scalar curvatures. We compute each component of the Ricci tensor individually: let us begin with $R_{\theta \theta}$. By equation the expression we have for the Ricci tensor, we have that $R_{\theta \theta} = R^{\theta}_{\theta \theta\theta} + R^{\varphi}_{\theta \varphi\theta} = R^{\varphi}_{\theta \varphi \theta}$, since the Riemann tensor is antisymmetric in the last two indices (it is a matrix-valued 2-form). Write $\Omega = \Omega_{kl} dx^k \wedge dx^l$. Since $(\Omega_{kl})^i_j = R^i_{jkl}$, and using our equation for $\Omega$, we see that $R^{\varphi}_{\theta \varphi\theta} = 1$. Similarly, we compute $R_{\varphi\varphi} = R^{\theta}_{\varphi \theta\varphi} + R^{\varphi}_{\varphi \varphi\varphi} = R^{\theta}_{\varphi \theta\varphi} = \sin^2\theta$. By symmetry of the Riemann tensor, we see that $R_{\theta\varphi} = R_{\varphi\theta} = 0$. Thus, the Ricci tensor (written in matrix form) is
\begin{equation}
\text{Ricci} = \begin{pmatrix}  1 &amp;amp; 0 \\\  0 &amp;amp; \sin^2\theta \end{pmatrix}.
\end{equation}
The scalar curvature is now easy to compute:
\begin{equation}
R = g^{\theta\theta}R_{\theta\theta} + g^{\varphi\varphi} R_{\varphi\varphi} = 1 + \frac{1}{\sin^2\theta}\sin^2\theta = 2,
\end{equation} so that the sphere is of constant positive curvature, as expected. In fact, the scalar curvature computed in this fashion for surfaces is always twice the Gaussian curvature.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exerise.&lt;/strong&gt;
Compute the Ricci and scalar curvatures of $S^2$ using stereographic coordinates. &lt;em&gt;Solution:&lt;/em&gt; The Ricci curvature is
\begin{equation}
\text{Ricci} = \begin{pmatrix}  4/\rho &amp;amp; 0 \\\  0 &amp;amp; 4/\rho \end{pmatrix},
\end{equation}
and the scalar curvature is $R = 2$, which is consistent with the result from the previous example.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The Torus $\mathbb{T}^2$.&lt;/em&gt;
The Ricci and scalar curvatures of the torus can be found by directly applying the formalism to the Riemann tensor we derived on the torus. By symmetry of the Riemann tensor, we again see that the components $R_{\theta\varphi} = R_{\varphi \theta} = 0$. It remains to compute the diagonal entries. Using the expression for the Ricci curvature, we have that $R_{\theta \theta} = R^{\theta}_{\theta \theta\theta} + R^{\varphi}_{\theta \varphi\theta} = R^{\varphi}_{\theta \varphi \theta} = \frac{r\cos\theta}{R+r\cos\theta}.$ Similarly, we compute $R_{\varphi\varphi}$ to be $R_{\varphi\varphi} = R^{\theta}_{\varphi \theta\varphi} = \frac{\cos\theta\left(R+r\cos\theta\right)}{r}$. The Ricci tensor is therefore
\begin{equation}
\text{Ricci} = \begin{pmatrix}  \frac{r\cos\theta}{R+r\cos\theta} &amp;amp; 0 \\\  0 &amp;amp; \frac{\cos\theta\left(R+r\cos\theta\right)}{r} \end{pmatrix}.
\end{equation}
We can then compute the scalar curvature as
\begin{equation}
R = g^{\theta\theta}R_{\theta\theta} + g^{\varphi\varphi} R_{\varphi\varphi} = \frac{1}{r^2}\left( \frac{r\cos\theta}{R+r\cos\theta}\right) + \frac{1}{(R+r\cos\theta)^2} \left(\frac{\cos\theta\left(R+r\cos\theta\right)}{r} \right) = \frac{2\cos\theta}{r(R+r\cos\theta)}.
\end{equation}
As a consistency check, let us compute the integrated total curvature:
\begin{align}
\int R dA  = \iint\ R \sqrt{g} d\theta d\varphi = \iint \left(\frac{2\cos\theta}{r(R+r\cos\theta)}\right) r(R+r\cos\theta) d\theta d\varphi = \iint 2\cos \theta d\theta d\varphi = 0,
\end{align}
which is consistent with the Gauss-Bonnet theorem ($\int_{\mathbb{T}^2} R dA  = 4\pi \chi (\mathbb{T}^2) = 0$).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;em&gt;The Upper Half Plane $\mathbb{H}$.&lt;/em&gt;
We now compute the derived curvature quantities for the upper half plane. By symmetry, the off-diagonal components of the Ricci tensor are zero. The $xx$ component of the Ricci tensor is: $R_{xx} = R^x_{xxx} + R^y_{xyx} =  -1/y^2$. Similarly, $R_{yy} = R^x_{yxy} + R^y_{yyy} = -1/y^2$. Thus, the Ricci tensor is
\begin{equation}
\text{Ricci} = \begin{pmatrix}  -1/y^2 &amp;amp; 0 \\\  0 &amp;amp; -1/y^2 \end{pmatrix}.
\end{equation}
We can then compute the scalar curvature to be
\begin{equation}
R = g^{xx} R_{xx} + g^{yy} R_{yy} = -1 -1 = -2,
\end{equation}
So that the upper half plane is of constant negative curvature, as expected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exercise.&lt;/strong&gt;
Compute the Ricci and scalar curvatures of the disc with the hyperbolic metric. &lt;em&gt;Solution:&lt;/em&gt; Recall that $\sigma(x,y) = 1 - x^2 - y^2$. The Ricci tensor is:
\begin{equation}
\text{Ricci} = \begin{pmatrix}  -2/\sigma^2 &amp;amp; 0 \\\  0 &amp;amp; -2/\sigma^2 \end{pmatrix}.
\end{equation}
The scalar curvature is $R = -2$, which agrees with the constant negative-curvature solution of the previous example.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt; &lt;em&gt;The 3-sphere $S^3$.&lt;/em&gt;
Using the expression for the Riemann tensor of the $3$-sphere, and the same procedure as before,
we compute the Ricci tensor to be
\begin{equation} \label{3-sphere-Ricci}
\text{Ricci} = 2\begin{pmatrix} 1 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; \sin^2\psi &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; \sin^2\psi \sin^2 \theta \end{pmatrix}.
\end{equation}
Taking the trace of this against the metric tensor, we compute the scalar curvature to be
\begin{equation}
R = g^{\psi\psi} R_{\psi \psi} + g^{\theta\theta} R_{\theta \theta} + g^{\varphi \varphi} R_{\varphi\varphi}
= 2 + 2 + 2 = 6,
\end{equation}
so that the $3$-sphere is a 3-manifold of constant curvature.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;em&gt;The left-invariant metric on $S^3 \cong SU(2)$ &amp;amp; Hopf coordinates.&lt;/em&gt;
Using the expression we derived for the Riemann tensor in Hopf coordinates, we compute the nontrivial components of the Ricci tensor to be
$R_{\eta \eta} = 2$, $R_{\xi_1 \xi_1} = 2\sin^2 \eta$, and $R_{\xi_2 \xi_2} = 2\cos^2 \eta$, so that
\begin{equation}
\text{Ricci} = 2\begin{pmatrix} 1 &amp;amp; 0 &amp;amp; 0\\\ 0 &amp;amp; \sin^2\psi &amp;amp; 0\\\ 0 &amp;amp; 0 &amp;amp; \cos^2\eta \end{pmatrix}.
\end{equation}
Contracting with the metric tensor, we obtain an expression for the scalar curvature:
\begin{equation}
R = g^{\eta \eta} R_{\eta \eta} + g^{\xi_1 \xi_1} R_{\xi_1 \xi_1} + g^{\xi_2 \xi_2} R_{\xi_2 \xi_2} = 2+2+2 = 6,
\end{equation}
which coincides with our previous result for the scalar curvature of $S^3$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computing Reflection Coefficient of Schrondinger Operator via WKB method</title>
      <link>/posts/wkb-reflection/</link>
      <pubDate>Sun, 08 Nov 2020 23:25:17 -0500</pubDate>
      <guid>/posts/wkb-reflection/</guid>
      <description>&lt;p&gt;See the PDF:
&lt;script type=&#34;text/javascript&#34; src=&#34;/js/pdf-js/build/pdf.js&#34;&gt;&lt;/script&gt;
&lt;style&gt;
#the-canvas {
  border: 1px solid black;
  direction: ltr;
  width: 100%;
  height: auto;
}
#paginator{
    text-align: center;
    margin-bottom: 10px;
}
&lt;/style&gt;

&lt;div id=&#34;paginator&#34;&gt;
    &lt;button id=&#34;prev&#34;&gt;Previous&lt;/button&gt;
    &lt;button id=&#34;next&#34;&gt;Next&lt;/button&gt;
    &amp;nbsp; &amp;nbsp;
    &lt;span&gt;Page: &lt;span id=&#34;page_num&#34;&gt;&lt;/span&gt; / &lt;span id=&#34;page_count&#34;&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;embed-pdf-container&#34;&gt;
    &lt;canvas id=&#34;the-canvas&#34;&gt;&lt;/canvas&gt;
&lt;/div&gt;

&lt;script type=&#34;text/javascript&#34;&gt;
window.onload = function() {



var url = &#34;\/pdf\/wkb.pdf&#34;;


var pdfjsLib = window[&#39;pdfjs-dist/build/pdf&#39;];


pdfjsLib.GlobalWorkerOptions.workerSrc = &#39;/js/pdf-js/build/pdf.worker.js&#39;;


var pdfDoc = null,
    pageNum = 1,
    pageRendering = false,
    pageNumPending = null,
    scale = 3,
    canvas = document.getElementById(&#39;the-canvas&#39;),
    ctx = canvas.getContext(&#39;2d&#39;);



function renderPage(num) {
  pageRendering = true;
  
  pdfDoc.getPage(num).then(function(page) {
    var viewport = page.getViewport({scale: scale});
    canvas.height = viewport.height;
    canvas.width = viewport.width;

    
    var renderContext = {
      canvasContext: ctx,
      viewport: viewport
    };
    var renderTask = page.render(renderContext);

    
    renderTask.promise.then(function() {
      pageRendering = false;
      if (pageNumPending !== null) {
        
        renderPage(pageNumPending);
        pageNumPending = null;
      }
    });
  });

  
  document.getElementById(&#39;page_num&#39;).textContent = num;
}



function queueRenderPage(num) {
  if (pageRendering) {
    pageNumPending = num;
  } else {
    renderPage(num);
  }
}



function onPrevPage() {
  if (pageNum &lt;= 1) {
    return;
  }
  pageNum--;
  queueRenderPage(pageNum);
}
document.getElementById(&#39;prev&#39;).addEventListener(&#39;click&#39;, onPrevPage);



function onNextPage() {
  if (pageNum &gt;= pdfDoc.numPages) {
    return;
  }
  pageNum++;
  queueRenderPage(pageNum);
}
document.getElementById(&#39;next&#39;).addEventListener(&#39;click&#39;, onNextPage);



pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {
  pdfDoc = pdfDoc_;
  document.getElementById(&#39;page_count&#39;).textContent = pdfDoc.numPages;

  
  renderPage(pageNum);
});
}

&lt;/script&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
